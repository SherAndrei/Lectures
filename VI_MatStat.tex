\documentclass[12pt]{article}

%Russian-specific packages
%--------------------------------------
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
%for search in russian
\usepackage{cmap}
%--------------------------------------

%Math-specific packages
%--------------------------------------
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{arydshln}

%Format-specific packages
%--------------------------------------
\usepackage[left=2cm,
            right=2cm,
            top=2cm,
            bottom=2cm,
            bindingoffset=0cm]{geometry}
%--------------------------------------

% for theorems, lemmas and definitions
%--------------------------------------
\usepackage{amsthm}

\counterwithin*{equation}{section}

\newtheorem{definition}{Опр.}
\counterwithin*{definition}{section}

\newtheorem{lemma}{Лемма}
\counterwithin*{lemma}{section}

\newtheorem{remark}{Замечание}
\counterwithin*{remark}{section}

\newtheorem*{remark*}{Замечание}

\newtheorem*{corollary}{Следствие}
\newtheorem*{proposition}{Предложение}
\newtheorem*{example}{Пример}

\newtheoremstyle{basic_theorem}    %<name>
                 {\topsep}   %<space above>
                 {\topsep}   %<space below>
                 {\itshape}  %<body font>
                 {}          %<indent amount>
                 {\bfseries} %Theorem head font>
                 {.}         %<punctuation after theorem head>
                 {\newline}  %<space after theorem head> (default .5em)
                 {}          %<Theorem head spec>
\theoremstyle{basic_theorem}
\newtheorem{theorem}{Теорема}
\counterwithin*{theorem}{subsection}

\newtheoremstyle{name_theorem}
                {\topsep}
                {\topsep}
                {\itshape}
                {}
                {\bfseries}
                {}
                {\newline}
                {\thmnote{#3}}
\theoremstyle{name_theorem}
\newtheorem*{named_theorem}{Теорема}
%--------------------------------------

% For images
%--------------------------------------
\usepackage{wrapfig}
\usepackage{graphicx}
\graphicspath{ {./images/} }

%--------------------------------------

\usepackage{framed}

% My commands
%--------------------------------------
% for definitions

\newcommand\defin[1]{\textbf{#1}}

\newcommand*{\defeq}{\stackrel{\text{def}}{=}}
\def\cov{ \mathrm{cov} }

\def\eps{ \varepsilon }
\def\Eps{ \mathcal{E} }

\def\R{ \mathbb{R} }
\def\E{ \mathrm{E} }
\def\D{ \mathrm{D} }
\def\P{ \mathrm{P} }

\def\littleO{ \overline{\overline{o}} }
%--------------------------------------

\begin{document}

\tableofcontents

\newpage

\section{Асимптотические оптимальные оценки}
    Пусть сл. векторы \(\xi_n, \xi \in \R^K\), и определены на \((\Omega, \mathcal{F}, \P)\).
Пусть функция распределения \(\xi_n\) есть \(F_n(x)\), хар. ф-ция есть \(\phi_n(t)\), а распределение
есть \(Q_n\). Для вектора \(\xi\) функцию распределения, хар. ф-цию и распреденеие обозначим \(F(x)\),
\(\phi(t),\ Q\) соответственно.

\begin{definition}
    Функция распределения \(F_n(x)\) сходится к \(F(x)\) при \(n \rightarrow \infty\) в основном
    (пишем \(F_n(x) \Rightarrow F\)), если \(F_n(x) \rightarrow F(x) \ \forall x \in C(F)\)
\end{definition}

\begin{definition}
    Распределение \(Q_n\) сходится к распределению \(Q\) слабо (пишем \(Q_n \xrightarrow{w} Q\)),
    если \(\forall\) непреревной и ограниченной \(g: \R^K \rightarrow \R^1\)
    \[ \int_{\R^K} g(x)Q_n(dx) \rightarrow \int_{\R^K} g(x)Q(dx)\]
    или, эквивалентно, \(\E g(\xi_n) \rightarrow \E g(\xi)\).
\end{definition}

\begin{theorem}
    Следующие условия эквивалентны:
    \begin{enumerate}
        \item \(F_n(x) \Rightarrow F\)
        \item \(Q_n \xrightarrow{w} Q\)
        \item \(\phi_n(t) \rightarrow \phi \ \forall t \in \R^K\)
    \end{enumerate}
    Если выполненое любое из условий \(1 - 3\), будем писать
    \(\xi_n \xrightarrow{d} \xi\) и говорить, что \(\xi_n\) сходится к \(\xi\) по распределению.
\end{theorem}

\begin{theorem}[О наследовании сходимости]
\label{th::inherit_conv}
    Пусть сл. векторы \(\xi_n, \xi \in \R^K, H: \R^K \rightarrow \R^1\)
    Тогда:
    \begin{enumerate}
        \item Если \(\xi_n \xrightarrow{d} \xi\), то \(H(\xi_n) \xrightarrow{d} H(\xi)\)
        \item Если \(\xi_n \xrightarrow{\P} \xi\), то \(H(\xi_n) \xrightarrow{\P} H(\xi)\)
    \end{enumerate}
\end{theorem}

\begin{named_theorem}[Лемма Слуцкого]
\label{th::slut}
    Пусть \(\xi_n, \xi, \eta_n, a \in \R^1, \xi_n \xrightarrow{d} \xi\), а \(\eta_n \xrightarrow{\P} a\).
    Тогда:
    \begin{enumerate}
        \item \label{th::slut:sum} \(\xi_n + \eta_n \xrightarrow{d} \xi + a\)
        \item \label{th::slut:mult} \(\xi_n \eta_n \xrightarrow{d} a\xi\)
    \end{enumerate}
\end{named_theorem}
\begin{proof}
    Достаточно показать, что вектор
    \begin{equation} \label{eq::LemmaSlut}
        (\xi_n, \eta_n)^T \xrightarrow{d} (\xi, a)^T
    \end{equation}
    Действительно, если \eqref{eq::LemmaSlut} верно, то при \(H(x, y) = x + y\) в силу Теоремы \ref{th::inherit_conv} получаем пункт \ref{th::slut:sum} леммы,
    а при \(H(x, y) = xy\) -  пункт \ref{th::slut:mult}.

    Для доказательства \ref{th::slut:sum}, проверим, что хар. ф-ция
    вектора \((\xi_n, \eta_n)^T\) сходится к хар. функции вектора \((\xi, \eta)^T\).
    Имеем:
    \[\left\lvert\E e^{it\xi_n + is\eta_n} - \E e^{it\xi + isa}\right\rvert \leq \left\lvert \E e^{it\xi_n + is\eta_n} - \E e^{it\xi_n + isa}\right\rvert + \left\lvert \E e^{it\xi_n + isa} - \E e^{it\xi + isa}\right\rvert = \alpha_n + \beta_n\]

    \[\alpha_n \leq \E \left\lvert e^{it\xi_n}(e^{it\eta_n + isa}) \right\rvert  = \E \left\lvert e^{it\eta_n + isa} \right\rvert  = \E g(\eta_n), \ g(x)\defeq\left\lvert e^{isx} - e^{isa} \right\rvert \]
    Ф-ция g(x) непрерывна и ограничена, а т.к. \(\eta_n \xrightarrow{d} a\),
    то в силу Теоремы \ref{th::inherit_conv} \(\E g(\eta_n) \rightarrow \E g(a) = 0\)
    Итак, \(\alpha \rightarrow 0\).

    \[\beta_n = \left\lvert \E e^{isa}(e^{it\xi_n} - e^{it\xi}) \right\rvert  = \left\lvert e^{isa} \E (e^{it\xi_n} - e^{it\xi}) \right\rvert  = \left\lvert \E (e^{it\xi_n} - e^{it\xi}) \right\rvert  \rightarrow 0\]
    т.к. \(\xi_n \xrightarrow{d} \xi\) и \(\phi_n(t) \rightarrow \phi(t)\).
\end{proof}

Пусть наблюедние \(X \sim  P_{\theta},\ \theta \in \Theta \subseteq \R^{K}\), а \(\widehat{\theta}_n\) - оценка \(\theta\)

\begin{definition}
    Если \(\sqrt{n}(\widehat{\theta}_n - \theta) \xrightarrow{d} N(0, \Sigma(\theta)) \ \forall \theta \in \Theta\)
    и ковариционная матрица \(0 < \Sigma(\theta) < \infty\), то \(\widehat{\theta}_n\) называется асимптотической нормальной оценкой.
\end{definition}

\begin{definition}
    Если \(\widehat{\theta}_n \xrightarrow{\P} \theta \ \forall \theta \in \Theta\), то \(\widehat{\theta}_n\) называется состоятельной оценкой.
\end{definition}

\begin{remark}
    Дальше \(\theta \in \Theta \subseteq \R^1\), то есть \(\theta\) и \(\widehat{\theta}_n\) - скаляры.
\end{remark}

Если \(\widehat{\theta}_n\) - состоятельная оценка \(\theta\), то при больших и \(\widehat{\theta}_n \approx \theta\) с вероятностью, близкой к единице.

Если \(\widehat{\theta}_n\) - асимптотическая нормальная оценка \(\theta\) (так как \(\theta\) и \(\widehat{\theta}_n\) скаляры:

\(\sqrt{n}(\widehat{\theta}_n - \theta) \xrightarrow{d} N(0, \sigma^2(\theta)) \ 0 < \sigma^2 < \infty,\ \forall \theta \in \Theta\)), то: \begin{enumerate}

    \item \(\widehat{\theta}_n\) - состоятельная оценка \(\theta\), так как \(\widehat{\theta}_n - \theta = n^{-1/2} \sqrt{n}(\widehat{\theta}_n - \theta) \xrightarrow{\P} 0\)
        в силу п. \ref{th::slut:mult} леммы Слуцкого.
    \item Скорость сходимости \(\widehat{\theta}_n\) к \(\theta\) есть \(O(\sqrt{n})\)
    \item При больших \(n\) со сл. в. \(\sqrt{n}(\widehat{\theta}_n - \theta)\) можно обращаться (с осторожностью!) как с Гауссовской величиной.

    Например, пусть дисперсия предельного Гауссовского закона \(\sigma^2(\theta)\) будет непреревной ф-цией \(\theta\). Тогда
        \[ \frac{\sqrt{n}(\widehat{\theta}_n - \theta)}{\sigma(\widehat{\theta}_n)} =
        \underbrace{\frac{\sqrt{n}(\widehat{\theta}_n - \theta)}{\sigma(\theta)}}_{\xrightarrow{d} N(0, 1)}
        \underbrace{\frac{\sigma(\theta)}{\sigma(\widehat{\theta}_n)}}_{\xrightarrow{\P} 1} \xrightarrow{d} \eta \sim N(0, 1)\]
        в силу п. 2 леммы Слуцкого. Значит,
        \[\P_\theta \left(\left\lvert \frac{\sqrt{n}(\widehat{\theta}_n - \theta)}{\sigma(\widehat{\theta}_n)} \right\rvert  < \xi_{1 - \alpha/2}\right) \rightarrow \P(\left\lvert \eta \right\rvert  < \xi_{1 - \alpha/2}) = 1 - \alpha\]

        То есть примерно с вероятностью \(1 - \alpha\) выполнено неравенство, или эквивалентно раскроем по модулю
        \[\underbrace{\widehat{\theta}_n - n^{-1/2}\sigma(\widehat{\theta}_n)\xi_{1 - \alpha /2} < \theta < \widehat{\theta}_n + n^{-1/2}\sigma(\widehat{\theta}_n)\xi_{1 - \alpha /2}}_{\mbox{Асимптотический доверительный интервал уровня \(1 - \alpha\)}}\]

    \item Асимптотические Гауссовские оценки можно сравнивать между собой: \\
    Если \(\sqrt{n} (\widehat{\theta}_{i,n} - \theta) \xrightarrow{d} N(0, \sigma^2_{i}(\theta)),\ i = 1, 2, \ldots\), то
    можно посчитать асимптотическую относительную эффективность (АОЭ):
    \[e_{1,2} = \frac{\sigma_2^2(\theta)}{\sigma_1^2(\theta)}\]
    Напомним, $e_{1, 2} = \lim_{n \to \infty} \frac{n'(x)}{n (x)}, \mbox{ где }
    \sqrt{n}(\widehat{\theta}_{1,n} - \theta) \xrightarrow{d} N(0, \sigma_1^2(\theta))$
    и \(\sqrt{n}(\widehat{\theta}_{2,n'} - \theta) \xrightarrow{d} N(0, \sigma_1^2(\theta))\).
\end{enumerate}

    Вопрос: Есть ли такая оценка \(\theta^*_n\), что АОЭ \(e_{\theta^*_n, \widehat{\theta}_n}(\theta) \geq 1 \ \forall \widehat{\theta}_n\)
    и всех \(\theta \in \Theta\), то есть эффективнее всех остальных?

    Если да, то \(\theta^*_n\) требует не больше наблюдений, чем любая \(\widehat{\theta}_n\), чтобы достичь одинаковой с \(\widehat{\theta}_n\) точности.
    Ясно, что пределеная дисперсия \(\sqrt{n}(\theta^*_n - \theta)\) должна быть не больше асимптотической дисперсии
    \(\sqrt{n}(\widehat{\theta}_n - \theta)\) для любой асимптотической Гауссовской оценки \(\widehat{\theta}_n\). Но
    какова самая маленькая асимптотическая дисперсия у \(\sqrt{n}(\widehat{\theta}_n - \theta)\)?

    \begin{named_theorem}[Теорема Бахадура]
        \label{th::bahadur}
        Пусть \(X_1, \ldots, X_n\) - н. о. р. сл. в., \(X_1\) имеет
        плотность вероятности \(f(x, \theta),\ \theta \in \Theta \subseteq \R^1\),
        по мере \(\nu\). Пусть выполнены следующие условия:
        \begin{enumerate}
            \item \(\Theta\) - интервал.
            \item Носитель \(N_f = \{x: f(x, \theta) > 0\}\) не зависит от \(\theta\).
            \item \label{th::bahadur:density} \(\forall x \in N_f\) плотность \(f(x, \theta)\) дважды непрерывно
                дифференцируема по \(\theta\)
            \item \label{th::bahadur:integral} Интеграл \(\int f(x, \theta)\nu(dx)\)  можно
                дважды дифференцировать по \(\theta\), внося знак
                дифференцирования под знак интеграла.
            \item Информация Фишера \(0 < i(\theta) < \infty \ \forall \theta \in \Theta\)
            \item \label{th::bahadur:second_partial} \(\left\lvert \frac{\partial^2}{\partial \theta^2} \ln(f(x, \theta)) \right\rvert  \leq M(x) \ \forall x \in N_f, \ \theta \in \Theta, \ \E_\theta M(X_1) < \infty\)
        \end{enumerate}
        Тогда, если \(\sqrt{n}(\widehat{\theta}_n - \theta) \xrightarrow{d} N(0, \sigma^2(\theta))\),
        то \(\sigma^2(\theta) \geq \frac{1}{i(\theta)}\) всюду за исключением
        множества Лебеговой меры нуль.
    \end{named_theorem}
    \begin{remark}
        Если вдобавок \(\sigma^2(\theta)\) и \(i(\theta)\) непрерывны,
        то \(\sigma^2(\theta) \geq \frac{1}{i(\theta)}\) при всех \(\theta \in \Theta\).
    \end{remark}
    \begin{proof}
        Без доказательства.
    \end{proof}

    \begin{definition}
        Если \(\theta, \widehat{\theta}_n \in \R^1\) и $\sqrt{n}(\widehat{\theta}_n - \theta)\xrightarrow{d} N(0, \frac{1}{i(\theta)}),
        \ n \rightarrow \infty, \ \forall \theta \in \Theta,$
        причем \(0 < i(\theta) < \infty\), то \(\widehat{\theta}_n\) называется \defin{асимптотически
        эффективной оценкой}.
    \end{definition}
    Вопрос: Вообще можно ли найти такую оценку \(\widehat{\theta}_n\)? Да

    \newpage

    Дальше \(X = (X_1, \ldots, X_n), \ X \sim \P_\theta,\ \theta \in \Theta \subseteq \R^1\).
    \underline{Условие (A)}:
    \begin{enumerate}
        \item \(\Theta\) - интервал, \(\P_{\theta_1} \neq P_{\theta_2}\) при \(\theta_1 \neq \theta_2\).
        \item \(X_1, \ldots, X_n\) - независимые одинаково распределенные случайные величины
        \item \(X_1\) имеет плотность вероятности \(f(x, \theta)\) по мере \(\nu\)
        \item Носитель \(N_f = \{x: f(x, \theta) > 0\}\) не зависит от \(\theta\).
        \item Плотность вектора \(X\) есть \(p(x, \theta) = \prod_{i=1}^n f(x_i, \theta)\).
    \end{enumerate}
    \begin{definition}
    Функция \(p(X, \theta)\) как функция \(\theta\) при фиксированном \(X\) называется
    \defin{правдоподобием} функции.
    \[L_n(X, \theta) = \ln p(X, \theta) = \sum_{i=1}^n \ln f(X_i, \theta)\]
    называется логарифмическим правдоподобием.
    \end{definition}

    Пусть \(\theta_0\) будет истинное значение параметра.
    \begin{lemma}[Неравенство Йенсена]
        Пусть \(g(x)\) выпукла книзу борелевская функция, \(\E\left\lvert \xi \right\rvert  <\infty\),
        \(\E\left\lvert g(\xi) \right\rvert  <\infty\). Тогда \(g(\E\xi) \leq \E g(\xi)\). Если \(\xi\)
        не является почти наверное константой и \(g\) строго выпукла, то неравенство строгое.
    \end{lemma}
    \begin{theorem}[Экстремальное свойство правдоподобия]
        \label{th::extr_plausibility}
        Пусть выполнено Условие (A). Пусть \(E_{\theta_0} \left\lvert \ln f(X_1, \theta) \right\rvert  < \infty,\ \forall \theta \in \Theta\).
        Тогда
        \[\P_{\theta_0}(p(X, \theta_0) > p(X, \theta)) \rightarrow 1,\ n\rightarrow \infty,\ \theta_0 \neq \theta\]
    \end{theorem}
    \begin{proof}
        \[p(X, \theta_0) > p(X, \theta) \Leftrightarrow \ln p(X, \theta_0) > \ln p(X, \theta) \Leftrightarrow\]
        \[\eta_n\defeq n^{-1} \sum_{i=1}^n \ln \left(\frac{f(X_i, \theta)}{f(X_i, \theta_0)}\right) < 0\]
    То есть надо показать, что \(\P_{\theta_0}(\eta_n < 0) \rightarrow 1\). Но по слабому закону больших чисел:
    \[\eta_n = n^{-1}\sum \ln \left(\frac{f(X_i, \theta)}{f(X_i, \theta_0)} \right) \xrightarrow{\P}
    E_{\theta_0}\ln \left(\frac{f(X_1, \theta)}{f(X_1, \theta_0)} \right) \]

    Возьмем функцию \(-\ln x\) - строго выпукла вниз и \(\frac{f(X_1, \theta)}{f(X_1, \theta_0)}\)
    не является п.н. константой (так как иначе если плотности п.н. совпадают,
    то и распределения при разных значениях совпадают, что противоречит Условию(A)(1)).

    В силу неравенства Йенсена:
    \[\E_{\theta_0} \ln \frac{f(X_1, \theta)}{f(X_1, \theta_0)} < \ln \E_{\theta_0} \frac{f(X_1, \theta)}{f(X_1, \theta_0)} = \ln \int_{N_f} \frac{f(x, \theta)}{f(x, \theta_0)} f(x, \theta_0) \nu(dx) = \ln1 = 0\]
    Но если \(\eta_n\) сходится по вероятности к отрицательному числу, то \(P_{\theta_0}(\eta_n < 0) \rightarrow 1\)
\end{proof}
    В силу теоремы \ref{th::extr_plausibility} естественно брать
    оценкой то значение \(\theta\), которое максимизирует \(p(X, \theta)\) при данном \(X\)

    \begin{definition}
        Случайная величина \(\widehat{\theta}_n \in \Theta\) называется
        \defin{оценкой максимального правдоподобия (о.м.п.)}, если
        \(p(X, \widehat{\theta}_n) = \max_{\theta\in\Theta} p(X, \theta)\),
        или эквивалентно \(L_n(X, \widehat{\theta}_n) = max_{\theta\in\Theta} L_n(X, \theta)\)
    \end{definition}
    Итак, о.м.п \(\widehat{\theta}_n = \arg\max_{\theta\in\Theta} L_n(X, \theta)\).

    Если в \(\forall\theta\in\Theta\) максимум не достигается, то о.м.п. не существует.

    Если \(\Theta\) - интервал, \(L_n(X, \theta)\) - гладкая по \(\theta\) функция,
    то \(\theta\) удовлетворяет уравнению правдоподобия
    \begin{equation} \label{eq::plausibility}
        \frac{\partial}{\partial\theta}L_n(X, \theta) = 0
    \end{equation}
    \begin{theorem}[О состоятельности решения уравнения правдоподобия]
        \label{th::consist_plausibility}
        Пусть выполнено Условие (А). Пусть \(\forall x \in N_f \ \exists\) непрерывная
        производная \(f'_{\theta}(x, \theta)\). Тогда уравнение \eqref{eq::plausibility}
        с вероятностью, стремящейся к 1 при \(n\rightarrow \infty\) имеет решение \(\in\Theta\).
        При этом среди всех такиъ решений есть такой корень \(\widehat{\theta}_n\), что он
        является состоятельнаой оценкой \(\theta_0\)
    \end{theorem}

    \begin{proof}
        Пусть \(S_n = \{\omega\}\), при которых уравнение \eqref{eq::plausibility} имеет
        решение для \(\theta\in\Theta\). Тогда теорема \ref{th::consist_plausibility} утверждает:
        \begin{enumerate}
            \item \(P_{\theta_0}(S_n) \rightarrow 1\).
            \item Существует такое решение \(\widehat{\theta}_n \in \Theta\), что
                \[P_{\theta_0} \left(\left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps, S_n\right) \rightarrow 1,\ n\rightarrow\infty,\ \forall\eps>0\]
        \end{enumerate}
        \underline{Докажем пункт 1}: Выберем малое \(a>0\) так, что на \((\theta_0 - a, \theta_0 + a) \subseteq\Theta\). Пусть
        \[S^a_n = \{\omega: L_n(X, \theta_0) > L_n(X, \theta_0 - a), L_n(X, \theta_0) > L_n(X, \theta_0 + a)\}\]
        В силу теоремы \ref{th::extr_plausibility} \(\P_{\theta_0}(S_n^a) \rightarrow 1\)

        При \(\omega\in S_n^a\) функция \(L_n(X, \theta)\) имеет
        локальный максимум \(\widehat{\theta}^a_n\) на интервале \((\theta_0 - a, \theta_0 + a)\)
        \begin{figure}[h]
            \includegraphics[width=0.4\textwidth]{L_n_func}
        \end{figure}

        Значит, \(\frac{\partial}{\partial\theta}L_n(X, \widehat{\theta}_n^a) = 0\).
        Тогда \(\P_{\theta_0}(S_n) \geq \P_{\theta_0}(S_n^a) \rightarrow 1\), так
        как \(S_n^a \subseteq S_n\), и пункт 1 доказан.

        \underline{Докажем пункт 2}: \(\forall n\) при \(\omega\in S_n\) может сущестовать целое множество корней
        \(\{\theta^*_n\}\). Выберем в этом множестве корень \(\widehat{\theta}_n\),
        ближайший к \(\theta_0\). Это можно сделать, так как
        функция \(\frac{\partial}{\partial\theta} L_n(x, \theta)\) непрерывна по \(\theta\),
        и последовательность корней есть корень. Этот корень \(\widehat{\theta}_n\)
        и есть состоятельная оценка \(\theta\). Покажем это:

        \(\forall \text{ малого } \eps > 0\):
        \begin{equation}
            \label{eq::S_n}
            \P_{\theta_0}(\left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps, S_n) \geq
            \P_{\theta_0}(\left\lvert \widehat{\theta}^\eps_n - \theta_0 \right\rvert  < \eps, S_n^\eps)
        \end{equation}
        Так как $S^\eps_n \subseteq S_n,\
        (\omega: \left\lvert \widehat{\theta}^\eps_n - \theta_0 \right\rvert  < \eps) \subseteq
        (\omega: \left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps)$

        Но $\P_{\theta_0}(\left\lvert \widehat{\theta}^\eps_n - \theta_0 \right\rvert  < \eps, S^\eps_n)
        \underset{\text{т.к. события из } S_n^\eps \text{ лежат в } \left\lvert \widehat{\theta}^\eps_n - \theta_0 \right\rvert  < \eps}{=}
        \P_{\theta_0}(S_n^\eps) \rightarrow 1$, значит в силу \eqref{eq::S_n}
        \[\P_{\theta_0} (\left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps, S_n) \rightarrow 1\]
    \end{proof}

    \begin{remark}
        Пусть
        \[\theta^*_n = \begin{cases}
            \text{сост. корню уравнения правдоподобия, если он сущ.} \\
            \theta',\ \theta'\in\Theta, \text{иначе}
        \end{cases}\]
        Тогда случайная величина \(\theta^*_n\) всегда определена, и
        \(\theta^*_n \xrightarrow{\P} \theta_0\), так как
        \[\P(\left\lvert \theta^*_n - \theta_0 \right\rvert  < \eps) =
        \P(\left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps, S_n) +
        \P(\left\lvert \theta' - \theta_0 \right\rvert  < \eps, \overline{S}_n) \rightarrow 1\]
        Ясно, что
        \begin{equation}
            \frac{\partial}{\partial\theta} L_n(X, \theta^*_n) = \littleO_p(1)
        \end{equation}
        Так как производная отлична от нуля только на \(\overline{S}_n\).

        Будем называть \(\theta_n^*\) \defin{обобщенным состоятельным корнем уравнения
        правдоподобия}
    \end{remark}

    \begin{theorem}[Об асимптотической эффективности состоятельности решения]
        \label{th::asympt_consist}
        Пусть \(X = (X_1, \ldots, X_n),\ \{X_i\}\) - н.о.р. сл.в., и
        удовлетворяются предположения Теоремы Бахадура, в которых условия
        \ref{th::bahadur:density} и \ref{th::bahadur:second_partial} заменены на
        предположения о третьей, а не второй производной. То есть
        \[\left\lvert \frac{\partial^3}{\partial \theta^3} \ln f(x, \theta) \right\rvert  \leq M(x) \ \forall x\in N_f,\ \forall\theta\in\Theta,\ \E_{\theta_0}M(X_1) < \infty\]
        Тогда, если \(\theta^*_n\) - обобщенный состоятельный корень из теоремы \ref{th::consist_plausibility}, то
        \[\sqrt{n}(\theta^*_n - \theta_0) \xrightarrow{d} N(0, \frac{1}{i(\theta_0)})\]
        То есть \(\theta^*_n\) - асимптотическая эффективная оценка.
    \end{theorem}
    \begin{proof}
        Будем обозначать \(\frac{\partial}{\partial\theta}L_n(X, \theta), \frac{\partial^2}{\partial\theta^2}L_n(X, \theta), \ldots\)
        через \(L'_n(\theta), L^{(2)}_n(\theta), \ldots\).

        Для фиксированного \(X\) в силу формулы Тейлора и последнего замечания:
        \[\littleO_p(1) = L'_n(\theta^*_n) = L'_n(\theta_0) + L^{(2)}_n(\theta_0)(\theta^*_n - \theta_0) +
        \frac{1}{2}L_n^{(3)}(\widetilde{\theta}_n)(\theta^*_n - \theta_0)^2,\ \widetilde{\theta}_n \in(\theta_0, \theta_n^*)\]
        Отсюда,
        \begin{equation}
            \label{eq::taylor_frac}
            \sqrt{n}(\theta^*_n - \theta_0) = -\frac{n^{-1/2} L'_n(\theta_0) + \littleO_p(1)}{n^{-1}(L^{(2)}_n(\theta_0) + \frac{1}{2}L^{(3)}_n(\widetilde{\theta}_n)(\theta^*_n - \theta_0))}
        \end{equation}
        \underline{Рассмотрим числитель \eqref{eq::taylor_frac}} и покажем, что
        \begin{equation}
            \label{eq::taylor_frac::num}
            n^{-1/2}L_n'(\theta_0) = n^{-1/2}\sum_{i=1}^n \frac{f'_\theta(X_i, \theta_0)}{f(X_i, \theta_0)} \xrightarrow{d} \xi\sim N(0, i(\theta_0))
        \end{equation}
        Действительно,
        \[\E_{\theta_0}\frac{f'_{\theta_0}(X_1, \theta_0)}{f(X_i, \theta_0)} = \int_{N_f}\frac{f'_\theta(x, \theta_0)}{f(x,\theta_0)} f(x,\theta_0) \nu(dx) = 0\]
        \[\D_{\theta_0}\frac{f'_{\theta_0}(X_1, \theta_0)}{f(X_i, \theta_0)} = \E_{\theta_0}\left(\frac{\partial}{\partial\theta}\ln f(X_1, \theta_0)\right)^2 - \underbrace{\left(\E_{\theta_0}\frac{f'_{\theta_0}(X_1, \theta_0)}{f(X_i, \theta_0)}\right)^2 }_{ =\ 0} \underset{\text{по опр.}}{=} i(\theta_0)\]
        Так как \(f, f'\) - борелевские функции, то случайные величины \(\{\frac{f'_\theta(X_i, \theta_0)}{f(X_i, \theta_0)},\ i=1,\ldots,n\}\) - н.о.р.,
        соотношение \eqref{eq::taylor_frac::num} следует из Центр. пред. Теоремы.

        В силу Леммы Слуцкого числитель \eqref{eq::taylor_frac} \(\xrightarrow{\P} N(0, i(\theta_0))\)

        Теперь \underline{рассмотрим знаменатель \eqref{eq::taylor_frac}}:
        \begin{equation}
            \label{eq::taylor_frac::den}
            n^{-1}L_n^{(2)}(\theta_0) = n^{-1}\sum^n_{i=1}\left[ \frac{f^{(2)}_\theta(X_i, \theta_0)}{f(X_i, \theta_0)} - \left(\frac{f'_\theta(X_i, \theta_0)}{f(X_i, \theta_0)}\right)^2\right] \xrightarrow{\P} -i(\theta)
        \end{equation}
        Действительно, в силу ЗБЧ
        \[n^{-1}\sum^n_{i=1} \frac{f^{(2)}_\theta(X_i, \theta_0)}{f(X_i, \theta_0)} \xrightarrow{\P} \E_{\theta_0}\frac{f^{(2)}_\theta(X_1, \theta_0)}{f(X_1, \theta_0)} = \int_{N_f} \frac{f^{(2)}_\theta(x, \theta_0)}{f(x, \theta_0)} f(x, \theta_0) \nu(dx) = 0\]
        \[n^{-1}\sum^n_{i=1} \left(\frac{f'_\theta(X_i, \theta_0)}{f(X_i, \theta_0)}\right)^2 \xrightarrow{\P} E_{\theta_0} \left(\frac{\partial}{\partial\theta} \ln f(X_1, \theta_0)\right)^2 = i(\theta)\]

    Применяя лемму Слуцкого, получим \eqref{eq::taylor_frac::den}.

    Далее рассмотрим второе слагаeмое в знаменете \eqref{eq::taylor_frac}
    \begin{equation}
        \label{eq::taylor_frac::den2}
        \left\lvert \frac{1}{2n} L_n^{(3)}(\widetilde{\theta}_n)(\theta^*_n - \theta_0) \right\rvert  \leq \frac{1}{2}\left\lvert \theta_n^* - \theta_0 \right\rvert  n^{-1} \sum_{i=1}^n M(X_i) \xrightarrow[\text{л. Слуцкого}]{\P} 0
    \end{equation}

    В силу \eqref{eq::taylor_frac::den} и \eqref{eq::taylor_frac::den2} и Леммы Слуцкого
    знаменатель \eqref{eq::taylor_frac} сходится по вероятности к \(-i(\theta_0)\)

    Значит, что вся дробь \eqref{eq::taylor_frac} сходится по распределению к
    \(\frac{1}{i(\theta_0)} \xi \sim N(0, \frac{1}{i(\theta_0)})\)
\end{proof}

\subsection*{Оценки максимального правдоподобия для векторого параметра}
    Пусть \(X = (X_1, \ldots, X_n)\) - н.о.р., $X_1 \sim f(x, \theta),\ \theta\in\Theta\subseteq\R^k,\
    \Theta$ - открытое множество

    Тогда логарифмические правдоподобие имеет вид
    \[L_n(X, \theta) = \sum_{i=1}^n\ln f(X_i, \theta)\]

    Система уравнений правдоподобия
    \begin{equation*}
        \label{eq::sys_plausibility}
        \frac{\partial L_n(X, \theta)}{\partial\theta_i} = 0,\ i =1,2,\ldots,k
    \end{equation*}

    При условиях регулярности, похожих на условия теоремы \ref{th::asympt_consist},
    показыватся:
    \begin{enumerate}
        \item С вероятностью, стремящейся к единице при \(n \rightarrow \infty\),
            система уравнений \eqref{eq::sys_plausibility} имеет такое решение \(\widehat{\theta}_n\in\Theta\),
            что \(\widehat{\theta}_n\) сходится к истинному значению \(\theta_0\).
        \item Соответствующая оценка \(\theta^*_n\) асимптотически нормальна. А именно
         \[\sqrt{n}(\theta^*_n - \theta_0) \xrightarrow{d} N(0, I^{-1}(\theta_0)),\ n\rightarrow\infty\]
         Здесь \(I(\theta) > 0\) - матрица информации Фишера, то есть
         \[I(\theta) = (I_{ij}(\theta)),\ I_{ij}(\theta) = \E_\theta \left\{\frac{\partial \ln f(X, \theta)}{\partial\theta_i} \cdot \frac{\partial\ln f(X, \theta)}{\partial\theta_j}\right\}\]
    \end{enumerate}
\begin{example}
        \(X = (X_1, \ldots, X_n)\), где \(\{X_i\}\) - н.о.р., \(X_1 \sim N(0, \sigma^2),\ a < \theta < b\),
        \(a\) и \(b\) - известные конечные числа, дисперсия \(\sigma^2\) известна.
        Построим асимптотически эффективную оценку \(\theta^*_n\) для \(\theta\).

        Здесь \(p(x, \theta) = \left(\frac{1}{\sqrt{2\pi} \sigma}\right)^ne^{-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\theta)^2}\),
        значит
        \[L_n(X, \theta) = \ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n - \frac{1}{2\sigma^2}\sum_{i=1}^n(X_i-\theta)^2\]
        Уравнение правдоподобия имеет вид
        \[\frac{\partial L_n(X, \theta)}{\partial\theta} = \frac{1}{\sigma^2}\sum_{i=1}^n(X_i - \theta) = 0\]
        Его решение существует и единственно, это \(\overline{X}\), причем
        в т. \(\theta = \overline{X}\ L_n(X,\theta)\) достигает максимума,
        так как \(\frac{\partial^2 L_n(X, \overline{X})}{\partial\theta^2} = - \frac{1}{\sigma^2} < 0\)

        Таким образом, если \(a < \overline{X} < b\), то о.м.п. сущесвтует и равна \(\overline{X}\),
        в противном случае о.м.п. не существует. Если положить
        \begin{equation}
            \label{eq::ex::as_appraisal}
            \theta^*_n = \begin{cases}
                \overline{X},\ a < \overline{X} < b \\
                \frac{a+b}{2},\ \overline{X} \notin (a,b)
            \end{cases}
        \end{equation}
        То в силу теоремы \ref{th::asympt_consist} (её условия
        выполнены, проверьте сами), \(\theta^*_n\) - асимптотически эффективная оценка, то есть
        \begin{equation}
            \label{eq::ex::as_conv}
            \sqrt{n}(\theta^*_n - \theta_0) \xrightarrow{d} N(0, \sigma^2)
        \end{equation}
        Напомним, что в этой модели \(i(\theta) = \frac{1}{\sigma^2}\).
        Cправедливость \eqref{eq::ex::as_conv} c \(\theta^*_n\)
        из \eqref{eq::ex::as_appraisal} легко проверить непосредственно.
\end{example}
\begin{example}
    Если \(\Theta\) - компакт (то есть отрезок \([a, b]\)), то о.м.п. существует
    всегда, так как непрерывная функция на отрезке всегда достигает своего максимума.
    Значит значение о.м.п.
    \begin{equation*}
        \theta^*_n = \begin{cases}
            \overline{X},\ a < \overline{X} < b \\
            a,\ \overline{X} < a \\
            b,\ \overline{X} > b
        \end{cases}
    \end{equation*}
    Но на границах теряется асимптотическая Гауссовость.
\end{example}


\section{Проверка статистических гипотез}
\(X = (X_1, \ldots, X_n)\) имеет плотность вероятности \(p(X, \theta)\)
по мере \(\mu,\ \theta\in\Theta\subseteq\R^1\)

\begin{definition}
    Предположение вида \(H_0: \theta\in\Theta_0\), где
    \(\Theta_0\in\Theta\), называется параметрической гипотезой.
    Альтернатива \(H_1:\theta\in\Theta_1\), где
    \(\Theta_1\in\Theta\backslash\Theta_0\)
\end{definition}

\begin{definition}
    Если \(\Theta_0(\Theta_1)\) состоит из одной точки,
    то гипотеза \(H_0\) (альтернатива \(H_1\)) называется
    простой.
    В противном случае \(H_0(H_1)\) - сложная
\end{definition}

\underline{Постановка задачи}:

Необходимо построить правило (статистический критерий - \(test\)),
который позволяет заключить, согласуется ли наблюдение \(X\)
с \(H_0\) или нет.

\underline{Правило.}

Выберем в множестве значений \(x\) вектора \(X\) (у нас либо
\(x = \R^n\), либо \(x = N_p \subseteq \R^n\) - носитель
плотности) подмножество \(S\). Если \(X \in S\), то \(H_0\) отвергается и
принимается \(H_1\). Если \(X \in \overline{S} = X \backslash S\), то
\(H_0\) принимается.

\begin{definition}
    Множество \(S\) называется критическим множеством или критерием,
    \(\overline{S}\) - область принятия гипотезы.
\end{definition}

\begin{definition}
    \defin{Ошибка 1-го рода} - принять \(H_1\), когда
    верна \(H_0\). Вероятность ошибки 1-го рода $\alpha
    = \P(H_1 | H_0)$ (это условная запись, а не условная вероятность).
    \defin{Ошибка 2-го рода} - принять \(H_0\), когда
    верна \(H_1\). Вероятность ошибки 2-го рода $\beta
    = \P(H_0 | H_1)$.
\end{definition}

\begin{definition}
    \defin{Мощность критерия \(S\)} называется функция \(W(S, \theta) = W(\theta)
    \defeq\P_\theta(X\in S)\) (вероятность отвергнуть \(H_0\), когда
    значение параметра есть \(\theta\)).
\end{definition}

Тогда
\begin{align*}
    \alpha &= \alpha(\theta) = W(\theta),\ \theta\in\Theta_0; \\
    \beta  &= \beta(\theta) = 1 - W(\theta),\ \theta\in\Theta_1
\end{align*}

\begin{definition}
Обычно \(H_0\) более важна. Поэтому рассматривают критерии
такие, что
\[\alpha(\theta) = W(\theta) = \P_{\theta}(X\in S) \leq\alpha \ \forall \theta\in\Theta_0\]
    Число \(\alpha\) называют \defin{уровнем значимости критерия}.
    Пишут \(S_\alpha\) - критерий уровня \(\alpha\). Обычно \(\alpha\) -
    маленькое число, которое мы задаем сами.
\end{definition}

\begin{definition}
    Если критерий \(S^*_\alpha \in \{S_\alpha\}\) и \(\forall\theta\in\Theta_1\) и
    \(\forall S_\alpha \ W(S^*_\alpha,\theta) \geq W(S_\alpha, \theta)\),
    то критерий \(S^*_\alpha\) называется \defin{РНМ-критерием (равномерно наиболее мощным)}.
\end{definition}

Если \(H_0:\theta = \theta_0,\ H_1:\theta = \theta_1\) (то есть
\(H_0\) и \(H_1\) - простые), то задача отыскания РНМ-критерия
заданного уровня \(\alpha\) имеет вид:
\begin{align*}
  \P_{\theta_0}(X\in S^*_\alpha) &\leq \alpha, \\
  \P_{\theta_1}(X\in S^*_\alpha) &\geq \P_{\theta_1}(X\in S_\alpha) \ \forall S_\alpha
\end{align*}

Положим для краткости:
\(p_0(x)\defeq p(x, \theta_0),\ \E_0 = \E_{\theta_0},\ p_{1}(x) = p(x, \theta_1),\ \E_1 = \E_{\theta_1}\)

Введем множество
\[S(\lambda) = \{x: p_1(x) - \lambda p_0(x) > 0\}, \lambda > 0\]

\begin{theorem}[Лемма Неймана-Пирсона]
    \label{th::lemma_N_P}
    Пусть для некоторого \(\lambda > 0\) и критерия \(R\)
    (когда \(X\) попадает в \(R\), то \(H_0\) отвергается)
    выполнено:
    \begin{enumerate}
        \item  \(\P_0(X\in R) \leq \P_0(X\in S(\lambda))\)

        Тогда:
        \item  \(P_1(X\in R) \leq \P_1(X\in S(\lambda))\)
        \item  \(P_1(X\in S(\lambda)) \geq \P_0(X\in S(\lambda))\)
    \end{enumerate}
\end{theorem}
\begin{remark}
    \(X\in S(\lambda) \Leftrightarrow \frac{p_1(x)}{p_0(x)} > \lambda\).
    Так как \(p_1(X)\) и \(p_0(X)\) - правдоподобие, то критерий
    называется критерием отношения правдоподобия Неймана-Пирсона.
\end{remark}
\begin{remark}
    Утверждение 3 для \(S(\lambda)\)
    означает, что
    \[\P(H_1 \left\lvert  H_1) \geq \P(H_1 \right\rvert H_0) \Leftrightarrow W(S(\lambda), \theta_1) \geq W(S(\lambda), \theta_0)\]
    Это свойство назыается несмещенностью критерия \(S(\lambda)\)
\end{remark}
\begin{proof}
    Дальше для краткости \(S(\lambda) = S\). Пусть
    \(I_R(x) = \begin{cases}
        1, x\in \R \\
        0, x\notin \R
    \end{cases}\), \(I_S(x)\) определяем аналогично.
    Тогда Условие (А) имеет вид:
    \begin{equation}
        \label{eq::cond_A}
        \E_0I_R(x) \leq \E_0I_S(x)
    \end{equation}

    \underline{Докажем пункт 2}:
    Верно неравенство
    \begin{equation} \label{eq::LemmaNP::dens}
        I_R(x)[p_1(x) - \lambda p_0(x)] \leq I_S(x)[p_1(x) - \lambda p_0(x)]
    \end{equation}

    Действительно, если \((p_1(x) - \lambda p_0(x)) > 0\),
    то \(I_S(x) = 1\) и \eqref{eq::LemmaNP::dens} очевидно.

    Если же \(p_1(x) - \lambda p_0(x) \leq 0\), то правая часть
    \eqref{eq::LemmaNP::dens} есть ноль, а левая \(\leq\) нуля.

    Итак, \eqref{eq::LemmaNP::dens} верно: интегрируем это неравенство по \(x\in\R^n\):
    \[\E_1I_R(X) - \lambda\E_0I_R(X) \leq \E_1I_S(X) - \lambda\E_0I_S(X)\]
    \begin{equation} \label{eq::LemmaNP::MO}
        \E_1I_S(X) - \E_1I_R(X) \geq \lambda\underbrace{[\E_0I_S(X) - \E_0I_R(X)]}_{\geq 0 \text{ по условию \eqref{eq::cond_A}}}
    \end{equation}
    В силу \eqref{eq::cond_A}, \eqref{eq::LemmaNP::MO} и условия \(\lambda > 0\) получаем:
        \[E_1I_S(X) \geq E_1I_\R(X)\]

    \underline{Докажем пункт 3}: Пусть \(\lambda \geq 1\).
    Из определения \(S\ p_1(x) > p_0(x) \ \forall x\in S.\)
    Отсюда
    \[\P_0(X\in S) = \int_{R^n} I_S(X)p_0(x)\mu(dx) \leq \int_{R^n} I_S(X)p_1(x)\mu(dx) = \P_1(X\in S)\]
    То есть \(\P(H_1 \left\lvert  H_0) \leq \P(H_1  \right\rvert  H_1)\)

    Пусть \(\lambda < 1\). Рассмотрим \(\overline{S} = \{x: p_1(x) \leq \lambda p_0(x)\}\).
    При \(\lambda < 1\ p_1(x) < p_0(x)\) при \(x\in \overline{S}\).
    Отсюда
    \[\P_1(X\in \overline{S}) = \int_{R^n} I_{\overline{S}}(X)p_1(x)\mu(dx) \leq \int_{R^n} I_{\overline{S}}(X)p_0(x)\mu(dx) = \P_0(X\in \overline{S})\]
    То есть \(1 - \P_1(X\in S) \leq 1 - \P_0(X\in S)\), откуда
    \(\P_1(X\in S) \geq \P_0 (X\in S)\)
\end{proof}

\begin{example}
    \(X = (X_1,\ldots, X_n), \{X_i\}\) - н.о.р., \(X_1 \sim N(\theta, \sigma^2)\),
    дисперсия \(\sigma^2\) известна. Построим наиболее мощный критерий
    для проверки \(H_0: \theta = \theta_0\) против \(H_1: \theta = \theta_1\)
    (в случае \(\theta_1 > \theta_0\)). Уровень значимости возьмем \(\alpha\).
    \begin{enumerate}
        \item Имеем
        \[p_0 = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp{\left\{-\frac{1}{2\sigma^2} \sum^n_{i=1} (x_i -\theta_0)^2\right\}},\
        p_1 = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp{\left\{-\frac{1}{2\sigma^2} \sum^n_{i=1} (x_i -\theta_1)^2\right\}};\]
        \[S(\lambda) = \{x:p_1(x) - \lambda p_0(x) > 0\} \underset{\text{делим на }p_0}{\Leftrightarrow}
        \exp{\left\{\frac{1}{2\sigma^2}\sum_{i=1}^n\left[ (x_i-\theta_1)^2 -(x_i-\theta_0)^2 \right]\right\}} > \lambda\Leftrightarrow\]
        \[\Leftrightarrow \sum_{i=1}^n\left[(x_i - \theta_1)^2 - (x_i - \theta_0)^2\right] < \lambda_1 = -2\sigma^2\ln\lambda
        \underset{\text{арифметика}}{\Leftrightarrow} (\theta_0 - \theta_1)\sum_{i=1}^n x_i \leq\lambda_2 \Leftrightarrow\]
        \[\Leftrightarrow \sum_{i=1}^n x_i > \widetilde{\lambda},\ \widetilde{\lambda}(\lambda, n, \sigma^2, \theta_0, \theta_1)\]
        Итак,
        \[S(\lambda) = \left\{x: \sum_{i=1}^n x_i > \widetilde{\lambda}\right\} \text{ при некотором } \widetilde{\lambda}\]

        \item Определим \(\widetilde{\lambda} = \widetilde{\lambda}_\alpha\)
            из уравнения
            \[\alpha = \P_{\theta_0}(X \in S(\widetilde{\lambda}_\alpha)) =
            \P_{\theta_0}\left(\sum_{i=1}^n X_i > \widetilde{\lambda}_\alpha\right)\]
            Преобразуем левую сумму в стандартную Гауссовскую величину. Тогда
            \[\alpha = \P_{\theta_0}\left(\frac{1}{\sqrt{n} \sigma} \sum_{i=1}^n(X_i - \theta_0) > \frac{\widetilde{\lambda}_\alpha - n\theta_0}{\sqrt{n}\sigma}\right)=
            1 - \Phi\left(\frac{\widetilde{\lambda}_\alpha - n\theta_0}{\sqrt{\pi}\sigma}\right)\]
            так как \(\frac{1}{\sqrt{n}\sigma}\sum_{i} (X_i - \theta_0) \sim N(0, 1)\) при \(H_0\).

            Значит $\Phi\left(\frac{\widetilde{\lambda}_\alpha - n\theta_0}{\sqrt{\pi}\sigma}\right) = 1 - \alpha,\
            \left(\frac{\widetilde{\lambda}_\alpha - n\theta_0}{\sqrt{\pi}\sigma}\right) = \xi_{1-\alpha}$
             - квантиль станд. норм. закона уровня \(1 - \alpha\).
            Окончательно, \(\widetilde{\lambda}_\alpha = n\theta_0 + \sqrt{n}\sigma \xi_{1-\alpha}\)

        \item Положим \(S^*_{\alpha} = \{x: \sum_{i=1}^n x_i > \widetilde{\lambda}_\alpha\}\)
            Тогда \(\P_{\theta_0}(X\in S_\alpha^*)=\alpha,\) и:
            \[\forall S_\alpha\ \P_{\theta_0}(X\in S_\alpha)\leq\alpha = \P_{\theta_0}(X\in S_\alpha^*)\]

            Значит, выполнено условие 1 Леммы Неймана-Пирсона, и в силу
            пункта 2 этой леммы
            \[\P_{\theta_1}(X\in S_\alpha) \leq \P_{\theta_1}(X\in S_\alpha^*)\]
            То есть \(S_\alpha^*\) - наиболее мощный критерий уровня \(\alpha\).
        \end{enumerate}
        Так как \(S^*_{\alpha}\) не зависит от \(\theta_1\),
        то \(S^*_{\alpha}\) - РНМ-критерий для \(H_0: \theta = \theta_0\)
        против \(H^+_1 : \theta > \theta_1\)
        Мощность критерия \(S^*_{\alpha}\) для \(H_0\) при альт. \(H^+_1\)

        \[W(\theta, S^*_{\alpha}) = \P_\theta\left(\sum_{i=1}^nX_i > n\theta_0 + \sqrt{n}\sigma\xi_{1-\alpha}\right) = \]
        \[ = \P_\theta\left(\frac{1}{\sqrt{n}\sigma} \sum_{i=1}^n(X_i - \theta) > \frac{\sqrt{n}(\theta_0 - \theta)}{\sigma} + \xi_{1-\alpha}\right) =
        1 - \Phi\left(\xi_{1-\alpha} + \frac{\sqrt{n}(\theta - \theta_0)}{\sigma}\right)\]

        **** TODO вставить график стр 134 ****

\end{example}

\subsection*{О связи между доверительным оцениванием и проверкой гипотез}

\begin{definition}
    Случайное подмножесто \(\Theta^*=\Theta*(X,\alpha)\subseteq\Theta\)
    называется доверительным множеством уровня \(1-\alpha,\ 0<\alpha<1\),
    если
    \[\P_\theta(\theta\in\Theta^*(X,\alpha))\geq 1-\alpha\ \forall\theta\in\Theta\]
\end{definition}
\begin{theorem} \label{th::trusted_set_eq_plausibility_test}
    \begin{enumerate}
        \item Пусть \(\forall\theta_0\in\Theta\) гипотеза \(H_0:\theta=\theta_0\)
        при альтернативе \(H_1:\theta\neq\theta_0\) имеет \(S_\alpha(\theta_0)\)
        критерием уровня \(\alpha\). Пусть \(\Theta^*(x,\alpha) = \{\theta:x\in\overline{S_\alpha}(\theta)\}\).
        тогда \(\Theta^*(X,\alpha)\) - доверительное множество уровня \(1-\alpha\).
        (Если есть критерий, то можно по этому  построить доверительное множество)

        \item Если \(\Theta^*(X,\alpha)\) - доверительное множество уровня \(1-\alpha\),
        то \(\overline{S_\alpha}(\theta_0)=\{x:\theta_0\notin\Theta(x,\alpha)\}\)
        есть обрасть применения гипотезы \(H_0\) (следовательно и критерий).
    \end{enumerate}
\end{theorem}
\begin{remark} \label{hyp::accept_HO}
    Пункт 2 означает, что если \(\theta_0\) попало в доверительное множество,
    то \(H_0\) надо применять.
\end{remark}
\begin{proof}
    \[1.\ \P_\theta(\theta\in\Theta^*(X,\alpha)) = \P_\theta(X\in\overline{S_\alpha}(\theta)) = 1 - \underbrace{\P_\theta(X\in S_\alpha(\theta))}_{\leq\alpha}\geq 1-\alpha\ \forall\theta\in\Theta\]
    \[2.\ \P_{\theta_0}(X\in S_\alpha(\theta_0)) = 1-\P_{\theta_0}(X\in \overline{S_\alpha}(\theta_0))=
        1-\underbrace{\P_{\theta_0}(\theta_0\in\Theta^*(X,\alpha))}_{\geq 1-\alpha} \leq 1-(1-\alpha) = \alpha\]
\end{proof}
\begin{example}
    Пусть \(X=(X_1,\ldots,X_n)\), \(\{X_i\}\) - н.о.р. сл.в., \(X_1\sim N(0,\sigma^2),\theta\in\R^1\).
    Построим критерий для \(H_0:\theta=\theta_0\) против \(H_1:\theta\neq\theta_0\).
    Уровень значимости пусть будет \(\alpha,\ 0<\alpha<1\).

    Построим доверительное множество для \(\theta\) уровня \(1-\alpha\).
    Пусть \(\overline{X}=\frac{1}{n}\sum^n_{i=1}X_i\) - оптимальная оценка \(\theta\).
    Тогда \(\frac{\sqrt{n}(\overline{X}-\theta)}{\sigma}\sim N(0,1)\),
    \[\P_\theta\left(\left\lvert\frac{\sqrt{n}(\overline{X}-\theta)}{\sigma}\right\rvert<\xi_{1-\alpha/2}\right) = 1-\alpha\]
    \[\Phi(\xi_{1-\alpha/2})=1-\alpha/2\]
    То есть \(\Theta^*(X,\alpha)=\{\theta:\left\lvert \frac{\sqrt{n}(\overline{X}-\theta)}{\sigma}\right\rvert <\xi_{1-\alpha/2}\}\).
    В силу замечания к Теореме \ref{th::trusted_set_eq_plausibility_test}
    \(S_{\alpha}(\theta_0)=\{X:\left\lvert \frac{\sqrt{n}(\overline{X}-\theta_0)}{\sigma}\right\rvert \geq\xi_{1-\alpha}\}\)
    есть критическое множество для \(H_0\). Мощность
    \[W(\theta)=\P_\theta(X\in S_\alpha(\theta_0))=\P_\theta\left(\left\lvert \frac{\sqrt{n}(\overline{X}-\theta_0)}{\sigma}\right\rvert \geq\xi_{1-\alpha/2}\right)=
    1-\P_\theta\left(\left\lvert \frac{\sqrt{n}(\overline{X}-\theta_0)}{\sigma}\right\rvert <\xi_{1-\alpha/2}\right)=\]
    \[=1-\P\left(-\xi_{1-\alpha/2} + \frac{\sqrt{n}(\theta_0-\theta)}{\sigma} < \frac{\sqrt{n}(\overline{X}-\theta)}{\sigma} < \xi_{1-\alpha}+\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}\right)=\]
    \[=1-\left[\Phi\left(\xi_{1-\alpha/2}+\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}\right) - \Phi\left(-\xi_{1-\alpha/2}\frac{\sqrt{n}+(\theta_0-\theta)}{\sigma}\right)\right] = \]
    \[=\left[\Phi\left(\xi_{\alpha/2}+\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}\right) + \Phi\left(\xi_{\alpha/2}+\frac{\sqrt{n}+(\theta-\theta_0)}{\sigma}\right)\right]\]


    ******* TODO: вставить график стр 136*******


    При \(n\rightarrow\infty\ W(\theta)\rightarrow 1\ \forall\theta\neq\theta_0!\).
    То есть \(S_\alpha(\theta_0)\) состоятелен против любой фиксированной альтернативы.
\end{example}

\subsection{Критерий Фишера (\(F\)-критерий) в Гауссовской линейной регрессии}
\begin{definition}
        Если \(\xi\sim N(0,1),\ \eta_k\sim\chi^2(k)\), \(\xi\) и \(\eta_k\)
        независимы, а константа \(\mu\in\R^1\), то сл.в.
        \[t_k(\mu)\overset{d}{=}\frac{\xi+\mu}{\sqrt{\frac{1}{k}\eta_k}}\sim S(k,\mu)\]
        имеет нецентральное распределение Стьюдента с \(k\) степенями свободы
        и параметром нецентральности \(\mu\)
\end{definition}
\begin{definition}
    Если \(\xi_i\sim N(a_i,1),i=1,\ldots,k\), и \(\{\xi_1,\ldots,\xi_k\}\)
    независимы, а \(\Delta^2=\sum_{j=1}^{k}a_j^2\), то сл. в.
    \[\eta_k(\Delta)\overset{d}{=}\xi_1^2+\ldots+\xi_k^2\sim\chi^2(k,\Delta^2)\]
    имеет нецентральное распределение хи-квадрат с \(k\) степенями свободы
    и параметром нецентральности \(\Delta^2\)
\end{definition}
\begin{definition}
    Если \(\eta_k\sim\chi^2(k,\Delta^2),\ \nu_m\sim\chi^2(m)\), и \(\eta_k\) и \(\nu_m\)
    независимы, то сл.в.
    \[f_{k,m}(\Delta)\overset{d}{=}\frac{\frac{1}{k}\eta_k}{\frac{1}{m}\nu_m}\sim F(k,m,\Delta^2)\]
    имеет нецентральное распределение Фишера с \((k,m)\) степенями свободы и параметром
    нецентральности \(\Delta^2\)
\end{definition}
\begin{lemma} \label{delta_dependency}
    \begin{enumerate}
        \item Распределение сл.в. \(\eta_k\sim\chi^2(k,\Delta^2)\) зависит лишь
            от \(\Delta\), но не от \(a_1,\ldots,a_k\). А именно
            \[\eta_k\overset{d}{=}(z_1+\Delta)^2+z^2+\ldots+z^k \mbox{, где \((z_1,\ldots,z_k)\) - н.о.р. \(N(0,1)\) сл.в. } \]

        \item Если вектор \(\xi\in\R^k,\xi\sim N(a, \Sigma),\Sigma>0\), то
        \[\xi^T\Sigma^{-1}\xi\sim\chi^2(k,\Delta^2),\Delta^2 = a^{T}\Sigma^{-1}a\]
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item По определению \(\eta_k(\Delta)\overset{d}{=}\sum_{i=1}^k\xi^2_i\),
        где \((\xi_1,\ldots,\xi_k)\) - н.о.р. \(N(a_i,1)\) сл.в.

        Пусть \(\xi=(\xi_1,\ldots, \xi_k)^T\), ортогональная матрица
        \[C=\begin{pmatrix}
            \frac{a_1}{\Delta}& \ldots& \frac{a_k}{\Delta} \\
            \ldots & \ldots &\ldots
        \end{pmatrix},\ \nu=C\xi\]
        Тогда \(\eta_k\overset{d}{=}\left\lvert \xi \right\rvert ^2=\left\lvert \nu \right\rvert ^2\), так как \(C\) - ортог.
        Но
        \[\nu=C\begin{pmatrix}
            a_1 \\
            \vdots \\
            a_k
        \end{pmatrix} + C\overset{\circ}{\xi}=\begin{pmatrix}
            \Delta \\
            0 \\
            \vdots \\
            0
        \end{pmatrix} + Z \mbox{, где \(\overset{\circ}{\xi}=\xi-\E\xi, Z=C\overset{\circ}{\xi}\sim N(0, \E_k)\)}\]
        Итак, \(\eta_k\overset{d}{=}\left\lvert \nu \right\rvert ^2=(z_1+\Delta)^2+z_2^2+\ldots+z_k^2\)

        \item \(\xi^T\Sigma^{-1}\xi=\left\lvert \Sigma^{-1/2}\xi \right\rvert ^2\), причем \(\Sigma^{-1/2}\xi\sim N(\Sigma^{-1/2}a, \E_k)\).
        Отсюда \(\left\lvert \Sigma^{-1/2}\xi \right\rvert ^2\sim\chi^2(k,\Delta^2)\) с \(\Delta^2=\left\lvert \Sigma^{-1/2}a \right\rvert ^2=a^T\Sigma^{-1}a\)
    \end{enumerate}
\end{proof}
    \begin{lemma}
        Случайная величина \(t_k(\mu)\) обладает следующим свойством стохастической упорядоченности.
        при \(\mu_2>\mu_1\)
        \begin{equation}\label{iid::st}
            \P\left(t_k(\mu_2)>x\right) > \P\left(t_k(\mu_1)>x\right)\mbox{ при всех \(x\in\R^1\)}
        \end{equation}
        Аналогично
        \begin{equation}\label{iid::xi}
            \P(\eta_k(\Delta_2)>x) > \P(\eta_k(\Delta_1)>x), \Delta_2>\Delta_1
        \end{equation}
        \begin{equation}\label{iid::fi}
            \P(f_{k,m}(\Delta_2)>x) > \P(f_{k,m}(\Delta_1)>x), \Delta_2>\Delta_1
        \end{equation}
    \begin{leftbar}
        Нецентральные распределения Пирсона и Фишера стохастически упорядочены
        по параметру нецентральности.
    \end{leftbar}
    \end{lemma}
    \begin{proof}
        Докажем соотношение \ref{iid::st}, \ref{iid::xi} и \ref{iid::fi} доказываются
        аналогично.

        Заметим, что, если \(\xi\) и \(\eta\) - независимые случайные величины,
        и \(\E\left\lvert \phi(\xi,\eta) \right\rvert <\infty\), то
        \begin{equation} \label{eq::mo_phi}
            \E\phi(\xi,\eta)=\E\left\{\left.\E\phi(\xi,\eta)\right\vert_{\xi=\eta}\right\}
        \end{equation}
        В силу \eqref{eq::mo_phi}
        \[\P(t_k(\mu_2)>x) = \P\left(\frac{\xi+\mu_2}{\sqrt{\frac{1}{k}\eta_k}}>x\right)=\E I\left(\xi>x\sqrt{\frac{1}{k}\eta_k}-\mu_2\right)=\]
        \[=\E\left\{1-I\left(\xi\leq x\sqrt{\frac{1}{k}\eta_k}-\mu_2\right)\right\}
        =1-E\left\{EI(\xi\leq y)\bigg|_{y=x\sqrt{\frac{\eta_k}{k}}-\mu_2}\right\}=\]
        \[=1-\E\Phi\left(x\sqrt{\frac{1}{k}\eta_k}-\mu_2\right) \underset{}{>} 1 - \E\Phi\left(x\sqrt{\frac{1}{k}\eta_k}-\mu_1\right)=\P(t_k(\mu_1)>x)\]
        \[\mbox{ так как } \E\Phi\left(x\sqrt{\frac{1}{k}\eta_k}-\mu_2\right) < \E\Phi\left(x\sqrt{\frac{1}{k}\eta_k}-\mu_1\right) \mbox{ в силу возрастающей }\Phi(y)\]
    \end{proof}

    \underline{Обратимся к линейной гауссовской модели}
    \[X=Zc+\Eps\]
    \[X=(X_1,\ldots, X_n)^T\text{ - наблюдения},\ Z\text{ - \(n\times p\) матрица регрессоров \(p<n\)}\]
    \[\Eps\sim N(0, \sigma^2\E_n),\ c=(c_1,\ldots, c_p)^T\]
    \[\text{\underline{\(c\) и \(\sigma^2\) неизвестны}}\]
    Рассмотрим новый вектор \(\beta=Ac\), \(A - k\times p\) матрица, \(rkA=k, k\leq p\).

    \underline{Построим для \(\beta\) доверительное множество уровня \(1-\alpha\)}

    Пусть \(\widehat{c}_n\) - оценка наименьших квадратов (о.н.к.) для \(c\), \(\widehat{s}^2_n\) - о.н.к. для \(\sigma^2\).
    Пусть \(\widehat{\beta}_n=A\widehat{c}_n\).
    \[\widehat{c}_n\sim N(c,\sigma^2(Z^TZ)^{-1}) \Rightarrow \widehat{\beta}_n\sim N(\underbrace{Ac}_{\beta}{}, \sigma^2D)\text{, где }D = A(Z^TZ)^{-1}A^T\]

    \begin{leftbar}
    Заметим, что \(D > 0\), так как для \(\alpha\in\R^k,\alpha\neq0\),
    \[\alpha^TD\alpha=(A^t\alpha)^T(Z^TZ)^{-1}(A^T\alpha)>0, \text{ т.к. } (Z^TZ)^{-1}>0, A^T\alpha\neq0 \text{ при } rkA=k, \alpha\neq0\]
    \end{leftbar}
    В силу пункта 2 леммы \ref{delta_dependency}
    \[\frac{1}{\sigma^2}\left(\widehat{\beta}_n-\beta\right)D^{-1}\left(\widehat{\beta}_n-\beta\right)\sim \chi^2(k)\]
    так как \(\frac{(n-p)\widehat{s}^2_n}{\sigma^2}\sim\chi^2(n-p)\), \(\widehat{\beta}_n\) и \(\widehat{s}^2_n\) независимы, то
    \[f_{k,n-p}(X, \beta)\defeq\frac{\frac{1}{k}(\widehat{\beta}_n-\beta)^TD^{-1}(\widehat{\beta}_n-\beta)/\sigma^2}{\frac{1}{n-p}(n-p)\widehat{s}^2_n/\sigma^2} =
    \frac{(\widehat{\beta}_n-\beta)D^{-1}(\widehat{\beta}_n-\beta)}{k\widehat{s}^2_n}\sim F(k, n-p)\]

    Значит,
    \[\P_{\beta, \sigma^2}\left((\widehat{\beta}_n-\beta)^TD^{-1}(\widehat{\beta}_n-\beta)\leq k\widehat{s}_n^2f_{1-\alpha}(k,n-p)\right) = 1-\alpha\]
    \(f_{1-\alpha}(k.n-p)\) - квантиль уровня \(1-\alpha\ F(k,n-p)\).\\ Доверительное
    множество для \(\beta\) уровня \(1-\alpha\)
    \[\Theta^*(X, \alpha)=\left\{\beta:(\widehat{\beta}_n-\beta)^TD^{-1}(\widehat{\beta}_n-\beta)<k\widehat{s}^2_nf_{1-\alpha}(k,n-p)\right\} = \]
    \[=\left\{\beta:f_{k,n-p}(X,\beta)<f_{1-\alpha}(k,n-p)\right\}\text{ - \textbf{доверительный эллипсойд}}\]

    Рассмотрим проверку гипотезы
    \underline{$H_0:\beta=\beta_0$ против $H_1:\beta\neq\beta_0$}.
    $H_0$ называют линейной гипотезой, так как $\beta=Ac$ получается
    линейным преобразованием $c$.
    В силу замечания \ref{hyp::accept_HO} $H_0$ надо принимать, если
    $\beta_0\in\Theta^*(X,\alpha)$, то есть область принятия $H_0$:
    \[\overline{S}_\alpha(\beta_0)=\{x:f_{k,n-p}(x,\beta_0)\leq f_{1-\alpha}(k,n-p)\}\]
    То есть критическое множество (критерий уровня $\alpha$):
    \begin{equation} \label{criterion::F}
        S_\alpha(\beta_0)=\{x:f_{k,n-p}(x,\beta_0)> f_{1-\alpha}(k,n-p)\}
    \end{equation}
    Критерий \ref{criterion::F} называют \textbf{критерием Фишера} или \textbf{$F$-критерием}.
    $f_{k,n-p}(X,\beta_0)$ - статистика $F$-критерия.

    \underline{Рассмотрим поведение $F$-критерия при альтернативе $H_1$}. \\
    При $H_1$ в силу пункта 2 Леммы \ref{delta_dependency}
    \[f_{k,n-p}(X,\beta_0)=\frac{\frac{1}{k}\overbrace{(\widehat{\beta}_n-\beta)^TD^{-1}(\widehat{\beta}_n-\beta)/\sigma^2}^{\chi^2(k,\Delta^2)}}{\frac{1}{n-p}\underbrace{(n-p)\widehat{s}^2_n/\sigma^2}_{\chi^2(n-p)}}\sim F(k,n-p,\Delta^2)\]
    Параметр нецентральности
    \begin{equation}\label{eq::fi_noncentral}
        \Delta^2 = \frac{1}{\sigma^2}(\beta-\beta_0)^TD^{-1}(\beta-\beta_0)
    \end{equation}
    Мощность $F$-критерия
    \[W(\beta,S_\alpha(\beta_0))=\P_{\beta,\sigma^2}(f_{k,n-p}(X,\beta_0)>f_{1-\alpha}(k,n-p))=1-F_{k,n-p}(f_{1-\alpha}(k,n-p),\Delta^2)\]

    \underline{Свойства мощности}
    \begin{enumerate}
        \item Так как $\Delta =\Delta(\beta)=\Delta(\beta_0)>0$ при $\beta\neq\beta_0$,
        то в силу соотношения \ref{iid::fi}
        \[\P_{\beta,\sigma^2}(f_{k,n-p}(X, \beta_0)>f_{1-\alpha}(k,n-p))>\P_{\beta_0,\sigma}(f_{k,n-p}(X,\beta_0)>f_{1-\alpha}(k,n-p))=\alpha\]
        То есть при $\beta\neq\beta_0\ \P(H_1\vert H_1)>\P(H_1\vert H_0)$.
        То есть \underline{$F$-критерий несмещенный}!
        \item Мощность $W(\beta,S_\alpha(\beta_0))$ строго монотонна по $\Delta$
        из соотношения \ref{eq::fi_noncentral}
    \end{enumerate}

    \begin{example}[Определение порядка регрессии]
        $c_n^T=(\underbrace{c_{(1)n}^T}_{m-\text{вектор}}, \underbrace{c_{(2)n}^T}_{p-m-\text{вектор}}),\ 1\leq m\leq p$ \\
        $\begin{array}{c cc}
            H_0:& c_{(2)}=0    &\ (\text{порядок не больше m})\\
            H_1:& c_{(2)}\neq0 & \par
        \end{array}$

        Рассмотрим матрицу
        \[ A =
        \begin{array}{c}
        \overbrace{
            \left(
            \begin{array}{ccc:ccc}
                0 &        &   & 1 &        &    \\
                  & \ddots &   &   & \ddots &    \\
                  &        & 0 &   &        & 1  \\
            \end{array}
            \right)
            }^{p}
            \\
            \begin{array}{ccc ccc}
                 & m &   &  & & p - m \\
            \end{array}
        \end{array}
        \Rightarrow
        Ac = c_{(2)}
        \Rightarrow
        H_0 \Leftrightarrow Ac=0
        % =(\beta_0)
        \]
    \end{example}
    Пусть $\widehat{c}_n^T=(\underbrace{\widehat{c}_{(1)n}^T}_{m-\text{в-р}}, \underbrace{\widehat{c}_{(2)n}^T}_{p-m-\text{в-р}})$.
    Тогда $\widehat{\beta}_n=A\widehat{c}_n=\widehat{c}_{(2)n}$.
    \[(Z^TZ)^{-1}=\left(\begin{array}{c|c}
        B_{11} & B_{12} \\ \hline
        B_{21} & B_{22}
    \end{array}\right)\rightarrow D=A(Z^TZ)^{-1}A^T=B_{22}\Rightarrow\]
    \[\Rightarrow f_{p-m,n-p}(X,0)=\frac{\widehat{c}_{(2)n}^TB_{22}^{-1}\widehat{c}_{(2)n}}{(p-m)\widehat{s}^2_n}\underset{H_0}{\sim}F(p-m,n-p)\]
    $H_0$ отвергается, если $f_{p-m,n-p}(X,0)>f_{1-\alpha}(p-m,n-m)$,
    то есть
    \begin{equation}\label{fisher::ex::crit}
        S_{\alpha}(0)=\{x:\frac{\widehat{c}_{(2)n}^TB_{22}^{-1}\widehat{c}_{(2)n}}{(p-m)\widehat{s}^2_n}>f_{1-\alpha}(p-m,n-m)\}
    \end{equation}
    \begin{equation}\label{fisher::ex::stat}
        f_{p-m,n-p}(X,0)\underset{H_1}{\sim} F(p-m,n-p, \Delta^2),\text{ где }\Delta^2=\frac{c_{(2)}^TB_{22}^{-1}c_{(2)}}{\sigma^2}
    \end{equation}
    Критерий \ref{fisher::ex::crit} - несмещенный, то есть $\P(H_1\vert H_1)>\P(H_1\vert H_0)=\alpha$. Его мощность
    \[W(c_{(2)}, S_{\alpha}(0))=\P_{c_{(2), \sigma^2}}(f_{p-m,n-p}(X,0)>f_{1-\alpha}(p-m,n-p))=1-F_{p-m,n-p}(f_{1-\alpha}(p-m,n-p),\Delta^2)\]
    строго возрастает по $\Delta^2$. Параметр нецентральности $\Delta^2$ определен в \ref{fisher::ex::stat}.

    \begin{example}[Проверка однородности двух выборок]
        $X=(X_1,\ldots,X_m),\ Y=(Y_1,\ldots,Y_n)$ - независимые гауссовские выборки.
        То есть $\{X_i\},\ \{Y_i\}$ - н.о.р., $X_1\sim N(a, \sigma^2),\ Y_1\sim N(b, \sigma^2)$.
        Совокупность $\{X_i\}$ и $\{Y_j\}$ независимы, $m+n>2$. \par
        \underline{Дисперсии $DX_1,\ DY_1$ одинаковы ($=\sigma^2$), неизвестны, средние
        $a$ и $b$ неизвестны}.

        $\begin{array}{c cc}
            H_0:& a = b   &\ (\text{\textbf{гипотеза однородности}}) \\
            H_1:& a \neq b&
        \end{array}$

        \begin{remark*}
            При $\D X_{1}\neq\D X_2$ эта задача называется \textbf{проблемой Беренса-Фишера}.
        \end{remark*}

        \begin{equation*}
            \begin{cases}
                X_i = a+\eps_i,\ &i=1,\ldots,m,\ \eps_i=X_i-a\\
                Y_i = b+\widehat{\eps}_j,\ &j=1,\ldots,n,\ \widehat{\eps}_j=Y_j-b
            \end{cases}
            \Rightarrow
            \eps_1,\ldots,\eps_m,\widehat{\eps}_1,\ldots, \widehat{\eps}_n\text{ - н.о.р. $N(0, \sigma^2)$ сл.в.}
        \end{equation*}

        \begin{equation}
        \begin{array}{cc}
            \begin{array}{l}
                \\
                \widehat{X}\defeq(X_1,\ldots,X_m,Y_1,\ldots,Y_n)^T \\
                \\
                c=(a,b)^T \\
                \\
                \Eps^T=(\eps_1,\ldots,\eps_m,\widehat{\eps}_1,\ldots, \widehat{\eps}_n)^T \\
            \end{array} &
            Z= \left(\begin{array}{cc}
                m \begin{cases}
                    1 \\
                    \vdots \\
                    1
                \end{cases} & 0 \\
                0 & n \begin{cases}
                    1 \\
                    \vdots \\
                    1
                \end{cases}
            \end{array}\right)
        \end{array}
        \Rightarrow
        \underset{\text{\textbf{гаусс. лин. регрессия}}}{\widehat{X}=Zc+\Eps}
    \end{equation}

    Положим $A=(1, -1)$. Тогда $Ac=a-b=\beta$.

    $\begin{array}{ccc}
        H_0:& Ac=a-b=\beta=0& (=\beta_0)\\
        H_1:& Ac=a-b\neq0& (\beta\neq0)
    \end{array}$

    О.н.к. для вектора $c$ - решение задачи
    \[
        \sum_{i=1}^m(X_i-a)^2+\sum_{j=1}^m(Y_j-b)^2\rightarrow \min_{a, b}
        \Leftrightarrow \begin{cases}
            -2\sum_i(X_i-a) = 0 \\
            -2\sum_j(Y_j-a) = 0
        \end{cases}
    \]
    Решением системы является $\widehat{a}_n=\overline{X},\ \widehat{b}_m=\overline{Y}$ -
    оптимальные оценки $a$ и $b$, $\widehat{c}_n=(\overline{X}, \overline{Y})^T$ - оптимальная
    оценка для $c$. Оптимальная оценка для $\sigma^2$:
    \[ \widehat{S}^2_{m+n}=\frac{1}{m+n-2}\left[\sum_i(X_i-\overline{X})^2+\sum_j(Y_j-\overline{Y})^2\right] \]
    Тогда
    \[
    \begin{array}{l}
        \widehat{\beta}_n=A\widehat{c}_n=\overline{X}-\overline{Y} \\
        Z^TZ=\left(\begin{array}{cc}
            \overbrace{1\ \ldots\ 1}^m & 0 \\
                  0      & \underbrace{1\ \ldots\ 1}_n
        \end{array}\right)
        \left(\begin{array}{cc}
              & 1 \\
            0 & \vdots \\
              & 1 \\
            1  & \\
            \vdots & 0 \\
            1  &
        \end{array}\right) = \left(\begin{array}{cc}
            m & 0 \\
            0 & n
        \end{array}\right) \\
        D=A(Z^TZ)^{-1}A^T=
        \left(1\ -1 \right)
        \left(\begin{array}{cc}
            \frac{1}{m} & 0 \\
            0 & \frac{1}{n}
        \end{array}\right)
        \left(\begin{array}{c}
            1 \\
            -1
        \end{array}\right) = \frac{1}{n} + \frac{1}{m}
    \end{array}
    \Rightarrow
    \boxed{f_{1, m+n-2}(X,0)=\frac{(\overline{X}-\overline{Y})^2}{\left(\frac{1}{n} + \frac{1}{m}\right)\widehat{S}^2_{m+n}}}
    \]
    $F$-критерий для $H_0$ имеет вид
    \[S_{\alpha}(0)=\{x\in\R^{m+n}:f_{1,m+n-2}(x,0)>f_{1-\alpha}(1,m+n-2)\}\]
    \[f_{1,m+n-2}(X,0)\underset{H_0}{\sim}F(1,\ m+n-2)\]
    \[f_{1,m+n-2}(X,0)\underset{H_1}{\sim}F(1,\ m+n-2,\Delta^2),\]
    \[\text{ где параметр нецентральности } \Delta^2=\Delta^2(\underset{a-b}{\beta})=\frac{(a-b)^2}{\sigma^2\left(\frac{1}{n}+\frac{1}{m}\right)}\]
    \begin{enumerate}
        \item Если $\vert a-b\vert$  возрастает, то мощность $F$-теста возрастает
        \item Если $\sigma\rightarrow0$ или $\frac{1}{n}+\frac{1}{m}\rightarrow0$, то мощность возрастает
    \end{enumerate}
    \end{example}
\subsection{Критерий согласия Хи-квадрат Пирсона. Проверка простой гипотезы в схеме Бернулли.}
    Пусть проводятся $n$ независимых испытаний, и в каждом испытании возможны
    $m\geq2$ исходов $A_1,\ldots,A_m$ таких, что $A_iA_j=\emptyset,\ i\neq j,\ \sum A_i=\Omega$, тогда
    $\P(A_j)=p_j>0,\ \sum_{j=1}^mp_j=1$. Пусть $\nu=(\nu_1,\ldots,\nu_m)^T$, а $\nu_j$ -
    число появления $A_j$ в $n$ опытах, тогда $\sum_{j=1}^m\nu_j=n$.
    По вектору наблюдений $\nu$ необходимо проверить следующую гипотезу:

    $\begin{array}{cl}
        H_0:& p_j=p_j^\circ,\ j=1,\ldots,m\\
        H_1:& p_j\neq p_j^\circ\ \forall j
    \end{array}$

    \begin{remark*}
        $H_0$ - простая гипотеза, т.к. полностью определяет распределение
        вектора $\nu$.
        \[ \P(\nu_1=k_1,\ldots,\nu_m=k_m)\underset{H_0}{=}\frac{n!}{k_1!\ldots k_m!}(p_1^\circ)^{k_1}\ldots(p_m^\circ)^{k_m}\]
    \end{remark*}
    Это полиномиальное распределение $\prod(n,p_1^\circ, \ldots, p_m^\circ)$.
    Статистика Хи-квадрат Пирсона:
    \[\chi_n^2\underset{H_0}{\defeq}\sum_{j=1}^m\frac{(\nu_j-np_j^\circ)^2}{np_j^\circ}\]
    \underline{Поведение при альтернативе:} Очевидно
    \[\chi_n^2=n\sum_{j=1}^m\frac{(\nu_j/n-p_j^\circ)^2}{p_j^\circ}\]
    В силу теоремы Бернулли $\frac{\nu_j}{n}\xrightarrow{\P}p_j$.
    Поэтому
    \[\sum_{j=1}^m\frac{(\nu_j/n-p_j^\circ)^2}{p_j^\circ}\xrightarrow[\text{Т. о наслед. сход.}]{\P}\sum_{j=1}^m\frac{(p_j-p_j^\circ)^2}{p_j^\circ}\underset{H_1}{>}0\]
    Значит,
    \[\chi_n^2\xrightarrow[H_1]{\P}\infty,\ n\rightarrow\infty\]
    Поэтому большие значения $\chi_n^2$ часто свидетельсвтуют о том, что
    стоит отвергнуть $H_0$. Но насколько "большие" значения?
    \begin{named_theorem}[Теорема Пирсона]
        \[ \chi_n^2\xrightarrow[H_0]{d}\chi^2(m-1),\ n\rightarrow\infty \]
        \underline{Правило}: Если $\chi_n^2\leq\chi_{1-\alpha}(m-1)$, то принимаем $H_0$,
        иначе принимаем $H_1$.
    \end{named_theorem}
    \begin{remark*} Тогда
        \[\P(H_1\vert H_0)=\P(\chi^2_n>\chi_{1-\alpha}(m-1)\vert H_0)\rightarrow\alpha\]
        \[\P(H_0\vert H_1)=\P(\chi^2_n\leq\chi_{1-\alpha}(m-1)\vert H_1)\rightarrow0\]
        То есть
        \[\begin{cases}
            \P(H_0\vert H_0)\rightarrow1-\alpha \\
            \P(H_1\vert H_1)\rightarrow1
        \end{cases}\]
        \underline{Вероятность принять правильную гипотезу близка к единице!}
    \end{remark*}
    \begin{proof}
        Покажем, что вектор $\nu=(\nu_1,\ldots,\nu_m)^T$ асимптотически нормален, то есть
        \begin{equation}\label{th::pirson::asym_norm}
            \sqrt{n}(\nu/n-p)\xrightarrow{d}N(0,P-pp^T),\text{ где }P\defeq\begin{pmatrix}
                p_1^\circ & & 0 \\
                  & \ddots & \\
                0 & & p_m^\circ
            \end{pmatrix}
        \end{equation}
        Введем вектора $X_1,\ldots,X_n$, где $X_i=(0,\ldots,0,\frac{1}{j},0,\ldots,0)^T$,
        если в $i$-ом испытании произошло $A_j$. Тогда $\nu=\sum_{i=1}^nX_i$
        \begin{equation}\label{th::pirson::subst_nu}
            \sqrt{n}(\nu/n-p)=\sqrt{n}\sum_{i=1}^n(X_i-p)
        \end{equation}
        Здесь $\{X_i\}$ - н.о.р., $EX_1=p,\ \cov(X_1,X_1)=\E(X_1-p)(X_1-p)^T=\E X_1X_1^T-pp^T=P-pp^T$.
        Поэтому соотношение \eqref{th::pirson::asym_norm} следует из соотноешния \eqref{th::pirson::subst_nu}
        и ЦПТ. \\
        Матрица $P-pp^T$ вырождена, так как сумма ее столбцов равна нулю:
        если $e=(1,\ldots,1)^T$, то $(P-pp^T)e=p-p(p^Te)=p-p=0$

        Пусть
        \[P^{-1/2}\defeq\begin{pmatrix}
            \frac{1}{\sqrt{p_1^\circ}} & & 0 \\
              & \ddots & \\
            0 & & \frac{1}{\sqrt{p_m^\circ}}
        \end{pmatrix},\
        \xi_n\defeq\sqrt{n}P^{-1/2}(\nu/n-p)
        \]
        В силу теоремы о наследовании слабой сходимости и соотношения \eqref{th::pirson::asym_norm}
        \begin{equation} \label{th::pirson::xi_conv}
            \xi_n\xrightarrow{d}N(0, P^{-1/2}(P-pp^T)(P^{-1/2})^T)=N(0, E_m-zz^T),\text{ где } z=(\sqrt{p_1^\circ},\ldots,\sqrt{p_m^\circ})^T
        \end{equation}
        Пусть ортогональная матрица $U=\begin{pmatrix}
            \sqrt{p_1^\circ} & \ldots & \sqrt{p_m^\circ} \\
            \ldots & \ldots & \ldots
        \end{pmatrix}$. Тогда
        \[U(E_m-zz^T)U^T=E_m-(Uz)(Uz)^T=\]
        \[=\begin{pmatrix}
            1 &        & 0 \\
              & \ddots &   \\
            0 &        & 1 \\
        \end{pmatrix}-\begin{pmatrix}
            1 \\ 0 \\ \vdots \\ 0
        \end{pmatrix}\begin{pmatrix}
            1  & 0  & \ldots  & 0
        \end{pmatrix}=\begin{pmatrix}
            0 &   &        & 0 \\
              & 1 &        &   \\
              &   & \ddots &   \\
            0 &   &        & 1 \\
        \end{pmatrix}=\widetilde{\E}_1\]
        В силу \eqref{th::pirson::xi_conv} и теоремы о слабой сходимости
        \begin{equation} \label{th::pirson::uxi_conv}
            U\xi_n\xrightarrow{d}N(0,\widetilde{\E}_1) = (0,\eta_2,\ldots,\eta_m)^T
        \end{equation}
        где $\{\eta_2,\ldots,\eta_m\}$ - независимые $N(0,1)$ сл.в. Из \eqref{th::pirson::uxi_conv}
        и теоремы о наследовании слабой сходимости следует:
        \begin{equation}\label{th::pirson::absuxi_conv}
            \lvert U\xi_n\rvert^2\xrightarrow{d} \eta_2^2+\ldots+\eta_m^2\sim\chi^2(m-1)
        \end{equation}
        Осталось заметить, что
        \[\lvert U\xi_n\rvert^2=\lvert\xi_n\rvert^2=\sum_{j=1}^m\left[\frac{1}{\sqrt{p_j^\circ}}\sqrt{n}(\nu_j/n-p_j^\circ)\right]^2=\sum_{j=1}^m\frac{(\nu_j-np_j^\circ)^2}{np_j^\circ}=\chi_n^2\]
        Из этого равенства и соотноешния \eqref{th::pirson::absuxi_conv} следует теорема Пирсона.
    \end{proof}

    \begin{example}[Проверка простой гипотезы о виде функции распределения]
    $X=(X_1,\ldots,X_n)$, $\{X_i\}$ - н.о.р., $X_1\sim F(x)$.

    $\begin{array}{cl}
        H_0:& F(x)=F_0(x),\ (F_0\text{ известна})\\
        H_1:& F(x)\neq F_1(x),\ F_1(x)\neq F_0(x)
    \end{array}$

    Разобъем носитель $X_1$ на непересекающиеся отрезки $\Delta_1,\ldots,\Delta_m,\ m\geq2$ так, что
    $X_1\in\Delta_1\cup\Delta_2\cup\ldots\cup\Delta_m$
    \[p_j^\circ\defeq \P(X_1\in\Delta_j\vert H_0) =\int_{\Delta_j}dF_0(x)>0\ \forall j\]
    Тогда $\sum_{j=1}^mp_j^\circ=1$. С каждой величиной $X_i$ свяжем испытание
    с исходами $A_1,\ldots,A_m$, причем $A_j$ происходит тогда и только тогда,
    когда $X_i\in\Delta_j$. При $H_0$ $\P(A_j)=p_j^\circ$. Тогда наблюдения
    $X_1,\ldots,X_n$ порождают полиномиальную схему независимых испытаний.
    Пусть $\nu_j$-число исхода $A_j$ в этих испытаниях, то есть число
    наблюдений среди $X_1,\ldots,X_n$, попавших в $\Delta_j$.
    В силу теоремы Пирсона:
    \[\chi_n^2\defeq\sum_{j=1}^m\frac{(\nu_j-np^\circ_j)^2}{np^\circ_j}\xrightarrow[H_0]{d}\chi^2(m-1)\]

    \underline{Правило}: $H_0$ будем отвергать, если $\chi_n^2>\chi_{1-\alpha}(m-1)$. ($\alpha$ задано)
    Тогда $\P(H_1\vert H_0)\rightarrow\alpha,\ n\rightarrow\infty$.
    \[p_j\defeq \P(X_1\in\Delta_j\vert H_1)=\int_{\Delta_j}dF_1(x)\]
    Если верна $H_1$ и хоть при одном $j$ $p_j\neq p_j^\circ$, то
    $\P(H_0\vert H_1) = \P(\chi_n^2<\chi_{1-\alpha}(m-1)\vert H_1)\rightarrow 0$
    \begin{remark*}
        Если $F_0\not\equiv F_1$, но $p_j=p_j^\circ\ \forall j$,
        то $\P(H_0\vert H_1)=\P(H_0\vert H_0)\rightarrow1-\alpha\neq0$.
        Например
        **** TODO вставить график стр 152 ****

        Здесь $\P(X_1\in\Delta_1\vert H_0)=F_0(0)=\P(X_1\in\Delta_1\vert H_1)=F_1(0)$.
        Значит, и $\P(X_1\in\Delta_2\vert H_0)=1-F_0(0)=\P(X_1\in\Delta_1\vert H_1)=1-F_1(0)$.
    \end{remark*}
    \end{example}
\subsubsection*{Проверка сложной гипотезы в схеме исп. Бернулли}
    Пусть проводится $n$ независимых испытаний, искходы
    $A_1,\ldots,A_m$, $\nu=(\nu_1,\ldots,\nu_m)^T$ - вектор наблюдений.
    Пусть $H_0:\ \P(A_j)=p_j(\theta),\ \theta\in\Theta\in\R^k,\ k<m-1$.

    \underline{Условия регулярности}
    \begin{enumerate}
        \item $\sum_{j=1}^mp_j(\theta)=1,\ \theta\in\Theta$
        \item $p_j(\theta)\geq c>0\ \forall j=1,\ldots,m$ и $\exists\ \frac{\partial p_j(\theta)}{\partial \theta_l},\frac{\partial^2p_j(\theta)}{\partial\theta_l\partial\theta_r}$
        \item $rank(\underbrace{\frac{\partial p_j(\theta)}{\partial \theta_l}}_{\text{$m\times k$}})=k,\ \forall\theta\in\Theta$
    \end{enumerate}
    В качестве оценки $\theta$ при $H_0$ будем использовать мультиномиальные
    оценки максимального правдоподобия:
    \[\P(\nu_1=k_1,\ldots,\nu_m=k_m)=\frac{n!}{k_1!\ldots k_m!}p_1^{k_1}(\theta)\ldots p_m^{k_m}(\theta)\]
    логарифмического правдоподобия:
    \[L_n(\nu,\theta)=\ln\left(\frac{n!}{k_1!\ldots k_m!}\right)+\sum_{j=1}^m\nu_j\ln p_j(\theta)\]
    оценки максимального правдоподобия (мультиномиальные):
    \[L_n(\nu,\theta)\rightarrow\max_{\theta\in\Theta}\]
    \begin{named_theorem}[Теорема Фишера]
        Пусть выполнены условия регулярности, $\widehat{\theta}_n$ - мульт. о.м.п. Тогда
        \[\widehat{\chi}_n^2=\sum_{j=1}^m\frac{(\nu_j-np_j(\widehat{\theta}_n))^2}{np_j(\widehat{\theta}_n)}\xrightarrow[H_0]{d}\chi(m-k-1)\]
        \underline{Правило}: Если $\widehat{\chi}_n^2\leq X_{1-\alpha}(m-k-1)$, то принимаем $H_0$, иначе принимаем $H_1$.
        Тогда $\P(\overline{H_0}\vert H_0)\rightarrow\alpha$
    \end{named_theorem}
    \begin{example}[Проверка независимости признаков]
        Пусть объект классифицирован по двум $A$ и $B$,
        $A=\{A_1,\ldots,A_s\},\ B=\{B_1,\ldots,B_r\},\ s>1,\ r>1$.
        Проводится $n$ опытов, и пусть $\nu_{ij}$ - число объектов,
        имеющих признаки $A_iB_j$. \\
        Пусть $p_{ij}=\P(A_iB_j)$. Гипотеза независимости
        $H_0:\ p_{ij}=p_{i\bullet}p_{\bullet j}$ для положительных $p_{i\bullet}$ и $p_{\bullet j}$
        таких, что $\sum_{i=1}^sp_{i\bullet}=1,\ \sum_{j=1}^rp_{\bullet j}=1$. \\
        При $H_0$ логарифмическое правдоподобие
        \[L_n(\nu,p_{i\bullet},p_{\bullet j})=\ln\frac{n!}{\prod_{i,j}\nu_{ij}}+\sum_{i=1}^s\sum_{j=1}^r\nu_{ij}\ln(p_{i\bullet}p_{\bullet j})\]
        Максимизируя эту функцию по $p_{i\bullet},\ p_{\bullet j}$ при условиях, что $\sum_{i=1}^sp_{i\bullet}=1,\ \sum_{j=1}^rp_{\bullet j}=1$,
        находим оценки
        \[\widehat{p}_{i\bullet}=\frac{\nu_{i\bullet}}{n},\ \widehat{p}_{\bullet j}=\frac{\nu_{\bullet j}}{n},\text{ где } \nu_{i\bullet}=\sum_{j}\nu_{ij},\ \nu_{\bullet j}=\sum_{j}\nu_{ij}\]
        Статистика Хи-квадрат имеет вид
        \[\widehat{\chi}_n^2=\sum_{i=1}^s\sum_{j=1}^r\frac{(\nu_{ij}-n\widehat{p}_{i\bullet}\widehat{p}_{\bullet j})^2}{n\widehat{p}_{i\bullet}\widehat{p}_{\bullet j}}\]
        \[\widehat{\chi}_n^2\xrightarrow[H_0]{d}\chi((s-1)(r-1))\]
        так как $m-k-1=sr-(s+r-2)-1=(s-1)(r-1)$. \\
        \underline{Правило}: Если $\widehat{\chi}_n^2>\chi_{1-\alpha}((s-1)(r-1))$,
        то отвергаем $H_0$. Асимптотический уровень теста есть $\alpha$
    \end{example}
    \begin{example}[W.H.Gilby. Biometrika, 8,94]
        1725 школьников классифицировали в соответствии с их
        качеством одежды и в соответствии с умственными способностями.
        Использовали следующие градации:
        
        \[
        \begin{array}{lr}
            \begin{array}{lcl}
                A &-& \text{умственно отсталый} \\
                B &-& \text{медлительный и тупой} \\
                C &-& \text{тупой} \\
                D &-& \text{медлительный, но умный} \\
                E &-& \text{достаточно умный} \\
                F &-& \text{способный} \\
                G &-& \text{очень способный} \\
            \end{array} & 
            \boxed{H_0:\ \text{признаки независимы}}
        \end{array}
        \]
        \newpage
        \begin{table}[h!]
            \centering
            \begin{tabular}{ c|c|c|c|c|c|c|c }
                & \multicolumn{6}{|c|}{Способности} & \\ \hline
                Как одевается & A и B & C   & D   & E   & F   & G  & Сумма \\ \hline
                Очень хорошо  &  33   & 48  & 113 & 209 & 194 & 39 & 636   \\ \hline
                Хорошо        &  41   & 100 & 202 & 255 & 138 & 15 & 751   \\ \hline
                Сносно        &  39   & 58  & 70  & 61  & 33  & 4  & 256   \\ \hline
                Очень плохо   &  17   & 13  & 22  & 10  & 10  & 1  & 73    \\ \hline
                Сумма         &  130  & 219 & 407 & 535 & 375 & 59 & 1725
            \end{tabular}
        \end{table}
        Здесь $\chi_n^2=174.92>\chi_{0.999}(15)=37.697$. \par
        Здесь $15 = (s-1)(r-1) = (4-3)(6-1) \Rightarrow$ Отвергаем $H_0$
    \end{example}

\end{document}