\documentclass[12pt]{article}

%Russian-specific packages
%--------------------------------------
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
%for search in russian
% \usepackage{cmap}
%--------------------------------------

%Math-specific packages
%--------------------------------------
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{arydshln}

%Format-specific packages
%--------------------------------------
\usepackage[left=2cm,
            right=2cm,
            top=2cm,
            bottom=2cm,
            bindingoffset=0cm]{geometry}
%--------------------------------------

% for theorems, lemmas and definitions
%--------------------------------------
\usepackage{amsthm}

\counterwithin*{equation}{section}

\newtheorem{definition}{Опр.}
\counterwithin*{definition}{section}

\newtheorem{lemma}{Лемма}
\counterwithin*{lemma}{section}

\newtheorem{remark}{Замечание}
\counterwithin*{remark}{section}

\newtheorem*{remark*}{Замечание}

\newtheorem*{corollary}{Следствие}
\newtheorem*{proposition}{Предложение}
\newtheorem*{example}{Пример}

\newtheoremstyle{basic_theorem}    %<name>
                 {\topsep}   %<space above>
                 {\topsep}   %<space below>
                 {\itshape}  %<body font>
                 {}          %<indent amount>
                 {\bfseries} %Theorem head font>
                 {.}         %<punctuation after theorem head>
                 {\newline}  %<space after theorem head> (default .5em)
                 {}          %<Theorem head spec>
\theoremstyle{basic_theorem}
\newtheorem{theorem}{Теорема}
\counterwithin*{theorem}{subsection}

\newtheoremstyle{name_theorem}
                {\topsep}
                {\topsep}
                {\itshape}
                {}
                {\bfseries}
                {}
                {\newline}
                {\thmnote{#3}}
\theoremstyle{name_theorem}
\newtheorem*{named_theorem}{Теорема}
%--------------------------------------

% For images
%--------------------------------------
\usepackage{graphics}

\usepackage{epstopdf}
\epstopdfsetup{outdir=./}

\usepackage{gnuplottex}
 

%--------------------------------------

% for leftbar
\usepackage{framed}

% My commands
%--------------------------------------
\newcommand\defin[1]{\textbf{#1}}

\newcommand*{\defeq}{\stackrel{\text{def}}{=}}
\def\cov{ \mathrm{cov} }

\def\eps{ \varepsilon }
\def\Eps{ \mathcal{E} }

\def\R{ \mathbb{R} }
\def\E{ \mathrm{E} }
\def\D{ \mathrm{D} }
\def\P{ \mathrm{P} }

\def\littleO{ \overline{\overline{o}} }
%--------------------------------------

\begin{document}

\tableofcontents

\newpage

\section{Асимптотические оптимальные оценки}
    Пусть сл. векторы \(\xi_n, \xi \in \R^K\), и определены на \((\Omega, \mathcal{F}, \P)\).
Пусть функция распределения \(\xi_n\) есть \(F_n(x)\), хар. ф-ция есть \(\phi_n(t)\), а распределение
есть \(Q_n\). Для вектора \(\xi\) функцию распределения, хар. ф-цию и распреденеие обозначим \(F(x)\),
\(\phi(t),\ Q\) соответственно.

\begin{definition}
    Функция распределения \(F_n(x)\) сходится к \(F(x)\) при \(n \rightarrow \infty\) в основном
    (пишем \(F_n(x) \Rightarrow F\)), если \(F_n(x) \rightarrow F(x) \ \forall x \in C(F)\)
\end{definition}

\begin{definition}
    Распределение \(Q_n\) сходится к распределению \(Q\) слабо (пишем \(Q_n \xrightarrow{w} Q\)),
    если \(\forall\) непреревной и ограниченной \(g: \R^K \rightarrow \R^1\)
    \[ \int_{\R^K} g(x)Q_n(dx) \rightarrow \int_{\R^K} g(x)Q(dx)\]
    или, эквивалентно, \(\E g(\xi_n) \rightarrow \E g(\xi)\).
\end{definition}

\begin{theorem}
    Следующие условия эквивалентны:
    \begin{enumerate}
        \item \(F_n(x) \Rightarrow F\)
        \item \(Q_n \xrightarrow{w} Q\)
        \item \(\phi_n(t) \rightarrow \phi \ \forall t \in \R^K\)
    \end{enumerate}
    Если выполненое любое из условий \(1 - 3\), будем писать
    \(\xi_n \xrightarrow{d} \xi\) и говорить, что \(\xi_n\) сходится к \(\xi\) по распределению.
\end{theorem}

\begin{theorem}[О наследовании сходимости]
\label{th::inherit_conv}
    Пусть сл. векторы \(\xi_n, \xi \in \R^K, H: \R^K \rightarrow \R^1\)
    Тогда:
    \begin{enumerate}
        \item Если \(\xi_n \xrightarrow{d} \xi\), то \(H(\xi_n) \xrightarrow{d} H(\xi)\)
        \item Если \(\xi_n \xrightarrow{\P} \xi\), то \(H(\xi_n) \xrightarrow{\P} H(\xi)\)
    \end{enumerate}
\end{theorem}

\begin{named_theorem}[Лемма Слуцкого]
\label{th::slut}
    Пусть \(\xi_n, \xi, \eta_n, a \in \R^1, \xi_n \xrightarrow{d} \xi\), а \(\eta_n \xrightarrow{\P} a\).
    Тогда:
    \begin{enumerate}
        \item \label{th::slut:sum} \(\xi_n + \eta_n \xrightarrow{d} \xi + a\)
        \item \label{th::slut:mult} \(\xi_n \eta_n \xrightarrow{d} a\xi\)
    \end{enumerate}
\end{named_theorem}
\begin{proof}
    Достаточно показать, что вектор
    \begin{equation} \label{eq::LemmaSlut}
        (\xi_n, \eta_n)^T \xrightarrow{d} (\xi, a)^T
    \end{equation}
    Действительно, если \eqref{eq::LemmaSlut} верно, то при \(H(x, y) = x + y\) в силу Теоремы \ref{th::inherit_conv} получаем пункт \ref{th::slut:sum} леммы,
    а при \(H(x, y) = xy\) -  пункт \ref{th::slut:mult}.

    Для доказательства \ref{th::slut:sum}, проверим, что хар. ф-ция
    вектора \((\xi_n, \eta_n)^T\) сходится к хар. функции вектора \((\xi, \eta)^T\).
    Имеем:
    \[\left\lvert\E e^{it\xi_n + is\eta_n} - \E e^{it\xi + isa}\right\rvert \leq \left\lvert \E e^{it\xi_n + is\eta_n} - \E e^{it\xi_n + isa}\right\rvert + \left\lvert \E e^{it\xi_n + isa} - \E e^{it\xi + isa}\right\rvert = \alpha_n + \beta_n\]

    \[\alpha_n \leq \E \left\lvert e^{it\xi_n}(e^{it\eta_n + isa}) \right\rvert  = \E \left\lvert e^{it\eta_n + isa} \right\rvert  = \E g(\eta_n), \ g(x)\defeq\left\lvert e^{isx} - e^{isa} \right\rvert \]
    Ф-ция g(x) непрерывна и ограничена, а т.к. \(\eta_n \xrightarrow{d} a\),
    то в силу Теоремы \ref{th::inherit_conv} \(\E g(\eta_n) \rightarrow \E g(a) = 0\)
    Итак, \(\alpha \rightarrow 0\).

    \[\beta_n = \left\lvert \E e^{isa}(e^{it\xi_n} - e^{it\xi}) \right\rvert  = \left\lvert e^{isa} \E (e^{it\xi_n} - e^{it\xi}) \right\rvert  = \left\lvert \E (e^{it\xi_n} - e^{it\xi}) \right\rvert  \rightarrow 0\]
    т.к. \(\xi_n \xrightarrow{d} \xi\) и \(\phi_n(t) \rightarrow \phi(t)\).
\end{proof}

Пусть наблюедние \(X \sim  P_{\theta},\ \theta \in \Theta \subseteq \R^{K}\), а \(\widehat{\theta}_n\) - оценка \(\theta\)

\begin{definition}
    Если \(\sqrt{n}(\widehat{\theta}_n - \theta) \xrightarrow{d} N(0, \Sigma(\theta)) \ \forall \theta \in \Theta\)
    и ковариционная матрица \(0 < \Sigma(\theta) < \infty\), то \(\widehat{\theta}_n\) называется асимптотической нормальной оценкой.
\end{definition}

\begin{definition}
    Если \(\widehat{\theta}_n \xrightarrow{\P} \theta \ \forall \theta \in \Theta\), то \(\widehat{\theta}_n\) называется состоятельной оценкой.
\end{definition}

\begin{remark}
    Дальше \(\theta \in \Theta \subseteq \R^1\), то есть \(\theta\) и \(\widehat{\theta}_n\) - скаляры.
\end{remark}

Если \(\widehat{\theta}_n\) - состоятельная оценка \(\theta\), то при больших и \(\widehat{\theta}_n \approx \theta\) с вероятностью, близкой к единице.

Если \(\widehat{\theta}_n\) - асимптотическая нормальная оценка \(\theta\) (так как \(\theta\) и \(\widehat{\theta}_n\) скаляры:

\(\sqrt{n}(\widehat{\theta}_n - \theta) \xrightarrow{d} N(0, \sigma^2(\theta)) \ 0 < \sigma^2 < \infty,\ \forall \theta \in \Theta\)), то: \begin{enumerate}

    \item \(\widehat{\theta}_n\) - состоятельная оценка \(\theta\), так как \(\widehat{\theta}_n - \theta = n^{-1/2} \sqrt{n}(\widehat{\theta}_n - \theta) \xrightarrow{\P} 0\)
        в силу п. \ref{th::slut:mult} леммы Слуцкого.
    \item Скорость сходимости \(\widehat{\theta}_n\) к \(\theta\) есть \(O(\sqrt{n})\)
    \item При больших \(n\) со сл. в. \(\sqrt{n}(\widehat{\theta}_n - \theta)\) можно обращаться (с осторожностью!) как с Гауссовской величиной.

    Например, пусть дисперсия предельного Гауссовского закона \(\sigma^2(\theta)\) будет непреревной ф-цией \(\theta\). Тогда
        \[ \frac{\sqrt{n}(\widehat{\theta}_n - \theta)}{\sigma(\widehat{\theta}_n)} =
        \underbrace{\frac{\sqrt{n}(\widehat{\theta}_n - \theta)}{\sigma(\theta)}}_{\xrightarrow{d} N(0, 1)}
        \underbrace{\frac{\sigma(\theta)}{\sigma(\widehat{\theta}_n)}}_{\xrightarrow{\P} 1} \xrightarrow{d} \eta \sim N(0, 1)\]
        в силу п. 2 леммы Слуцкого. Значит,
        \[\P_\theta \left(\left\lvert \frac{\sqrt{n}(\widehat{\theta}_n - \theta)}{\sigma(\widehat{\theta}_n)} \right\rvert  < \xi_{1 - \alpha/2}\right) \rightarrow \P(\left\lvert \eta \right\rvert  < \xi_{1 - \alpha/2}) = 1 - \alpha\]

        То есть примерно с вероятностью \(1 - \alpha\) выполнено неравенство, или эквивалентно раскроем по модулю
        \[\underbrace{\widehat{\theta}_n - n^{-1/2}\sigma(\widehat{\theta}_n)\xi_{1 - \alpha /2} < \theta < \widehat{\theta}_n + n^{-1/2}\sigma(\widehat{\theta}_n)\xi_{1 - \alpha /2}}_{\mbox{Асимптотический доверительный интервал уровня \(1 - \alpha\)}}\]

    \item Асимптотические Гауссовские оценки можно сравнивать между собой: \\
    Если \(\sqrt{n} (\widehat{\theta}_{i,n} - \theta) \xrightarrow{d} N(0, \sigma^2_{i}(\theta)),\ i = 1, 2, \ldots\), то
    можно посчитать асимптотическую относительную эффективность (АОЭ):
    \[e_{1,2} = \frac{\sigma_2^2(\theta)}{\sigma_1^2(\theta)}\]
    Напомним, $e_{1, 2} = \lim_{n \to \infty} \frac{n'(x)}{n (x)}, \mbox{ где }
    \sqrt{n}(\widehat{\theta}_{1,n} - \theta) \xrightarrow{d} N(0, \sigma_1^2(\theta))$
    и \(\sqrt{n}(\widehat{\theta}_{2,n'} - \theta) \xrightarrow{d} N(0, \sigma_1^2(\theta))\).
\end{enumerate}

    Вопрос: Есть ли такая оценка \(\theta^*_n\), что АОЭ \(e_{\theta^*_n, \widehat{\theta}_n}(\theta) \geq 1 \ \forall \widehat{\theta}_n\)
    и всех \(\theta \in \Theta\), то есть эффективнее всех остальных?

    Если да, то \(\theta^*_n\) требует не больше наблюдений, чем любая \(\widehat{\theta}_n\), чтобы достичь одинаковой с \(\widehat{\theta}_n\) точности.
    Ясно, что пределеная дисперсия \(\sqrt{n}(\theta^*_n - \theta)\) должна быть не больше асимптотической дисперсии
    \(\sqrt{n}(\widehat{\theta}_n - \theta)\) для любой асимптотической Гауссовской оценки \(\widehat{\theta}_n\). Но
    какова самая маленькая асимптотическая дисперсия у \(\sqrt{n}(\widehat{\theta}_n - \theta)\)?

    \begin{named_theorem}[Теорема Бахадура]
        \label{th::bahadur}
        Пусть \(X_1, \ldots, X_n\) - н. о. р. сл. в., \(X_1\) имеет
        плотность вероятности \(f(x, \theta),\ \theta \in \Theta \subseteq \R^1\),
        по мере \(\nu\). Пусть выполнены следующие условия:
        \begin{enumerate}
            \item \(\Theta\) - интервал.
            \item Носитель \(N_f = \{x: f(x, \theta) > 0\}\) не зависит от \(\theta\).
            \item \label{th::bahadur:density} \(\forall x \in N_f\) плотность \(f(x, \theta)\) дважды непрерывно
                дифференцируема по \(\theta\)
            \item \label{th::bahadur:integral} Интеграл \(\int f(x, \theta)\nu(dx)\)  можно
                дважды дифференцировать по \(\theta\), внося знак
                дифференцирования под знак интеграла.
            \item Информация Фишера \(0 < i(\theta) < \infty \ \forall \theta \in \Theta\)
            \item \label{th::bahadur:second_partial} \(\left\lvert \frac{\partial^2}{\partial \theta^2} \ln(f(x, \theta)) \right\rvert  \leq M(x) \ \forall x \in N_f, \ \theta \in \Theta, \ \E_\theta M(X_1) < \infty\)
        \end{enumerate}
        Тогда, если \(\sqrt{n}(\widehat{\theta}_n - \theta) \xrightarrow{d} N(0, \sigma^2(\theta))\),
        то \(\sigma^2(\theta) \geq \frac{1}{i(\theta)}\) всюду за исключением
        множества Лебеговой меры нуль.
    \end{named_theorem}
    \begin{remark}
        Если вдобавок \(\sigma^2(\theta)\) и \(i(\theta)\) непрерывны,
        то \(\sigma^2(\theta) \geq \frac{1}{i(\theta)}\) при всех \(\theta \in \Theta\).
    \end{remark}
    \begin{proof}
        Без доказательства.
    \end{proof}

    \begin{definition}
        Если \(\theta, \widehat{\theta}_n \in \R^1\) и $\sqrt{n}(\widehat{\theta}_n - \theta)\xrightarrow{d} N(0, \frac{1}{i(\theta)}),
        \ n \rightarrow \infty, \ \forall \theta \in \Theta,$
        причем \(0 < i(\theta) < \infty\), то \(\widehat{\theta}_n\) называется \defin{асимптотически
        эффективной оценкой}.
    \end{definition}
    Вопрос: Вообще можно ли найти такую оценку \(\widehat{\theta}_n\)? Да

    \newpage

    Дальше \(X = (X_1, \ldots, X_n), \ X \sim \P_\theta,\ \theta \in \Theta \subseteq \R^1\).
    \underline{Условие (A)}:
    \begin{enumerate}
        \item \(\Theta\) - интервал, \(\P_{\theta_1} \neq P_{\theta_2}\) при \(\theta_1 \neq \theta_2\).
        \item \(X_1, \ldots, X_n\) - независимые одинаково распределенные случайные величины
        \item \(X_1\) имеет плотность вероятности \(f(x, \theta)\) по мере \(\nu\)
        \item Носитель \(N_f = \{x: f(x, \theta) > 0\}\) не зависит от \(\theta\).
        \item Плотность вектора \(X\) есть \(p(x, \theta) = \prod_{i=1}^n f(x_i, \theta)\).
    \end{enumerate}
    \begin{definition}
    Функция \(p(X, \theta)\) как функция \(\theta\) при фиксированном \(X\) называется
    \defin{правдоподобием} функции.
    \[L_n(X, \theta) = \ln p(X, \theta) = \sum_{i=1}^n \ln f(X_i, \theta)\]
    называется логарифмическим правдоподобием.
    \end{definition}

    Пусть \(\theta_0\) будет истинное значение параметра.
    \begin{lemma}[Неравенство Йенсена]
        Пусть \(g(x)\) выпукла книзу борелевская функция, \(\E\left\lvert \xi \right\rvert  <\infty\),
        \(\E\left\lvert g(\xi) \right\rvert  <\infty\). Тогда \(g(\E\xi) \leq \E g(\xi)\). Если \(\xi\)
        не является почти наверное константой и \(g\) строго выпукла, то неравенство строгое.
    \end{lemma}
    \begin{theorem}[Экстремальное свойство правдоподобия]
        \label{th::extr_plausibility}
        Пусть выполнено Условие (A). Пусть \(E_{\theta_0} \left\lvert \ln f(X_1, \theta) \right\rvert  < \infty,\ \forall \theta \in \Theta\).
        Тогда
        \[\P_{\theta_0}(p(X, \theta_0) > p(X, \theta)) \rightarrow 1,\ n\rightarrow \infty,\ \theta_0 \neq \theta\]
    \end{theorem}
    \begin{proof}
        \[p(X, \theta_0) > p(X, \theta) \Leftrightarrow \ln p(X, \theta_0) > \ln p(X, \theta) \Leftrightarrow\]
        \[\eta_n\defeq n^{-1} \sum_{i=1}^n \ln \left(\frac{f(X_i, \theta)}{f(X_i, \theta_0)}\right) < 0\]
    То есть надо показать, что \(\P_{\theta_0}(\eta_n < 0) \rightarrow 1\). Но по слабому закону больших чисел:
    \[\eta_n = n^{-1}\sum \ln \left(\frac{f(X_i, \theta)}{f(X_i, \theta_0)} \right) \xrightarrow{\P}
    E_{\theta_0}\ln \left(\frac{f(X_1, \theta)}{f(X_1, \theta_0)} \right) \]

    Возьмем функцию \(-\ln x\) - строго выпукла вниз и \(\frac{f(X_1, \theta)}{f(X_1, \theta_0)}\)
    не является п.н. константой (так как иначе если плотности п.н. совпадают,
    то и распределения при разных значениях совпадают, что противоречит Условию(A)(1)).

    В силу неравенства Йенсена:
    \[\E_{\theta_0} \ln \frac{f(X_1, \theta)}{f(X_1, \theta_0)} < \ln \E_{\theta_0} \frac{f(X_1, \theta)}{f(X_1, \theta_0)} = \ln \int_{N_f} \frac{f(x, \theta)}{f(x, \theta_0)} f(x, \theta_0) \nu(dx) = \ln1 = 0\]
    Но если \(\eta_n\) сходится по вероятности к отрицательному числу, то \(P_{\theta_0}(\eta_n < 0) \rightarrow 1\)
\end{proof}
    В силу теоремы \ref{th::extr_plausibility} естественно брать
    оценкой то значение \(\theta\), которое максимизирует \(p(X, \theta)\) при данном \(X\)

    \begin{definition}
        Случайная величина \(\widehat{\theta}_n \in \Theta\) называется
        \defin{оценкой максимального правдоподобия (о.м.п.)}, если
        \(p(X, \widehat{\theta}_n) = \max_{\theta\in\Theta} p(X, \theta)\),
        или эквивалентно \(L_n(X, \widehat{\theta}_n) = max_{\theta\in\Theta} L_n(X, \theta)\)
    \end{definition}
    Итак, о.м.п \(\widehat{\theta}_n = \arg\max_{\theta\in\Theta} L_n(X, \theta)\).

    Если в \(\forall\theta\in\Theta\) максимум не достигается, то о.м.п. не существует.

    Если \(\Theta\) - интервал, \(L_n(X, \theta)\) - гладкая по \(\theta\) функция,
    то \(\theta\) удовлетворяет уравнению правдоподобия
    \begin{equation} \label{eq::plausibility}
        \frac{\partial}{\partial\theta}L_n(X, \theta) = 0
    \end{equation}
    \begin{theorem}[О состоятельности решения уравнения правдоподобия]
        \label{th::consist_plausibility}
        Пусть выполнено Условие (А). Пусть \(\forall x \in N_f \ \exists\) непрерывная
        производная \(f'_{\theta}(x, \theta)\). Тогда уравнение \eqref{eq::plausibility}
        с вероятностью, стремящейся к 1 при \(n\rightarrow \infty\) имеет решение \(\in\Theta\).
        При этом среди всех такиъ решений есть такой корень \(\widehat{\theta}_n\), что он
        является состоятельнаой оценкой \(\theta_0\)
    \end{theorem}

    \begin{proof}
        Пусть \(S_n = \{\omega\}\), при которых уравнение \eqref{eq::plausibility} имеет
        решение для \(\theta\in\Theta\). Тогда теорема \ref{th::consist_plausibility} утверждает:
        \begin{enumerate}
            \item \(P_{\theta_0}(S_n) \rightarrow 1\).
            \item Существует такое решение \(\widehat{\theta}_n \in \Theta\), что
                \[P_{\theta_0} \left(\left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps, S_n\right) \rightarrow 1,\ n\rightarrow\infty,\ \forall\eps>0\]
        \end{enumerate}
        \underline{Докажем пункт 1}: Выберем малое \(a>0\) так, что на \((\theta_0 - a, \theta_0 + a) \subseteq\Theta\). Пусть
        \[S^a_n = \{\omega: L_n(X, \theta_0) > L_n(X, \theta_0 - a), L_n(X, \theta_0) > L_n(X, \theta_0 + a)\}\]
        В силу теоремы \ref{th::extr_plausibility} \(\P_{\theta_0}(S_n^a) \rightarrow 1\)

        При \(\omega\in S_n^a\) функция \(L_n(X, \theta)\) имеет
        локальный максимум \(\widehat{\theta}^a_n\) на интервале \((\theta_0 - a, \theta_0 + a)\)
        % TODO
        \begin{figure}[h!]
            \centering 
            \begin{gnuplot}[terminal=epslatex, scale=0.8]
                set xlabel '$\theta$'
                set border 1
                set xrange [-2:8]
                set yrange [0:7]
                set yzeroaxis ls -1
                unset ytics

                set xtics ("$\\theta_0-a$" -1, 0, "$\\theta_0$" 3, "$\\widehat{\\theta}^a_n$" 4, "$\\theta_0+a$" 7)
                set arrow from -1,0 to -1,2 nohead dashtype 3
                set arrow from 3,0 to 3,4.5 nohead dashtype 3
                set arrow from 4,0 to 4,6 nohead dashtype 3
                set arrow from 7,0 to 7,1 nohead dashtype 3
                plot "data/plaus_func.txt" smooth csplines with lines title '$L_n(X,\theta)$'
            \end{gnuplot}
        \end{figure}

        Значит, \(\frac{\partial}{\partial\theta}L_n(X, \widehat{\theta}_n^a) = 0\).
        Тогда \(\P_{\theta_0}(S_n) \geq \P_{\theta_0}(S_n^a) \rightarrow 1\), так
        как \(S_n^a \subseteq S_n\), и пункт 1 доказан.

        \underline{Докажем пункт 2}: \(\forall n\) при \(\omega\in S_n\) может сущестовать целое множество корней
        \(\{\theta^*_n\}\). Выберем в этом множестве корень \(\widehat{\theta}_n\),
        ближайший к \(\theta_0\). Это можно сделать, так как
        функция \(\frac{\partial}{\partial\theta} L_n(x, \theta)\) непрерывна по \(\theta\),
        и последовательность корней есть корень. Этот корень \(\widehat{\theta}_n\)
        и есть состоятельная оценка \(\theta\). Покажем это:

        \(\forall \text{ малого } \eps > 0\):
        \begin{equation}
            \label{eq::S_n}
            \P_{\theta_0}(\left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps, S_n) \geq
            \P_{\theta_0}(\left\lvert \widehat{\theta}^\eps_n - \theta_0 \right\rvert  < \eps, S_n^\eps)
        \end{equation}
        Так как $S^\eps_n \subseteq S_n,\
        (\omega: \left\lvert \widehat{\theta}^\eps_n - \theta_0 \right\rvert  < \eps) \subseteq
        (\omega: \left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps)$

        Но $\P_{\theta_0}(\left\lvert \widehat{\theta}^\eps_n - \theta_0 \right\rvert  < \eps, S^\eps_n)
        \underset{\text{т.к. события из } S_n^\eps \text{ лежат в } \left\lvert \widehat{\theta}^\eps_n - \theta_0 \right\rvert  < \eps}{=}
        \P_{\theta_0}(S_n^\eps) \rightarrow 1$, значит в силу \eqref{eq::S_n}
        \[\P_{\theta_0} (\left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps, S_n) \rightarrow 1\]
    \end{proof}

    \begin{remark}
        Пусть
        \[\theta^*_n = \begin{cases}
            \text{сост. корню уравнения правдоподобия, если он сущ.} \\
            \theta',\ \theta'\in\Theta, \text{иначе}
        \end{cases}\]
        Тогда случайная величина \(\theta^*_n\) всегда определена, и
        \(\theta^*_n \xrightarrow{\P} \theta_0\), так как
        \[\P(\left\lvert \theta^*_n - \theta_0 \right\rvert  < \eps) =
        \P(\left\lvert \widehat{\theta}_n - \theta_0 \right\rvert  < \eps, S_n) +
        \P(\left\lvert \theta' - \theta_0 \right\rvert  < \eps, \overline{S}_n) \rightarrow 1\]
        Ясно, что
        \begin{equation}
            \frac{\partial}{\partial\theta} L_n(X, \theta^*_n) = \littleO_p(1)
        \end{equation}
        Так как производная отлична от нуля только на \(\overline{S}_n\).

        Будем называть \(\theta_n^*\) \defin{обобщенным состоятельным корнем уравнения
        правдоподобия}
    \end{remark}

    \begin{theorem}[Об асимптотической эффективности состоятельности решения]
        \label{th::asympt_consist}
        Пусть \(X = (X_1, \ldots, X_n),\ \{X_i\}\) - н.о.р. сл.в., и
        удовлетворяются предположения Теоремы Бахадура, в которых условия
        \ref{th::bahadur:density} и \ref{th::bahadur:second_partial} заменены на
        предположения о третьей, а не второй производной. То есть
        \[\left\lvert \frac{\partial^3}{\partial \theta^3} \ln f(x, \theta) \right\rvert  \leq M(x) \ \forall x\in N_f,\ \forall\theta\in\Theta,\ \E_{\theta_0}M(X_1) < \infty\]
        Тогда, если \(\theta^*_n\) - обобщенный состоятельный корень из теоремы \ref{th::consist_plausibility}, то
        \[\sqrt{n}(\theta^*_n - \theta_0) \xrightarrow{d} N(0, \frac{1}{i(\theta_0)})\]
        То есть \(\theta^*_n\) - асимптотическая эффективная оценка.
    \end{theorem}
    \begin{proof}
        Будем обозначать \(\frac{\partial}{\partial\theta}L_n(X, \theta), \frac{\partial^2}{\partial\theta^2}L_n(X, \theta), \ldots\)
        через \(L'_n(\theta), L^{(2)}_n(\theta), \ldots\).

        Для фиксированного \(X\) в силу формулы Тейлора и последнего замечания:
        \[\littleO_p(1) = L'_n(\theta^*_n) = L'_n(\theta_0) + L^{(2)}_n(\theta_0)(\theta^*_n - \theta_0) +
        \frac{1}{2}L_n^{(3)}(\widetilde{\theta}_n)(\theta^*_n - \theta_0)^2,\ \widetilde{\theta}_n \in(\theta_0, \theta_n^*)\]
        Отсюда,
        \begin{equation}
            \label{eq::taylor_frac}
            \sqrt{n}(\theta^*_n - \theta_0) = -\frac{n^{-1/2} L'_n(\theta_0) + \littleO_p(1)}{n^{-1}(L^{(2)}_n(\theta_0) + \frac{1}{2}L^{(3)}_n(\widetilde{\theta}_n)(\theta^*_n - \theta_0))}
        \end{equation}
        \underline{Рассмотрим числитель \eqref{eq::taylor_frac}} и покажем, что
        \begin{equation}
            \label{eq::taylor_frac::num}
            n^{-1/2}L_n'(\theta_0) = n^{-1/2}\sum_{i=1}^n \frac{f'_\theta(X_i, \theta_0)}{f(X_i, \theta_0)} \xrightarrow{d} \xi\sim N(0, i(\theta_0))
        \end{equation}
        Действительно,
        \[\E_{\theta_0}\frac{f'_{\theta_0}(X_1, \theta_0)}{f(X_i, \theta_0)} = \int_{N_f}\frac{f'_\theta(x, \theta_0)}{f(x,\theta_0)} f(x,\theta_0) \nu(dx) = 0\]
        \[\D_{\theta_0}\frac{f'_{\theta_0}(X_1, \theta_0)}{f(X_i, \theta_0)} = \E_{\theta_0}\left(\frac{\partial}{\partial\theta}\ln f(X_1, \theta_0)\right)^2 - \underbrace{\left(\E_{\theta_0}\frac{f'_{\theta_0}(X_1, \theta_0)}{f(X_i, \theta_0)}\right)^2 }_{ =\ 0} \underset{\text{по опр.}}{=} i(\theta_0)\]
        Так как \(f, f'\) - борелевские функции, то случайные величины \(\{\frac{f'_\theta(X_i, \theta_0)}{f(X_i, \theta_0)},\ i=1,\ldots,n\}\) - н.о.р.,
        соотношение \eqref{eq::taylor_frac::num} следует из Центр. пред. Теоремы.

        В силу Леммы Слуцкого числитель \eqref{eq::taylor_frac} \(\xrightarrow{\P} N(0, i(\theta_0))\)

        Теперь \underline{рассмотрим знаменатель \eqref{eq::taylor_frac}}:
        \begin{equation}
            \label{eq::taylor_frac::den}
            n^{-1}L_n^{(2)}(\theta_0) = n^{-1}\sum^n_{i=1}\left[ \frac{f^{(2)}_\theta(X_i, \theta_0)}{f(X_i, \theta_0)} - \left(\frac{f'_\theta(X_i, \theta_0)}{f(X_i, \theta_0)}\right)^2\right] \xrightarrow{\P} -i(\theta)
        \end{equation}
        Действительно, в силу ЗБЧ
        \[n^{-1}\sum^n_{i=1} \frac{f^{(2)}_\theta(X_i, \theta_0)}{f(X_i, \theta_0)} \xrightarrow{\P} \E_{\theta_0}\frac{f^{(2)}_\theta(X_1, \theta_0)}{f(X_1, \theta_0)} = \int_{N_f} \frac{f^{(2)}_\theta(x, \theta_0)}{f(x, \theta_0)} f(x, \theta_0) \nu(dx) = 0\]
        \[n^{-1}\sum^n_{i=1} \left(\frac{f'_\theta(X_i, \theta_0)}{f(X_i, \theta_0)}\right)^2 \xrightarrow{\P} E_{\theta_0} \left(\frac{\partial}{\partial\theta} \ln f(X_1, \theta_0)\right)^2 = i(\theta)\]

    Применяя лемму Слуцкого, получим \eqref{eq::taylor_frac::den}.

    Далее рассмотрим второе слагаeмое в знаменете \eqref{eq::taylor_frac}
    \begin{equation}
        \label{eq::taylor_frac::den2}
        \left\lvert \frac{1}{2n} L_n^{(3)}(\widetilde{\theta}_n)(\theta^*_n - \theta_0) \right\rvert  \leq \frac{1}{2}\left\lvert \theta_n^* - \theta_0 \right\rvert  n^{-1} \sum_{i=1}^n M(X_i) \xrightarrow[\text{л. Слуцкого}]{\P} 0
    \end{equation}

    В силу \eqref{eq::taylor_frac::den} и \eqref{eq::taylor_frac::den2} и Леммы Слуцкого
    знаменатель \eqref{eq::taylor_frac} сходится по вероятности к \(-i(\theta_0)\)

    Значит, что вся дробь \eqref{eq::taylor_frac} сходится по распределению к
    \(\frac{1}{i(\theta_0)} \xi \sim N(0, \frac{1}{i(\theta_0)})\)
\end{proof}

\subsection*{Оценки максимального правдоподобия для векторого параметра}
    Пусть \(X = (X_1, \ldots, X_n)\) - н.о.р., $X_1 \sim f(x, \theta),\ \theta\in\Theta\subseteq\R^k,\
    \Theta$ - открытое множество

    Тогда логарифмические правдоподобие имеет вид
    \[L_n(X, \theta) = \sum_{i=1}^n\ln f(X_i, \theta)\]

    Система уравнений правдоподобия
    \begin{equation*}
        \label{eq::sys_plausibility}
        \frac{\partial L_n(X, \theta)}{\partial\theta_i} = 0,\ i =1,2,\ldots,k
    \end{equation*}

    При условиях регулярности, похожих на условия теоремы \ref{th::asympt_consist},
    показыватся:
    \begin{enumerate}
        \item С вероятностью, стремящейся к единице при \(n \rightarrow \infty\),
            система уравнений \eqref{eq::sys_plausibility} имеет такое решение \(\widehat{\theta}_n\in\Theta\),
            что \(\widehat{\theta}_n\) сходится к истинному значению \(\theta_0\).
        \item Соответствующая оценка \(\theta^*_n\) асимптотически нормальна. А именно
         \[\sqrt{n}(\theta^*_n - \theta_0) \xrightarrow{d} N(0, I^{-1}(\theta_0)),\ n\rightarrow\infty\]
         Здесь \(I(\theta) > 0\) - матрица информации Фишера, то есть
         \[I(\theta) = (I_{ij}(\theta)),\ I_{ij}(\theta) = \E_\theta \left\{\frac{\partial \ln f(X, \theta)}{\partial\theta_i} \cdot \frac{\partial\ln f(X, \theta)}{\partial\theta_j}\right\}\]
    \end{enumerate}
\begin{example}
        \(X = (X_1, \ldots, X_n)\), где \(\{X_i\}\) - н.о.р., \(X_1 \sim N(0, \sigma^2),\ a < \theta < b\),
        \(a\) и \(b\) - известные конечные числа, дисперсия \(\sigma^2\) известна.
        Построим асимптотически эффективную оценку \(\theta^*_n\) для \(\theta\).

        Здесь \(p(x, \theta) = \left(\frac{1}{\sqrt{2\pi} \sigma}\right)^ne^{-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\theta)^2}\),
        значит
        \[L_n(X, \theta) = \ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n - \frac{1}{2\sigma^2}\sum_{i=1}^n(X_i-\theta)^2\]
        Уравнение правдоподобия имеет вид
        \[\frac{\partial L_n(X, \theta)}{\partial\theta} = \frac{1}{\sigma^2}\sum_{i=1}^n(X_i - \theta) = 0\]
        Его решение существует и единственно, это \(\overline{X}\), причем
        в т. \(\theta = \overline{X}\ L_n(X,\theta)\) достигает максимума,
        так как \(\frac{\partial^2 L_n(X, \overline{X})}{\partial\theta^2} = - \frac{1}{\sigma^2} < 0\)

        Таким образом, если \(a < \overline{X} < b\), то о.м.п. существует и равна \(\overline{X}\),
        в противном случае о.м.п. не существует. Если положить
        \begin{equation}
            \label{eq::ex::as_appraisal}
            \theta^*_n = \begin{cases}
                \overline{X},\ a < \overline{X} < b \\
                \frac{a+b}{2},\ \overline{X} \notin (a,b)
            \end{cases}
        \end{equation}
        То в силу теоремы \ref{th::asympt_consist} (её условия
        выполнены, проверьте сами), \(\theta^*_n\) - асимптотически эффективная оценка, то есть
        \begin{equation}
            \label{eq::ex::as_conv}
            \sqrt{n}(\theta^*_n - \theta_0) \xrightarrow{d} N(0, \sigma^2)
        \end{equation}
        Напомним, что в этой модели \(i(\theta) = \frac{1}{\sigma^2}\).
        Cправедливость \eqref{eq::ex::as_conv} c \(\theta^*_n\)
        из \eqref{eq::ex::as_appraisal} легко проверить непосредственно.
\end{example}
\begin{example}
    Если \(\Theta\) - компакт (то есть отрезок \([a, b]\)), то о.м.п. существует
    всегда, так как непрерывная функция на отрезке всегда достигает своего максимума.
    Значит значение о.м.п.
    \begin{equation*}
        \theta^*_n = \begin{cases}
            \overline{X},\ a < \overline{X} < b \\
            a,\ \overline{X} < a \\
            b,\ \overline{X} > b
        \end{cases}
    \end{equation*}
    Но на границах теряется асимптотическая Гауссовость.
\end{example}


\section{Проверка статистических гипотез}
\(X = (X_1, \ldots, X_n)\) имеет плотность вероятности \(p(X, \theta)\)
по мере \(\mu,\ \theta\in\Theta\subseteq\R^1\)

\begin{definition}
    Предположение вида \(H_0: \theta\in\Theta_0\), где
    \(\Theta_0\in\Theta\), называется параметрической гипотезой.
    Альтернатива \(H_1:\theta\in\Theta_1\), где
    \(\Theta_1\in\Theta\backslash\Theta_0\)
\end{definition}

\begin{definition}
    Если \(\Theta_0(\Theta_1)\) состоит из одной точки,
    то гипотеза \(H_0\) (альтернатива \(H_1\)) называется
    простой.
    В противном случае \(H_0(H_1)\) - сложная
\end{definition}

\underline{Постановка задачи}:

Необходимо построить правило (статистический критерий - \(test\)),
который позволяет заключить, согласуется ли наблюдение \(X\)
с \(H_0\) или нет.

\underline{Правило.}

Выберем в множестве значений \(x\) вектора \(X\) (у нас либо
\(x = \R^n\), либо \(x = N_p \subseteq \R^n\) - носитель
плотности) подмножество \(S\). Если \(X \in S\), то \(H_0\) отвергается и
принимается \(H_1\). Если \(X \in \overline{S} = X \backslash S\), то
\(H_0\) принимается.

\begin{definition}
    Множество \(S\) называется критическим множеством или критерием,
    \(\overline{S}\) - область принятия гипотезы.
\end{definition}

\begin{definition}
    \defin{Ошибка 1-го рода} - принять \(H_1\), когда
    верна \(H_0\). Вероятность ошибки 1-го рода $\alpha
    = \P(H_1 | H_0)$ (это условная запись, а не условная вероятность).
    \defin{Ошибка 2-го рода} - принять \(H_0\), когда
    верна \(H_1\). Вероятность ошибки 2-го рода $\beta
    = \P(H_0 | H_1)$.
\end{definition}

\begin{definition}
    \defin{Мощность критерия \(S\)} называется функция \(W(S, \theta) = W(\theta)
    \defeq\P_\theta(X\in S)\) (вероятность отвергнуть \(H_0\), когда
    значение параметра есть \(\theta\)).
\end{definition}

Тогда
\begin{align*}
    \alpha &= \alpha(\theta) = W(\theta),\ \theta\in\Theta_0; \\
    \beta  &= \beta(\theta) = 1 - W(\theta),\ \theta\in\Theta_1
\end{align*}

\begin{definition}
Обычно \(H_0\) более важна. Поэтому рассматривают критерии
такие, что
\[\alpha(\theta) = W(\theta) = \P_{\theta}(X\in S) \leq\alpha \ \forall \theta\in\Theta_0\]
    Число \(\alpha\) называют \defin{уровнем значимости критерия}.
    Пишут \(S_\alpha\) - критерий уровня \(\alpha\). Обычно \(\alpha\) -
    маленькое число, которое мы задаем сами.
\end{definition}

\begin{definition}
    Если критерий \(S^*_\alpha \in \{S_\alpha\}\) и \(\forall\theta\in\Theta_1\) и
    \(\forall S_\alpha \ W(S^*_\alpha,\theta) \geq W(S_\alpha, \theta)\),
    то критерий \(S^*_\alpha\) называется \defin{РНМ-критерием (равномерно наиболее мощным)}.
\end{definition}

Если \(H_0:\theta = \theta_0,\ H_1:\theta = \theta_1\) (то есть
\(H_0\) и \(H_1\) - простые), то задача отыскания РНМ-критерия
заданного уровня \(\alpha\) имеет вид:
\begin{align*}
  \P_{\theta_0}(X\in S^*_\alpha) &\leq \alpha, \\
  \P_{\theta_1}(X\in S^*_\alpha) &\geq \P_{\theta_1}(X\in S_\alpha) \ \forall S_\alpha
\end{align*}

Положим для краткости:
\(p_0(x)\defeq p(x, \theta_0),\ \E_0 = \E_{\theta_0},\ p_{1}(x) = p(x, \theta_1),\ \E_1 = \E_{\theta_1}\)

Введем множество
\[S(\lambda) = \{x: p_1(x) - \lambda p_0(x) > 0\}, \lambda > 0\]

\begin{theorem}[Лемма Неймана-Пирсона]
    \label{th::lemma_N_P}
    Пусть для некоторого \(\lambda > 0\) и критерия \(R\)
    (когда \(X\) попадает в \(R\), то \(H_0\) отвергается)
    выполнено:
    \begin{enumerate}
        \item  \(\P_0(X\in R) \leq \P_0(X\in S(\lambda))\)

        Тогда:
        \item  \(P_1(X\in R) \leq \P_1(X\in S(\lambda))\)
        \item  \(P_1(X\in S(\lambda)) \geq \P_0(X\in S(\lambda))\)
    \end{enumerate}
\end{theorem}
\begin{remark}
    \(X\in S(\lambda) \Leftrightarrow \frac{p_1(x)}{p_0(x)} > \lambda\).
    Так как \(p_1(X)\) и \(p_0(X)\) - правдоподобие, то критерий
    называется критерием отношения правдоподобия Неймана-Пирсона.
\end{remark}
\begin{remark}
    Утверждение 3 для \(S(\lambda)\)
    означает, что
    \[\P(H_1 \left\lvert  H_1) \geq \P(H_1 \right\rvert H_0) \Leftrightarrow W(S(\lambda), \theta_1) \geq W(S(\lambda), \theta_0)\]
    Это свойство назыается несмещенностью критерия \(S(\lambda)\)
\end{remark}
\begin{proof}
    Дальше для краткости \(S(\lambda) = S\). Пусть
    \(I_R(x) = \begin{cases}
        1, x\in \R \\
        0, x\notin \R
    \end{cases}\), \(I_S(x)\) определяем аналогично.
    Тогда Условие (А) имеет вид:
    \begin{equation}
        \label{eq::cond_A}
        \E_0I_R(x) \leq \E_0I_S(x)
    \end{equation}

    \underline{Докажем пункт 2}:
    Верно неравенство
    \begin{equation} \label{eq::LemmaNP::dens}
        I_R(x)[p_1(x) - \lambda p_0(x)] \leq I_S(x)[p_1(x) - \lambda p_0(x)]
    \end{equation}

    Действительно, если \((p_1(x) - \lambda p_0(x)) > 0\),
    то \(I_S(x) = 1\) и \eqref{eq::LemmaNP::dens} очевидно.

    Если же \(p_1(x) - \lambda p_0(x) \leq 0\), то правая часть
    \eqref{eq::LemmaNP::dens} есть ноль, а левая \(\leq\) нуля.

    Итак, \eqref{eq::LemmaNP::dens} верно: интегрируем это неравенство по \(x\in\R^n\):
    \[\E_1I_R(X) - \lambda\E_0I_R(X) \leq \E_1I_S(X) - \lambda\E_0I_S(X)\]
    \begin{equation} \label{eq::LemmaNP::MO}
        \E_1I_S(X) - \E_1I_R(X) \geq \lambda\underbrace{[\E_0I_S(X) - \E_0I_R(X)]}_{\geq 0 \text{ по условию \eqref{eq::cond_A}}}
    \end{equation}
    В силу \eqref{eq::cond_A}, \eqref{eq::LemmaNP::MO} и условия \(\lambda > 0\) получаем:
        \[E_1I_S(X) \geq E_1I_\R(X)\]

    \underline{Докажем пункт 3}: Пусть \(\lambda \geq 1\).
    Из определения \(S\ p_1(x) > p_0(x) \ \forall x\in S.\)
    Отсюда
    \[\P_0(X\in S) = \int_{R^n} I_S(X)p_0(x)\mu(dx) \leq \int_{R^n} I_S(X)p_1(x)\mu(dx) = \P_1(X\in S)\]
    То есть \(\P(H_1 \left\lvert  H_0) \leq \P(H_1  \right\rvert  H_1)\)

    Пусть \(\lambda < 1\). Рассмотрим \(\overline{S} = \{x: p_1(x) \leq \lambda p_0(x)\}\).
    При \(\lambda < 1\ p_1(x) < p_0(x)\) при \(x\in \overline{S}\).
    Отсюда
    \[\P_1(X\in \overline{S}) = \int_{R^n} I_{\overline{S}}(X)p_1(x)\mu(dx) \leq \int_{R^n} I_{\overline{S}}(X)p_0(x)\mu(dx) = \P_0(X\in \overline{S})\]
    То есть \(1 - \P_1(X\in S) \leq 1 - \P_0(X\in S)\), откуда
    \(\P_1(X\in S) \geq \P_0 (X\in S)\)
\end{proof}

\begin{example}
    \(X = (X_1,\ldots, X_n), \{X_i\}\) - н.о.р., \(X_1 \sim N(\theta, \sigma^2)\),
    дисперсия \(\sigma^2\) известна. Построим наиболее мощный критерий
    для проверки \(H_0: \theta = \theta_0\) против \(H_1: \theta = \theta_1\)
    (в случае \(\theta_1 > \theta_0\)). Уровень значимости возьмем \(\alpha\).
    \begin{enumerate}
        \item Имеем
        \[p_0 = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp{\left\{-\frac{1}{2\sigma^2} \sum^n_{i=1} (x_i -\theta_0)^2\right\}},\
        p_1 = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp{\left\{-\frac{1}{2\sigma^2} \sum^n_{i=1} (x_i -\theta_1)^2\right\}};\]
        \[S(\lambda) = \{x:p_1(x) - \lambda p_0(x) > 0\} \underset{\text{делим на }p_0}{\Leftrightarrow}
        \exp{\left\{\frac{1}{2\sigma^2}\sum_{i=1}^n\left[ (x_i-\theta_1)^2 -(x_i-\theta_0)^2 \right]\right\}} > \lambda\Leftrightarrow\]
        \[\Leftrightarrow \sum_{i=1}^n\left[(x_i - \theta_1)^2 - (x_i - \theta_0)^2\right] < \lambda_1 = -2\sigma^2\ln\lambda
        \underset{\text{арифметика}}{\Leftrightarrow} (\theta_0 - \theta_1)\sum_{i=1}^n x_i \leq\lambda_2 \Leftrightarrow\]
        \[\Leftrightarrow \sum_{i=1}^n x_i > \widetilde{\lambda},\ \widetilde{\lambda}(\lambda, n, \sigma^2, \theta_0, \theta_1)\]
        Итак,
        \[S(\lambda) = \left\{x: \sum_{i=1}^n x_i > \widetilde{\lambda}\right\} \text{ при некотором } \widetilde{\lambda}\]

        \item Определим \(\widetilde{\lambda} = \widetilde{\lambda}_\alpha\)
            из уравнения
            \[\alpha = \P_{\theta_0}(X \in S(\widetilde{\lambda}_\alpha)) =
            \P_{\theta_0}\left(\sum_{i=1}^n X_i > \widetilde{\lambda}_\alpha\right)\]
            Преобразуем левую сумму в стандартную Гауссовскую величину. Тогда
            \[\alpha = \P_{\theta_0}\left(\frac{1}{\sqrt{n} \sigma} \sum_{i=1}^n(X_i - \theta_0) > \frac{\widetilde{\lambda}_\alpha - n\theta_0}{\sqrt{n}\sigma}\right)=
            1 - \Phi\left(\frac{\widetilde{\lambda}_\alpha - n\theta_0}{\sqrt{\pi}\sigma}\right)\]
            так как \(\frac{1}{\sqrt{n}\sigma}\sum_{i} (X_i - \theta_0) \sim N(0, 1)\) при \(H_0\).

            Значит $\Phi\left(\frac{\widetilde{\lambda}_\alpha - n\theta_0}{\sqrt{\pi}\sigma}\right) = 1 - \alpha,\
            \left(\frac{\widetilde{\lambda}_\alpha - n\theta_0}{\sqrt{\pi}\sigma}\right) = \xi_{1-\alpha}$
             - квантиль станд. норм. закона уровня \(1 - \alpha\).
            Окончательно, \(\widetilde{\lambda}_\alpha = n\theta_0 + \sqrt{n}\sigma \xi_{1-\alpha}\)

        \item Положим \(S^*_{\alpha} = \{x: \sum_{i=1}^n x_i > \widetilde{\lambda}_\alpha\}\)
            Тогда \(\P_{\theta_0}(X\in S_\alpha^*)=\alpha,\) и:
            \[\forall S_\alpha\ \P_{\theta_0}(X\in S_\alpha)\leq\alpha = \P_{\theta_0}(X\in S_\alpha^*)\]

            Значит, выполнено условие 1 Леммы Неймана-Пирсона, и в силу
            пункта 2 этой леммы
            \[\P_{\theta_1}(X\in S_\alpha) \leq \P_{\theta_1}(X\in S_\alpha^*)\]
            То есть \(S_\alpha^*\) - наиболее мощный критерий уровня \(\alpha\).
        \end{enumerate}
        Так как \(S^*_{\alpha}\) не зависит от \(\theta_1\),
        то \(S^*_{\alpha}\) - РНМ-критерий для \(H_0: \theta = \theta_0\)
        против \(H^+_1 : \theta > \theta_1\)
        Мощность критерия \(S^*_{\alpha}\) для \(H_0\) при альт. \(H^+_1\)

        \[W(\theta, S^*_{\alpha}) = \P_\theta\left(\sum_{i=1}^nX_i > n\theta_0 + \sqrt{n}\sigma\xi_{1-\alpha}\right) = \]
        \[ = \P_\theta\left(\frac{1}{\sqrt{n}\sigma} \sum_{i=1}^n(X_i - \theta) > \frac{\sqrt{n}(\theta_0 - \theta)}{\sigma} + \xi_{1-\alpha}\right) =
        1 - \Phi\left(\xi_{1-\alpha} + \frac{\sqrt{n}(\theta - \theta_0)}{\sigma}\right)\]

        % TODO
        \begin{figure}[h]
            \centering 
            \begin{gnuplot}[terminal=epslatex, scale=0.8]
                set xrange [0:4]
                set yrange [0:1.5]
                set border 1
                set yzeroaxis ls -1
                set xlabel '$\theta$'

                f(x) = atan(x) * 2 / pi

                x1 = 0.654
                f_cropped(x) = (x >= x1-0.01) ? f(x) : NaN

                set xtics ("$\\theta_0$" x1)
                set ytics ("$\\alpha$" f(x1), 1)

                set arrow from x1,0 to x1,f(x1) nohead dashtype 3 
                set arrow from 0,f(x1) to x1,f(x1) nohead dashtype 3
                set label "0" at 0,0 offset -2,-1
                plot f_cropped(x) title '$w(\theta,S^*_\alpha)$',\
                     1 linetype 0 notitle
            \end{gnuplot}
        \end{figure}
\end{example}

\subsection*{О связи между доверительным оцениванием и проверкой гипотез}

\begin{definition}
    Случайное подмножесто \(\Theta^*=\Theta*(X,\alpha)\subseteq\Theta\)
    называется доверительным множеством уровня \(1-\alpha,\ 0<\alpha<1\),
    если
    \[\P_\theta(\theta\in\Theta^*(X,\alpha))\geq 1-\alpha\ \forall\theta\in\Theta\]
\end{definition}
\begin{theorem} \label{th::trusted_set_eq_plausibility_test}
    \begin{enumerate}
        \item Пусть \(\forall\theta_0\in\Theta\) гипотеза \(H_0:\theta=\theta_0\)
        при альтернативе \(H_1:\theta\neq\theta_0\) имеет \(S_\alpha(\theta_0)\)
        критерием уровня \(\alpha\). Пусть \(\Theta^*(x,\alpha) = \{\theta:x\in\overline{S_\alpha}(\theta)\}\).
        тогда \(\Theta^*(X,\alpha)\) - доверительное множество уровня \(1-\alpha\).
        (Если есть критерий, то можно по этому  построить доверительное множество)

        \item Если \(\Theta^*(X,\alpha)\) - доверительное множество уровня \(1-\alpha\),
        то \(\overline{S_\alpha}(\theta_0)=\{x:\theta_0\notin\Theta(x,\alpha)\}\)
        есть обрасть применения гипотезы \(H_0\) (следовательно и критерий).
    \end{enumerate}
\end{theorem}
\begin{remark} \label{hyp::accept_HO}
    Пункт 2 означает, что если \(\theta_0\) попало в доверительное множество,
    то \(H_0\) надо применять.
\end{remark}
\begin{proof}
    \[1.\ \P_\theta(\theta\in\Theta^*(X,\alpha)) = \P_\theta(X\in\overline{S_\alpha}(\theta)) = 1 - \underbrace{\P_\theta(X\in S_\alpha(\theta))}_{\leq\alpha}\geq 1-\alpha\ \forall\theta\in\Theta\]
    \[2.\ \P_{\theta_0}(X\in S_\alpha(\theta_0)) = 1-\P_{\theta_0}(X\in \overline{S_\alpha}(\theta_0))=
        1-\underbrace{\P_{\theta_0}(\theta_0\in\Theta^*(X,\alpha))}_{\geq 1-\alpha} \leq 1-(1-\alpha) = \alpha\]
\end{proof}
\begin{example}
    Пусть \(X=(X_1,\ldots,X_n)\), \(\{X_i\}\) - н.о.р. сл.в., \(X_1\sim N(0,\sigma^2),\theta\in\R^1\).
    Построим критерий для \(H_0:\theta=\theta_0\) против \(H_1:\theta\neq\theta_0\).
    Уровень значимости пусть будет \(\alpha,\ 0<\alpha<1\).

    Построим доверительное множество для \(\theta\) уровня \(1-\alpha\).
    Пусть \(\overline{X}=\frac{1}{n}\sum^n_{i=1}X_i\) - оптимальная оценка \(\theta\).
    Тогда \(\frac{\sqrt{n}(\overline{X}-\theta)}{\sigma}\sim N(0,1)\),
    \[\P_\theta\left(\left\lvert\frac{\sqrt{n}(\overline{X}-\theta)}{\sigma}\right\rvert<\xi_{1-\alpha/2}\right) = 1-\alpha\]
    \[\Phi(\xi_{1-\alpha/2})=1-\alpha/2\]
    То есть \(\Theta^*(X,\alpha)=\{\theta:\left\lvert \frac{\sqrt{n}(\overline{X}-\theta)}{\sigma}\right\rvert <\xi_{1-\alpha/2}\}\).
    В силу замечания к Теореме \ref{th::trusted_set_eq_plausibility_test}
    \(S_{\alpha}(\theta_0)=\{X:\left\lvert \frac{\sqrt{n}(\overline{X}-\theta_0)}{\sigma}\right\rvert \geq\xi_{1-\alpha}\}\)
    есть критическое множество для \(H_0\). Мощность
    \[W(\theta)=\P_\theta(X\in S_\alpha(\theta_0))=\P_\theta\left(\left\lvert \frac{\sqrt{n}(\overline{X}-\theta_0)}{\sigma}\right\rvert \geq\xi_{1-\alpha/2}\right)=
    1-\P_\theta\left(\left\lvert \frac{\sqrt{n}(\overline{X}-\theta_0)}{\sigma}\right\rvert <\xi_{1-\alpha/2}\right)=\]
    \[=1-\P\left(-\xi_{1-\alpha/2} + \frac{\sqrt{n}(\theta_0-\theta)}{\sigma} < \frac{\sqrt{n}(\overline{X}-\theta)}{\sigma} < \xi_{1-\alpha}+\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}\right)=\]
    \[=1-\left[\Phi\left(\xi_{1-\alpha/2}+\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}\right) - \Phi\left(-\xi_{1-\alpha/2}+\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}\right)\right] = \]
    \[=\left[\Phi\left(\xi_{\alpha/2}+\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}\right) + \Phi\left(\xi_{\alpha/2}+\frac{\sqrt{n}(\theta-\theta_0)}{\sigma}\right)\right]\]
    % TODO 
    \begin{figure}[h]
        \centering
        \begin{gnuplot}[scale=0.5]
            set xlabel '$\theta$'
            set border 1
            set yzeroaxis ls 1
            set xrange [-1:4]
            set yrange [0:2]

            f(x) = 1 - (exp(-3*(x-1)**2))/2

            set xtics (0, "$\\theta_0$" 1)
            set ytics (1, "$\\alpha$" f(1))
            
            plot f(x) title '$W(\theta)$', \
                1 linetype 4 notitle
        \end{gnuplot}
    \end{figure}
    При \(n\rightarrow\infty\ W(\theta)\rightarrow 1\ \forall\theta\neq\theta_0!\).
    То есть \(S_\alpha(\theta_0)\) состоятелен против любой фиксированной альтернативы.
\end{example}

\subsection{Критерий Фишера (\(F\)-критерий) в Гауссовской линейной регрессии}
\begin{definition}
        Если \(\xi\sim N(0,1),\ \eta_k\sim\chi^2(k)\), \(\xi\) и \(\eta_k\)
        независимы, а константа \(\mu\in\R^1\), то сл.в.
        \[t_k(\mu)\overset{d}{=}\frac{\xi+\mu}{\sqrt{\frac{1}{k}\eta_k}}\sim S(k,\mu)\]
        имеет нецентральное распределение Стьюдента с \(k\) степенями свободы
        и параметром нецентральности \(\mu\)
\end{definition}
\begin{definition}
    Если \(\xi_i\sim N(a_i,1),i=1,\ldots,k\), и \(\{\xi_1,\ldots,\xi_k\}\)
    независимы, а \(\Delta^2=\sum_{j=1}^{k}a_j^2\), то сл. в.
    \[ \eta_k(\Delta)\overset{d}{=}\xi_1^2+\ldots+\xi_k^2\sim\chi^2(k,\Delta^2) \]
    имеет нецентральное распределение хи-квадрат с \(k\) степенями свободы
    и параметром нецентральности \(\Delta^2\)
\end{definition}
\begin{definition}
    Если \(\eta_k\sim\chi^2(k,\Delta^2),\ \nu_m\sim\chi^2(m)\), и \(\eta_k\) и \(\nu_m\)
    независимы, то сл.в.
    \[f_{k,m}(\Delta)\overset{d}{=}\frac{\frac{1}{k}\eta_k}{\frac{1}{m}\nu_m}\sim F(k,m,\Delta^2)\]
    имеет нецентральное распределение Фишера с \((k,m)\) степенями свободы и параметром
    нецентральности \(\Delta^2\)
\end{definition}
\begin{lemma} \label{delta_dependency}
    \begin{enumerate}
        \item Распределение сл.в. \(\eta_k\sim\chi^2(k,\Delta^2)\) зависит лишь
            от \(\Delta\), но не от \(a_1,\ldots,a_k\). А именно
            \[\eta_k\overset{d}{=}(z_1+\Delta)^2+z^2+\ldots+z^k \mbox{, где \((z_1,\ldots,z_k)\) - н.о.р. \(N(0,1)\) сл.в. } \]

        \item Если вектор \(\xi\in\R^k,\xi\sim N(a, \Sigma),\Sigma>0\), то
        \[\xi^T\Sigma^{-1}\xi\sim\chi^2(k,\Delta^2),\Delta^2 = a^{T}\Sigma^{-1}a\]
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item По определению \(\eta_k(\Delta)\overset{d}{=}\sum_{i=1}^k\xi^2_i\),
        где \((\xi_1,\ldots,\xi_k)\) - н.о.р. \(N(a_i,1)\) сл.в.

        Пусть \(\xi=(\xi_1,\ldots, \xi_k)^T\), ортогональная матрица
        \[C=\begin{pmatrix}
            \frac{a_1}{\Delta}& \ldots& \frac{a_k}{\Delta} \\
            \ldots & \ldots &\ldots
        \end{pmatrix},\ \nu=C\xi\]
        Тогда \(\eta_k\overset{d}{=}\left\lvert \xi \right\rvert ^2=\left\lvert \nu \right\rvert ^2\), так как \(C\) - ортог.
        Но
        \[\nu=C\begin{pmatrix}
            a_1 \\
            \vdots \\
            a_k
        \end{pmatrix} + C\overset{\circ}{\xi}=\begin{pmatrix}
            \Delta \\
            0 \\
            \vdots \\
            0
        \end{pmatrix} + Z \mbox{, где \(\overset{\circ}{\xi}=\xi-\E\xi, Z=C\overset{\circ}{\xi}\sim N(0, \E_k)\)}\]
        Итак, \(\eta_k\overset{d}{=}\left\lvert \nu \right\rvert ^2=(z_1+\Delta)^2+z_2^2+\ldots+z_k^2\)

        \item \(\xi^T\Sigma^{-1}\xi=\left\lvert \Sigma^{-1/2}\xi \right\rvert ^2\), причем \(\Sigma^{-1/2}\xi\sim N(\Sigma^{-1/2}a, \E_k)\).
        Отсюда \(\left\lvert \Sigma^{-1/2}\xi \right\rvert ^2\sim\chi^2(k,\Delta^2)\) с \(\Delta^2=\left\lvert \Sigma^{-1/2}a \right\rvert ^2=a^T\Sigma^{-1}a\)
    \end{enumerate}
\end{proof}
\begin{lemma}
    Случайная величина \(t_k(\mu)\) обладает следующим свойством стохастической упорядоченности.
    при \(\mu_2>\mu_1\)
    \begin{equation}\label{iid::st}
        \P\left(t_k(\mu_2)>x\right) > \P\left(t_k(\mu_1)>x\right)\mbox{ при всех \(x\in\R^1\)}
    \end{equation}
    Аналогично
    \begin{equation}\label{iid::xi}
        \P(\eta_k(\Delta_2)>x) > \P(\eta_k(\Delta_1)>x), \Delta_2>\Delta_1
    \end{equation}
    \begin{equation}\label{iid::fi}
        \P(f_{k,m}(\Delta_2)>x) > \P(f_{k,m}(\Delta_1)>x), \Delta_2>\Delta_1
    \end{equation}
\begin{leftbar}
    Нецентральные распределения Пирсона и Фишера стохастически упорядочены
    по параметру нецентральности.
\end{leftbar}
\end{lemma}
\begin{proof}
    Докажем соотношение \ref{iid::st}, \ref{iid::xi} и \ref{iid::fi} доказываются
    аналогично.

    Заметим, что, если \(\xi\) и \(\eta\) - независимые случайные величины,
    и \(\E\left\lvert \phi(\xi,\eta) \right\rvert <\infty\), то
    \begin{equation} \label{eq::mo_phi}
        \E\phi(\xi,\eta)=\E\left\{\left.\E\phi(\xi,\eta)\right\vert_{\xi=\eta}\right\}
    \end{equation}
    В силу \eqref{eq::mo_phi}
    \[\P(t_k(\mu_2)>x) = \P\left(\frac{\xi+\mu_2}{\sqrt{\frac{1}{k}\eta_k}}>x\right)=\E I\left(\xi>x\sqrt{\frac{1}{k}\eta_k}-\mu_2\right)=\]
    \[=\E\left\{1-I\left(\xi\leq x\sqrt{\frac{1}{k}\eta_k}-\mu_2\right)\right\}
    =1-E\left\{EI(\xi\leq y)\bigg|_{y=x\sqrt{\frac{\eta_k}{k}}-\mu_2}\right\}=\]
    \[=1-\E\Phi\left(x\sqrt{\frac{1}{k}\eta_k}-\mu_2\right) \underset{}{>} 1 - \E\Phi\left(x\sqrt{\frac{1}{k}\eta_k}-\mu_1\right)=\P(t_k(\mu_1)>x)\]
    \[\mbox{ так как } \E\Phi\left(x\sqrt{\frac{1}{k}\eta_k}-\mu_2\right) < \E\Phi\left(x\sqrt{\frac{1}{k}\eta_k}-\mu_1\right) \mbox{ в силу возрастающей }\Phi(y)\]
\end{proof}

\underline{Обратимся к линейной гауссовской модели}
\[X=Zc+\Eps\]
\[X=(X_1,\ldots, X_n)^T\text{ - наблюдения},\ Z\text{ - \(n\times p\) матрица регрессоров \(p<n\)}\]
\[\Eps\sim N(0, \sigma^2\E_n),\ c=(c_1,\ldots, c_p)^T\]
\[\text{\underline{\(c\) и \(\sigma^2\) неизвестны}}\]
Рассмотрим новый вектор \(\beta=Ac\), \(A - k\times p\) матрица, \(rkA=k, k\leq p\).

\underline{Построим для \(\beta\) доверительное множество уровня \(1-\alpha\)}

Пусть \(\widehat{c}_n\) - оценка наименьших квадратов (о.н.к.) для \(c\), \(\widehat{s}^2_n\) - о.н.к. для \(\sigma^2\).
Пусть \(\widehat{\beta}_n=A\widehat{c}_n\).
\[\widehat{c}_n\sim N(c,\sigma^2(Z^TZ)^{-1}) \Rightarrow \widehat{\beta}_n\sim N(\underbrace{Ac}_{\beta}{}, \sigma^2D)\text{, где }D = A(Z^TZ)^{-1}A^T\]

\begin{leftbar}
Заметим, что \(D > 0\), так как для \(\alpha\in\R^k,\alpha\neq0\),
\[\alpha^TD\alpha=(A^t\alpha)^T(Z^TZ)^{-1}(A^T\alpha)>0, \text{ т.к. } (Z^TZ)^{-1}>0, A^T\alpha\neq0 \text{ при } rkA=k, \alpha\neq0\]
\end{leftbar}
В силу пункта 2 леммы \ref{delta_dependency}
\[\frac{1}{\sigma^2}\left(\widehat{\beta}_n-\beta\right)D^{-1}\left(\widehat{\beta}_n-\beta\right)\sim \chi^2(k)\]
так как \(\frac{(n-p)\widehat{s}^2_n}{\sigma^2}\sim\chi^2(n-p)\), \(\widehat{\beta}_n\) и \(\widehat{s}^2_n\) независимы, то
\[f_{k,n-p}(X, \beta)\defeq\frac{\frac{1}{k}(\widehat{\beta}_n-\beta)^TD^{-1}(\widehat{\beta}_n-\beta)/\sigma^2}{\frac{1}{n-p}(n-p)\widehat{s}^2_n/\sigma^2} =
\frac{(\widehat{\beta}_n-\beta)D^{-1}(\widehat{\beta}_n-\beta)}{k\widehat{s}^2_n}\sim F(k, n-p)\]

Значит,
\[\P_{\beta, \sigma^2}\left((\widehat{\beta}_n-\beta)^TD^{-1}(\widehat{\beta}_n-\beta)\leq k\widehat{s}_n^2f_{1-\alpha}(k,n-p)\right) = 1-\alpha\]
\(f_{1-\alpha}(k.n-p)\) - квантиль уровня \(1-\alpha\ F(k,n-p)\).\\ Доверительное
множество для \(\beta\) уровня \(1-\alpha\)
\[\Theta^*(X, \alpha)=\left\{\beta:(\widehat{\beta}_n-\beta)^TD^{-1}(\widehat{\beta}_n-\beta)<k\widehat{s}^2_nf_{1-\alpha}(k,n-p)\right\} = \]
\[=\left\{\beta:f_{k,n-p}(X,\beta)<f_{1-\alpha}(k,n-p)\right\}\text{ - \textbf{доверительный эллипсойд}}\]

Рассмотрим проверку гипотезы
\underline{$H_0:\beta=\beta_0$ против $H_1:\beta\neq\beta_0$}.
$H_0$ называют линейной гипотезой, так как $\beta=Ac$ получается
линейным преобразованием $c$.
В силу замечания \ref{hyp::accept_HO} $H_0$ надо принимать, если
$\beta_0\in\Theta^*(X,\alpha)$, то есть область принятия $H_0$:
\[\overline{S}_\alpha(\beta_0)=\{x:f_{k,n-p}(x,\beta_0)\leq f_{1-\alpha}(k,n-p)\}\]
То есть критическое множество (критерий уровня $\alpha$):
\begin{equation} \label{criterion::F}
    S_\alpha(\beta_0)=\{x:f_{k,n-p}(x,\beta_0)> f_{1-\alpha}(k,n-p)\}
\end{equation}
Критерий \ref{criterion::F} называют \textbf{критерием Фишера} или \textbf{$F$-критерием}.
$f_{k,n-p}(X,\beta_0)$ - статистика $F$-критерия.

\underline{Рассмотрим поведение $F$-критерия при альтернативе $H_1$}. \\
При $H_1$ в силу пункта 2 Леммы \ref{delta_dependency}
\[f_{k,n-p}(X,\beta_0)=\frac{\frac{1}{k}\overbrace{(\widehat{\beta}_n-\beta)^TD^{-1}(\widehat{\beta}_n-\beta)/\sigma^2}^{\chi^2(k,\Delta^2)}}{\frac{1}{n-p}\underbrace{(n-p)\widehat{s}^2_n/\sigma^2}_{\chi^2(n-p)}}\sim F(k,n-p,\Delta^2)\]
Параметр нецентральности
\begin{equation}\label{eq::fi_noncentral}
    \Delta^2 = \frac{1}{\sigma^2}(\beta-\beta_0)^TD^{-1}(\beta-\beta_0)
\end{equation}
Мощность $F$-критерия
\[W(\beta,S_\alpha(\beta_0))=\P_{\beta,\sigma^2}(f_{k,n-p}(X,\beta_0)>f_{1-\alpha}(k,n-p))=1-F_{k,n-p}(f_{1-\alpha}(k,n-p),\Delta^2)\]

\underline{Свойства мощности}
\begin{enumerate}
    \item Так как $\Delta =\Delta(\beta)=\Delta(\beta_0)>0$ при $\beta\neq\beta_0$,
    то в силу соотношения \ref{iid::fi}
    \[\P_{\beta,\sigma^2}(f_{k,n-p}(X, \beta_0)>f_{1-\alpha}(k,n-p))>\P_{\beta_0,\sigma}(f_{k,n-p}(X,\beta_0)>f_{1-\alpha}(k,n-p))=\alpha\]
    То есть при $\beta\neq\beta_0\ \P(H_1\vert H_1)>\P(H_1\vert H_0)$.
    То есть \underline{$F$-критерий несмещенный}!
    \item Мощность $W(\beta,S_\alpha(\beta_0))$ строго монотонна по $\Delta$
    из соотношения \ref{eq::fi_noncentral}
\end{enumerate}

\begin{example}[Определение порядка регрессии]
    $c_n^T=(\underbrace{c_{(1)n}^T}_{m-\text{вектор}}, \underbrace{c_{(2)n}^T}_{p-m-\text{вектор}}),\ 1\leq m\leq p$ \\
    $\begin{array}{c cc}
        H_0:& c_{(2)}=0    &\ (\text{порядок не больше m})\\
        H_1:& c_{(2)}\neq0 & \par
    \end{array}$

    Рассмотрим матрицу
    \[ A =
    \begin{array}{c}
    \overbrace{
        \left(
        \begin{array}{ccc:ccc}
            0 &        &   & 1 &        &    \\
                & \ddots &   &   & \ddots &    \\
                &        & 0 &   &        & 1  \\
        \end{array}
        \right)
        }^{p}
        \\
        \begin{array}{ccc ccc}
                & m &   &  & & p - m \\
        \end{array}
    \end{array}
    \Rightarrow
    Ac = c_{(2)}
    \Rightarrow
    H_0 \Leftrightarrow Ac=0
    % =(\beta_0)
    \]
\end{example}
Пусть $\widehat{c}_n^T=(\underbrace{\widehat{c}_{(1)n}^T}_{m-\text{в-р}}, \underbrace{\widehat{c}_{(2)n}^T}_{p-m-\text{в-р}})$.
Тогда $\widehat{\beta}_n=A\widehat{c}_n=\widehat{c}_{(2)n}$.
\[(Z^TZ)^{-1}=\left(\begin{array}{c|c}
    B_{11} & B_{12} \\ \hline
    B_{21} & B_{22}
\end{array}\right)\rightarrow D=A(Z^TZ)^{-1}A^T=B_{22}\Rightarrow\]
\[\Rightarrow f_{p-m,n-p}(X,0)=\frac{\widehat{c}_{(2)n}^TB_{22}^{-1}\widehat{c}_{(2)n}}{(p-m)\widehat{s}^2_n}\underset{H_0}{\sim}F(p-m,n-p)\]
$H_0$ отвергается, если $f_{p-m,n-p}(X,0)>f_{1-\alpha}(p-m,n-m)$,
то есть
\begin{equation}\label{fisher::ex::crit}
    S_{\alpha}(0)=\{x:\frac{\widehat{c}_{(2)n}^TB_{22}^{-1}\widehat{c}_{(2)n}}{(p-m)\widehat{s}^2_n}>f_{1-\alpha}(p-m,n-m)\}
\end{equation}
\begin{equation}\label{fisher::ex::stat}
    f_{p-m,n-p}(X,0)\underset{H_1}{\sim} F(p-m,n-p, \Delta^2),\text{ где }\Delta^2=\frac{c_{(2)}^TB_{22}^{-1}c_{(2)}}{\sigma^2}
\end{equation}
Критерий \ref{fisher::ex::crit} - несмещенный, то есть $\P(H_1\vert H_1)>\P(H_1\vert H_0)=\alpha$. Его мощность
\[W(c_{(2)}, S_{\alpha}(0))=\P_{c_{(2), \sigma^2}}(f_{p-m,n-p}(X,0)>f_{1-\alpha}(p-m,n-p))=1-F_{p-m,n-p}(f_{1-\alpha}(p-m,n-p),\Delta^2)\]
строго возрастает по $\Delta^2$. Параметр нецентральности $\Delta^2$ определен в \ref{fisher::ex::stat}.

\begin{example}[Проверка однородности двух выборок]
    $X=(X_1,\ldots,X_m),\ Y=(Y_1,\ldots,Y_n)$ - независимые гауссовские выборки.
    То есть $\{X_i\},\ \{Y_i\}$ - н.о.р., $X_1\sim N(a, \sigma^2),\ Y_1\sim N(b, \sigma^2)$.
    Совокупность $\{X_i\}$ и $\{Y_j\}$ независимы, $m+n>2$. \par
    \underline{Дисперсии $DX_1,\ DY_1$ одинаковы ($=\sigma^2$), неизвестны, средние
    $a$ и $b$ неизвестны}.

    $\begin{array}{c cc}
        H_0:& a = b   &\ (\text{\textbf{гипотеза однородности}}) \\
        H_1:& a \neq b&
    \end{array}$

    \begin{remark*}
        При $\D X_{1}\neq\D X_2$ эта задача называется \textbf{проблемой Беренса-Фишера}.
    \end{remark*}

    \begin{equation*}
        \begin{cases}
            X_i = a+\eps_i,\ &i=1,\ldots,m,\ \eps_i=X_i-a\\
            Y_i = b+\widehat{\eps}_j,\ &j=1,\ldots,n,\ \widehat{\eps}_j=Y_j-b
        \end{cases}
        \Rightarrow
        \eps_1,\ldots,\eps_m,\widehat{\eps}_1,\ldots, \widehat{\eps}_n\text{ - н.о.р. $N(0, \sigma^2)$ сл.в.}
    \end{equation*}

    \begin{equation}
    \begin{array}{cc}
        \begin{array}{l}
            \\
            \widehat{X}\defeq(X_1,\ldots,X_m,Y_1,\ldots,Y_n)^T \\
            \\
            c=(a,b)^T \\
            \\
            \Eps^T=(\eps_1,\ldots,\eps_m,\widehat{\eps}_1,\ldots, \widehat{\eps}_n)^T \\
        \end{array} &
        Z= \left(\begin{array}{cc}
            m \begin{cases}
                1 \\
                \vdots \\
                1
            \end{cases} & 0 \\
            0 & n \begin{cases}
                1 \\
                \vdots \\
                1
            \end{cases}
        \end{array}\right)
    \end{array}
    \Rightarrow
    \underset{\text{\textbf{гаусс. лин. регрессия}}}{\widehat{X}=Zc+\Eps}
\end{equation}

Положим $A=(1, -1)$. Тогда $Ac=a-b=\beta$.

$\begin{array}{ccc}
    H_0:& Ac=a-b=\beta=0& (=\beta_0)\\
    H_1:& Ac=a-b\neq0& (\beta\neq0)
\end{array}$

О.н.к. для вектора $c$ - решение задачи
\[
    \sum_{i=1}^m(X_i-a)^2+\sum_{j=1}^m(Y_j-b)^2\rightarrow \min_{a, b}
    \Leftrightarrow \begin{cases}
        -2\sum_i(X_i-a) = 0 \\
        -2\sum_j(Y_j-a) = 0
    \end{cases}
\]
Решением системы является $\widehat{a}_n=\overline{X},\ \widehat{b}_m=\overline{Y}$ -
оптимальные оценки $a$ и $b$, $\widehat{c}_n=(\overline{X}, \overline{Y})^T$ - оптимальная
оценка для $c$. Оптимальная оценка для $\sigma^2$:
\[ \widehat{S}^2_{m+n}=\frac{1}{m+n-2}\left[\sum_i(X_i-\overline{X})^2+\sum_j(Y_j-\overline{Y})^2\right] \]
Тогда
\[
\begin{array}{l}
    \widehat{\beta}_n=A\widehat{c}_n=\overline{X}-\overline{Y} \\
    Z^TZ=\left(\begin{array}{cc}
        \overbrace{1\ \ldots\ 1}^m & 0 \\
                0      & \underbrace{1\ \ldots\ 1}_n
    \end{array}\right)
    \left(\begin{array}{cc}
            & 1 \\
        0 & \vdots \\
            & 1 \\
        1  & \\
        \vdots & 0 \\
        1  &
    \end{array}\right) = \left(\begin{array}{cc}
        m & 0 \\
        0 & n
    \end{array}\right) \\
    D=A(Z^TZ)^{-1}A^T=
    \left(1\ -1 \right)
    \left(\begin{array}{cc}
        \frac{1}{m} & 0 \\
        0 & \frac{1}{n}
    \end{array}\right)
    \left(\begin{array}{c}
        1 \\
        -1
    \end{array}\right) = \frac{1}{n} + \frac{1}{m}
\end{array}
\Rightarrow
\boxed{f_{1, m+n-2}(X,0)=\frac{(\overline{X}-\overline{Y})^2}{\left(\frac{1}{n} + \frac{1}{m}\right)\widehat{S}^2_{m+n}}}
\]
$F$-критерий для $H_0$ имеет вид
\[S_{\alpha}(0)=\{x\in\R^{m+n}:f_{1,m+n-2}(x,0)>f_{1-\alpha}(1,m+n-2)\}\]
\[f_{1,m+n-2}(X,0)\underset{H_0}{\sim}F(1,\ m+n-2)\]
\[f_{1,m+n-2}(X,0)\underset{H_1}{\sim}F(1,\ m+n-2,\Delta^2),\]
\[\text{ где параметр нецентральности } \Delta^2=\Delta^2(\underset{a-b}{\beta})=\frac{(a-b)^2}{\sigma^2\left(\frac{1}{n}+\frac{1}{m}\right)}\]
\begin{enumerate}
    \item Если $\vert a-b\vert$  возрастает, то мощность $F$-теста возрастает
    \item Если $\sigma\rightarrow0$ или $\frac{1}{n}+\frac{1}{m}\rightarrow0$, то мощность возрастает
\end{enumerate}
\end{example}
\subsection{Критерий согласия Хи-квадрат Пирсона. Проверка простой гипотезы в схеме Бернулли.}
Пусть проводятся $n$ независимых испытаний, и в каждом испытании возможны
$m\geq2$ исходов $A_1,\ldots,A_m$ таких, что $A_iA_j=\emptyset,\ i\neq j,\ \sum A_i=\Omega$, тогда
$\P(A_j)=p_j>0,\ \sum_{j=1}^mp_j=1$. Пусть $\nu=(\nu_1,\ldots,\nu_m)^T$, а $\nu_j$ -
число появления $A_j$ в $n$ опытах, тогда $\sum_{j=1}^m\nu_j=n$.
По вектору наблюдений $\nu$ необходимо проверить следующую гипотезу:

$\begin{array}{cl}
    H_0:& p_j=p_j^\circ,\ j=1,\ldots,m\\
    H_1:& p_j\neq p_j^\circ\ \forall j
\end{array}$

\begin{remark*}
    $H_0$ - простая гипотеза, т.к. полностью определяет распределение
    вектора $\nu$.
    \[ \P(\nu_1=k_1,\ldots,\nu_m=k_m)\underset{H_0}{=}\frac{n!}{k_1!\ldots k_m!}(p_1^\circ)^{k_1}\ldots(p_m^\circ)^{k_m}\]
\end{remark*}
Это полиномиальное распределение $\prod(n,p_1^\circ, \ldots, p_m^\circ)$.
Статистика Хи-квадрат Пирсона:
\[\chi_n^2\underset{H_0}{\defeq}\sum_{j=1}^m\frac{(\nu_j-np_j^\circ)^2}{np_j^\circ}\]
\underline{Поведение при альтернативе:} Очевидно
\[\chi_n^2=n\sum_{j=1}^m\frac{(\nu_j/n-p_j^\circ)^2}{p_j^\circ}\]
В силу теоремы Бернулли $\frac{\nu_j}{n}\xrightarrow{\P}p_j$.
Поэтому
\[\sum_{j=1}^m\frac{(\nu_j/n-p_j^\circ)^2}{p_j^\circ}\xrightarrow[\text{Т. о наслед. сход.}]{\P}\sum_{j=1}^m\frac{(p_j-p_j^\circ)^2}{p_j^\circ}\underset{H_1}{>}0\]
Значит,
\[\chi_n^2\xrightarrow[H_1]{\P}\infty,\ n\rightarrow\infty\]
Поэтому большие значения $\chi_n^2$ часто свидетельсвтуют о том, что
стоит отвергнуть $H_0$. Но насколько "большие" значения?
\begin{named_theorem}[Теорема Пирсона]
    \[ \chi_n^2\xrightarrow[H_0]{d}\chi^2(m-1),\ n\rightarrow\infty \]
    \underline{Правило}: Если $\chi_n^2\leq\chi_{1-\alpha}(m-1)$, то принимаем $H_0$,
    иначе принимаем $H_1$.
\end{named_theorem}
\begin{remark*} Тогда
    \[\P(H_1\vert H_0)=\P(\chi^2_n>\chi_{1-\alpha}(m-1)\vert H_0)\rightarrow\alpha\]
    \[\P(H_0\vert H_1)=\P(\chi^2_n\leq\chi_{1-\alpha}(m-1)\vert H_1)\rightarrow0\]
    То есть
    \[\begin{cases}
        \P(H_0\vert H_0)\rightarrow1-\alpha \\
        \P(H_1\vert H_1)\rightarrow1
    \end{cases}\]
    \underline{Вероятность принять правильную гипотезу близка к единице!}
\end{remark*}
\begin{proof}
    Покажем, что вектор $\nu=(\nu_1,\ldots,\nu_m)^T$ асимптотически нормален, то есть
    \begin{equation}\label{th::pirson::asym_norm}
        \sqrt{n}(\nu/n-p)\xrightarrow{d}N(0,P-pp^T),\text{ где }P\defeq\begin{pmatrix}
            p_1^\circ & & 0 \\
                & \ddots & \\
            0 & & p_m^\circ
        \end{pmatrix}
    \end{equation}
    Введем вектора $X_1,\ldots,X_n$, где $X_i=(0,\ldots,0,\frac{1}{j},0,\ldots,0)^T$,
    если в $i$-ом испытании произошло $A_j$. Тогда $\nu=\sum_{i=1}^nX_i$
    \begin{equation}\label{th::pirson::subst_nu}
        \sqrt{n}(\nu/n-p)=\sqrt{n}\sum_{i=1}^n(X_i-p)
    \end{equation}
    Здесь $\{X_i\}$ - н.о.р., $EX_1=p,\ \cov(X_1,X_1)=\E(X_1-p)(X_1-p)^T=\E X_1X_1^T-pp^T=P-pp^T$.
    Поэтому соотношение \eqref{th::pirson::asym_norm} следует из соотноешния \eqref{th::pirson::subst_nu}
    и ЦПТ. \\
    Матрица $P-pp^T$ вырождена, так как сумма ее столбцов равна нулю:
    если $e=(1,\ldots,1)^T$, то $(P-pp^T)e=p-p(p^Te)=p-p=0$

    Пусть
    \[P^{-1/2}\defeq\begin{pmatrix}
        \frac{1}{\sqrt{p_1^\circ}} & & 0 \\
            & \ddots & \\
        0 & & \frac{1}{\sqrt{p_m^\circ}}
    \end{pmatrix},\
    \xi_n\defeq\sqrt{n}P^{-1/2}(\nu/n-p)
    \]
    В силу теоремы о наследовании слабой сходимости и соотношения \eqref{th::pirson::asym_norm}
    \begin{equation} \label{th::pirson::xi_conv}
        \xi_n\xrightarrow{d}N(0, P^{-1/2}(P-pp^T)(P^{-1/2})^T)=N(0, E_m-zz^T),\text{ где } z=(\sqrt{p_1^\circ},\ldots,\sqrt{p_m^\circ})^T
    \end{equation}
    Пусть ортогональная матрица $U=\begin{pmatrix}
        \sqrt{p_1^\circ} & \ldots & \sqrt{p_m^\circ} \\
        \ldots & \ldots & \ldots
    \end{pmatrix}$. Тогда
    \[U(E_m-zz^T)U^T=E_m-(Uz)(Uz)^T=\]
    \[=\begin{pmatrix}
        1 &        & 0 \\
            & \ddots &   \\
        0 &        & 1 \\
    \end{pmatrix}-\begin{pmatrix}
        1 \\ 0 \\ \vdots \\ 0
    \end{pmatrix}\begin{pmatrix}
        1  & 0  & \ldots  & 0
    \end{pmatrix}=\begin{pmatrix}
        0 &   &        & 0 \\
            & 1 &        &   \\
            &   & \ddots &   \\
        0 &   &        & 1 \\
    \end{pmatrix}=\widetilde{\E}_1\]
    В силу \eqref{th::pirson::xi_conv} и теоремы о слабой сходимости
    \begin{equation} \label{th::pirson::uxi_conv}
        U\xi_n\xrightarrow{d}N(0,\widetilde{\E}_1) = (0,\eta_2,\ldots,\eta_m)^T
    \end{equation}
    где $\{\eta_2,\ldots,\eta_m\}$ - независимые $N(0,1)$ сл.в. Из \eqref{th::pirson::uxi_conv}
    и теоремы о наследовании слабой сходимости следует:
    \begin{equation}\label{th::pirson::absuxi_conv}
        \lvert U\xi_n\rvert^2\xrightarrow{d} \eta_2^2+\ldots+\eta_m^2\sim\chi^2(m-1)
    \end{equation}
    Осталось заметить, что
    \[\lvert U\xi_n\rvert^2=\lvert\xi_n\rvert^2=\sum_{j=1}^m\left[\frac{1}{\sqrt{p_j^\circ}}\sqrt{n}(\nu_j/n-p_j^\circ)\right]^2=\sum_{j=1}^m\frac{(\nu_j-np_j^\circ)^2}{np_j^\circ}=\chi_n^2\]
    Из этого равенства и соотноешния \eqref{th::pirson::absuxi_conv} следует теорема Пирсона.
\end{proof}

\begin{example}[Проверка простой гипотезы о виде функции распределения]
$X=(X_1,\ldots,X_n)$, $\{X_i\}$ - н.о.р., $X_1\sim F(x)$.

$\begin{array}{cl}
    H_0:& F(x)=F_0(x),\ (F_0\text{ известна})\\
    H_1:& F(x)\neq F_1(x),\ F_1(x)\neq F_0(x)
\end{array}$

Разобъем носитель $X_1$ на непересекающиеся отрезки $\Delta_1,\ldots,\Delta_m,\ m\geq2$ так, что
$X_1\in\Delta_1\cup\Delta_2\cup\ldots\cup\Delta_m$
\[p_j^\circ\defeq \P(X_1\in\Delta_j\vert H_0) =\int_{\Delta_j}dF_0(x)>0\ \forall j\]
Тогда $\sum_{j=1}^mp_j^\circ=1$. С каждой величиной $X_i$ свяжем испытание
с исходами $A_1,\ldots,A_m$, причем $A_j$ происходит тогда и только тогда,
когда $X_i\in\Delta_j$. При $H_0$ $\P(A_j)=p_j^\circ$. Тогда наблюдения
$X_1,\ldots,X_n$ порождают полиномиальную схему независимых испытаний.
Пусть $\nu_j$-число исхода $A_j$ в этих испытаниях, то есть число
наблюдений среди $X_1,\ldots,X_n$, попавших в $\Delta_j$.
В силу теоремы Пирсона:
\[\chi_n^2\defeq\sum_{j=1}^m\frac{(\nu_j-np^\circ_j)^2}{np^\circ_j}\xrightarrow[H_0]{d}\chi^2(m-1)\]

\underline{Правило}: $H_0$ будем отвергать, если $\chi_n^2>\chi_{1-\alpha}(m-1)$. ($\alpha$ задано)
Тогда $\P(H_1\vert H_0)\rightarrow\alpha,\ n\rightarrow\infty$.
\[p_j\defeq \P(X_1\in\Delta_j\vert H_1)=\int_{\Delta_j}dF_1(x)\]
Если верна $H_1$ и хоть при одном $j$ $p_j\neq p_j^\circ$, то
$\P(H_0\vert H_1) = \P(\chi_n^2<\chi_{1-\alpha}(m-1)\vert H_1)\rightarrow 0$
\begin{remark*}
    Если $F_0\not\equiv F_1$, но $p_j=p_j^\circ\ \forall j$,
    то $\P(H_0\vert H_1)=\P(H_0\vert H_0)\rightarrow1-\alpha\neq0$.
    Например:
    % TODO
    \begin{figure}[h!]
        \centering
        \begin{gnuplot}[terminal=epslatex, terminaloptions={size 9cm,5.4cm}]
            set xrange [-2:4]
            set yrange [0.4:3]
            set border 1
            set yzeroaxis linetype -1
            set xlabel '$m=2$'
            unset ytics

            set xtics (0)

            f(x) = 0.5+exp(x)

            set arrow from -2,1.5 to 0,1.5 heads
            set label "$\\Delta_1$" at -1, 1.65

            set arrow from 0,1.5 to 0.916291,1.5 heads
            set label "$\\Delta_2$" at 0.45814, 1.33

            plot f(x) ls 7 title '$F_0$', \
                 f(2*x) ls 1 title '$F_1$'
        \end{gnuplot}
    \end{figure}

    Здесь $\P(X_1\in\Delta_1\vert H_0)=F_0(0)=\P(X_1\in\Delta_1\vert H_1)=F_1(0)$.
    Значит, и $\P(X_1\in\Delta_2\vert H_0)=1-F_0(0)=\P(X_1\in\Delta_1\vert H_1)=1-F_1(0)$.
\end{remark*}
\end{example}
\subsubsection*{Проверка сложной гипотезы в схеме исп. Бернулли}
Пусть проводится $n$ независимых испытаний, искходы
$A_1,\ldots,A_m$, $\nu=(\nu_1,\ldots,\nu_m)^T$ - вектор наблюдений.
Пусть $H_0:\ \P(A_j)=p_j(\theta),\ \theta\in\Theta\in\R^k,\ k<m-1$.

\underline{Условия регулярности}
\begin{enumerate}
    \item $\sum_{j=1}^mp_j(\theta)=1,\ \theta\in\Theta$
    \item $p_j(\theta)\geq c>0\ \forall j=1,\ldots,m$ и $\exists\ \frac{\partial p_j(\theta)}{\partial \theta_l},\frac{\partial^2p_j(\theta)}{\partial\theta_l\partial\theta_r}$
    \item $rank(\underbrace{\frac{\partial p_j(\theta)}{\partial \theta_l}}_{\text{$m\times k$}})=k,\ \forall\theta\in\Theta$
\end{enumerate}
В качестве оценки $\theta$ при $H_0$ будем использовать мультиномиальные
оценки максимального правдоподобия:
\[\P(\nu_1=k_1,\ldots,\nu_m=k_m)=\frac{n!}{k_1!\ldots k_m!}p_1^{k_1}(\theta)\ldots p_m^{k_m}(\theta)\]
логарифмического правдоподобия:
\[L_n(\nu,\theta)=\ln\left(\frac{n!}{k_1!\ldots k_m!}\right)+\sum_{j=1}^m\nu_j\ln p_j(\theta)\]
оценки максимального правдоподобия (мультиномиальные):
\[L_n(\nu,\theta)\rightarrow\max_{\theta\in\Theta}\]
\begin{named_theorem}[Теорема Фишера]
    Пусть выполнены условия регулярности, $\widehat{\theta}_n$ - мульт. о.м.п. Тогда
    \[ \widehat{\chi}_n^2=\sum_{j=1}^m\frac{(\nu_j-np_j(\widehat{\theta}_n))^2}{np_j(\widehat{\theta}_n)}\xrightarrow[H_0]{d}\chi(m-k-1) \]
    \underline{Правило}: Если $\widehat{\chi}_n^2\leq X_{1-\alpha}(m-k-1)$, то принимаем $H_0$, иначе принимаем $H_1$.
    Тогда $\P(\overline{H_0}\vert H_0)\rightarrow\alpha$
\end{named_theorem}
\begin{example}[Проверка независимости признаков]
    Пусть объект классифицирован по двум $A$ и $B$,
    $A=\{A_1,\ldots,A_s\},\ B=\{B_1,\ldots,B_r\},\ s>1,\ r>1$.
    Проводится $n$ опытов, и пусть $\nu_{ij}$ - число объектов,
    имеющих признаки $A_iB_j$. \\
    Пусть $p_{ij}=\P(A_iB_j)$. Гипотеза независимости
    $H_0:\ p_{ij}=p_{i\bullet}p_{\bullet j}$ для положительных $p_{i\bullet}$ и $p_{\bullet j}$
    таких, что $\sum_{i=1}^sp_{i\bullet}=1,\ \sum_{j=1}^rp_{\bullet j}=1$. \\
    При $H_0$ логарифмическое правдоподобие
    \[L_n(\nu,p_{i\bullet},p_{\bullet j})=\ln\frac{n!}{\prod_{i,j}\nu_{ij}}+\sum_{i=1}^s\sum_{j=1}^r\nu_{ij}\ln(p_{i\bullet}p_{\bullet j})\]
    Максимизируя эту функцию по $p_{i\bullet},\ p_{\bullet j}$ при условиях, что $\sum_{i=1}^sp_{i\bullet}=1,\ \sum_{j=1}^rp_{\bullet j}=1$,
    находим оценки
    \[\widehat{p}_{i\bullet}=\frac{\nu_{i\bullet}}{n},\ \widehat{p}_{\bullet j}=\frac{\nu_{\bullet j}}{n},\text{ где } \nu_{i\bullet}=\sum_{j}\nu_{ij},\ \nu_{\bullet j}=\sum_{j}\nu_{ij}\]
    Статистика Хи-квадрат имеет вид
    \[\widehat{\chi}_n^2=\sum_{i=1}^s\sum_{j=1}^r\frac{(\nu_{ij}-n\widehat{p}_{i\bullet}\widehat{p}_{\bullet j})^2}{n\widehat{p}_{i\bullet}\widehat{p}_{\bullet j}}\]
    \[\widehat{\chi}_n^2\xrightarrow[H_0]{d}\chi((s-1)(r-1))\]
    так как $m-k-1=sr-(s+r-2)-1=(s-1)(r-1)$. \\
    \underline{Правило}: Если $\widehat{\chi}_n^2>\chi_{1-\alpha}((s-1)(r-1))$,
    то отвергаем $H_0$. Асимптотический уровень теста есть $\alpha$
\end{example}
\begin{example}[W.H.Gilby. Biometrika, 8,94]
    1725 школьников классифицировали в соответствии с их
    качеством одежды и в соответствии с умственными способностями.
    Использовали следующие градации:
    
    \[
    \begin{array}{lr}
        \begin{array}{lcl}
            A &-& \text{умственно отсталый} \\
            B &-& \text{медлительный и тупой} \\
            C &-& \text{тупой} \\
            D &-& \text{медлительный, но умный} \\
            E &-& \text{достаточно умный} \\
            F &-& \text{способный} \\
            G &-& \text{очень способный} \\
        \end{array} & 
        \boxed{H_0:\ \text{признаки независимы}}
    \end{array}
    \]
    \begin{table}[h]
        \centering
        \begin{tabular}{ c|c|c|c|c|c|c|c }
            & \multicolumn{6}{|c|}{Способности} & \\ \hline
            Как одевается & A и B & C   & D   & E   & F   & G  & Сумма \\ \hline
            Очень хорошо  &  33   & 48  & 113 & 209 & 194 & 39 & 636   \\ \hline
            Хорошо        &  41   & 100 & 202 & 255 & 138 & 15 & 751   \\ \hline
            Сносно        &  39   & 58  & 70  & 61  & 33  & 4  & 256   \\ \hline
            Очень плохо   &  17   & 13  & 22  & 10  & 10  & 1  & 73    \\ \hline
            Сумма         &  130  & 219 & 407 & 535 & 375 & 59 & 1725
        \end{tabular}
    \end{table}

    Здесь $\chi_n^2=174.92>\chi_{0.999}(15)=37.697$. \par
    Здесь $15 = (s-1)(r-1) = (4-3)(6-1) \Rightarrow$ Отвергаем $H_0$
\end{example}

\section{Введение в робастное оценивание}

Схема засорений Мартина-Йохаи имеет вид:
\[y_t = u_t + z^\gamma_t\xi_t,\ t = 1, \ldots, n\]
Здесь $\{u_t\}$ - "полезный сигнал" (временной ряд); \\
$\{z_t^\gamma\}$ - н.о.р. сл.в., $z_1^{\gamma} \sim Bin(1, \gamma)$
с $0 \leq \gamma\leq1$ ($\gamma$ - уровень засорения); \\
$\{\xi_t\}$ - н.о.р. сл.в. - грубые выбросы, $\xi_1$ - имеет распределение
$\mu_\xi\in M_\xi$;
Распределение $\mu_\xi$ неизвестно, а множество $M_\xi$ известно; \\
Последовательность $\{u_t\}, \{z^\gamma_t\}, \{\xi_t\}$ независимы между собой. \\
Пусть $y_1, \ldots, y_n$ - наблюдения, и распределение
вектора $Y_n=y_1, \ldots, y_n$ висит от неизвестного параметра $\beta$.
Пусть $\hat{\beta}_n$ - некоторая оценка $\beta$

\underline{Основное предположение}

При любом $0 \leq \gamma\leq1$ существует предел
\[\hat{\beta}_n\xrightarrow{\P}\theta_\gamma,\ n\rightarrow\infty;\ \theta_0=\beta\]

\begin{definition}
    Если существует предел
    \[IF(\theta_\gamma, \mu_\xi)\defeq\lim_{\gamma\rightarrow+0}\frac{\theta_\gamma-\theta_0}{\gamma}\]
    то $IF(\theta_\gamma, \mu_\xi)$ называется \defin{функционалом влияния оценки $\widehat{\beta}_n$}
\end{definition}

Если функционал влияния существует, то 
\[\theta_\gamma = \theta_0 + IF(\theta_\gamma, \mu_\xi)\gamma + \littleO(\gamma),\ \gamma\rightarrow+0\]
\begin{leftbar}
    $IF(\theta_\gamma, \mu_\xi)$ характеризует главный линейный по
    $\gamma$ член в разложении по $\gamma$ асимптотического смещения $\theta_\gamma - \theta_0=\theta_\gamma-\beta$
\end{leftbar}

\begin{definition}
    Величина $GES(\theta_\gamma,M_\xi)\defeq\sup_{\mu_\xi\in M_\xi}\left\lvert IF(\theta_\gamma, \mu_\xi)\right\rvert$
    называется чувствительностью оценки $\widehat{\beta}_n$ к засорениям (выбросам).
\end{definition}
\begin{leftbar}
    Если $GES(\theta_\gamma,M_\xi)<\infty$, то главный член по $\gamma$
    асимптотического смещения $IF(\theta_\gamma,\mu_\xi)\gamma$
    равномерно по $\mu_\xi$ $\max$ при таких $\gamma$
\end{leftbar}
\begin{definition}
    Если $GES(\theta_\gamma,M_\xi)<\infty$, то оценка $\widehat{\beta}_n$
    называется \defin{робастной по смещению}, или $B$-робастной.
\end{definition}

\begin{example}[Выборочное среднее]
    \[\begin{cases}
        u_t = a + \xi_t \\
        y_t = u_t + z^\gamma_t\xi_t, \ t=1,\ldots,n
    \end{cases}\]
    $\{\xi_t\}$ - н.о.р. сл.в., $\E\eps_1=0$ (тогда $\E u_t=a$), $\E\vert\xi_1\vert<\infty$ \\
    Возьмем оценкой $a$ эмпирическое среднее $\overline{y}=\frac{1}{n}\sum_{t=1}^ny_t$.
    Тогда $\overline{y}\xrightarrow{\P}E(u_1+z_1^\gamma\xi_1)=a+\gamma\E\xi_1=\theta^{LS}_\gamma$
    Функция $\theta_\gamma^{LS}$ определена при всех $\gamma$,
    \[\frac{d\theta_\gamma^{LS}}{d\gamma}=\E\xi_1=IF(\theta_\gamma^{LS}, \mu_\xi)\]
    Если $M_1$ - класс распределений с конечным первым моментом, то
    \[GES(\theta_\gamma^{LS}, M_1) = \sup_{\mu_\xi\in M_1} \vert\E\xi_1\vert = \infty\]
    Оценка $\overline{y}$ \underline{не $B$-робастна} на  классе $M_1$!
\end{example}
\begin{example}[Выборочная медиана]
    Пусть
    \begin{equation} \label{def::var_series}
        u_t=a+\eps_t,\ t=1,\ldots,n,\text{ где }\{\eps_t\}\text{ - н.о.р. сл.в.}
    \end{equation} 
    $\eps_1\sim G(x)$, функция распределения $G(x)$ неизвестна,
    $G(0)=1/2$. Тогда функция распределения $u_1$ есть $F(x)=G(x-a)$,
    то есть $F(a)=1/2$. Таким образом, медиана $G(x)$ это 0, медиана $F(x)$ - $a$.
    \begin{leftbar}
        Если $\eps_1$ имеет симметричное относительно 0 распределение
        (то есть $\eps_1\overset{d}{=}-\eps_1$, что для непрерывной
        $G(x)$ равносильно условию $G(x)+G(-x)=1\ \forall x$), то
        автоматически $G(0)=1/2$. Если вдобавок $\E\lvert\eps_1\rvert<\infty$,
        то $\E\eps_1=0,\E u_1=a$
    \end{leftbar}
    Итак, при сформулированных условия оценку медианы можно использовать
    как оценку математического ожидания.

    Пусть $u_{(1)}\leq u_{(2)}\leq\ldots\leq u_{(n)}$ будет вариационный
    ряд наблюдений $u_1,\ldots,u_n$.
    \begin{definition}
        Величина
        \[
            \widehat{m}_n=\begin{cases}
                u_{(k+1)}\ &n=2k-1 \\
                \frac{u_{(k+1)} + u_{(k)}}{2}\ &n=2k
            \end{cases}
        \]
        называется \defin{выборочной медианой} наблюдений $u_1,\ldots,u_n$.
    \end{definition}

    Мы знаем, что если $G(x)$ дифф. в нуле, и $g(0) = G'(0)>0$,
    то для выборочной медианы справедлива асимптотическая нормальность:
    \[ \sqrt{n}(\widehat{m}_n - a)\xrightarrow{d}N(0, \frac{1}{4g^2(0)}),\ n\rightarrow\infty\]
    Если в \eqref{def::var_series} $\{\eps_t\}$ - н.о.р., $\E\eps_1=0$,
    $0<\E\eps_!^2=\sigma^2<\infty$, то $\sqrt{n}(???-a)\xrightarrow{d}N(0,\sigma^2)$.
    Значит асимптотическая оптимальная эффективность выборочной медианы относительно ???
    равна
    \[\boxed{e_{\widehat{m}_n,\ \overline{X}}=4g^2(0)\sigma^2}\]

    \underline{Изучим $B$-робастность выборочной медианы.}
    Пусть
    \[
    \begin{array}{cc}
    \begin{cases}
        u_t=a+\eps_t \\
        y_t=u_t+z_t^\gamma\xi_t,\ t=1,\ldots,n
    \end{cases} &
    \widehat{m}_n^y=
    \begin{cases}
        y_{(k+1)},\ n=2k-1 \\
        \frac{y_{(k)}+y_{(k+1)}}{2},\ n=2k
    \end{cases} \\
    \end{array}
    \]
    \begin{theorem}
        Пусть $\exists\ g(x)=G'(x)$, $g(x)$ непрерывна и ограничена,
        $g(0)>0,\ G(0)=1/2$. Тогда:
        \begin{enumerate}
            \item $\widehat{m}_n^y\xrightarrow{\P}\theta_\gamma^m,\ \theta_0=a$
            \item Существует функционал влияния выборочной медианы
            \[IF(\theta_\gamma^m,\mu_\xi)=\frac{1-2\E G(-\xi_1)}{2g(0)}\]
            \item Чувствительность выборочной медианы на классе всех возможных
            распределений $M_\xi$
            \[GES(\theta_\gamma^m,\mu_\xi)=\sum_{\mu_\xi\in M_\xi}\lvert IF(\theta_\gamma^m,\mu_\xi)\rvert=\frac{1}{2g(0)}<\infty\]
            то есть \underline{выборочная медиана $B$-робастна}.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        \begin{enumerate}
            \item Выборочная медиана $\widehat{m}_n^y$ 
            удовлетворяет уравнению
            \begin{equation} \label{def::sample_median}
                l_n(\theta)\defeq\frac{1}{n}\sum_{t=1}^nsgn(y_t-\theta)=0,\text{ где }sgn(x)=\begin{cases}
                    -1, x<0 \\
                    0, x=0 \\
                    1, x>0
                \end{cases}
            \end{equation}
            Справедливость формулы \eqref{def::sample_median} легко понять из следующих рисунков
            \begin{figure}[h!]
                \centering
                \begin{gnuplot}[scale=0.6]
                    set xlabel '$n=1$'
                    set border 0
                    set xzeroaxis
                    set yzeroaxis
                    set xrange [-0.2:3]
                    set yrange [-1.5:1.5]
                    set ytics (0, 1, -1)
                    unset xtics

                    set arrow from -0.2,1 to 1,1
                    set arrow from 3,-1 to 1,-1

                    set label at 1,0 "$y_{(1)}$" point pointtype 7 pointsize 2 offset -1,-1.3

                    set label at 2,1 "$\\widehat{m}_1^y$"
                    set arrow from 1.9,0.9 to 1.1,0.1

                    plot 0 notitle
                \end{gnuplot}
                \begin{gnuplot}[terminal=epslatex,scale=0.6]
                    set xlabel '$n=2$'
                    set border 0
                    set xzeroaxis ls -1
                    set yzeroaxis ls -1
                    set xrange [-0.2:3]
                    set yrange [-2.5:2.5]
                    set ytics (0, 1, -1, 2, -2)
                    unset xtics

                    set arrow from -0.2,2 to 1,2
                    set arrow from 3,-2 to 2,-2
                    set arrow from 1,0 to 2,0 heads

                    set label at 1,1 "" point pointtype 7 pointsize 1
                    set label at 2,-1 "" point pointtype 7 pointsize 1

                    set label at 1,0 "$y_{(1)}$" offset -1,-1.3
                    set label at 2,0 "$y_{(2)}$" offset -1,-1.3

                    set label at 2,1 "$\\widehat{m}_2^y$"
                    set arrow from 1.96,0.9 to 1.6,0.1
                    set label at 1.5,0 "X"

                    plot 0 notitle ls -1
                \end{gnuplot}
            \end{figure}
            Так бывает всегда: при нечетном $n$ решение уравнения \eqref{def::sample_median}
            всегда $\exists!$, это $\widehat{m}_n^y$; при четном $n$ решений целый
            интервал и $\widehat{m}_n^y$ - его середина.

            В силу Закона Больших Чисел при любом $\theta$ и любом $0\leq\gamma\leq1$
            \[ l_n(\theta)=\frac{1}{n}\sum_{t=1}^nsgn(y_t-\theta)\xrightarrow{\P}\E sgn(y_1-\theta)\defeq\Lambda_M(\gamma,\theta) \]
            
            \underline{Задача:} Пусть $\xi$ и $\eta$ - независимые случайные векторы, причем
            $\eta$ - дискретный вектор со значениями $\eta_1,\eta_2,\ldots$. Проверить, что
            \[\E \phi(\xi,\eta)=\sum_{k\geq1}\E\phi(\xi,\eta_k)\P(\eta=\eta_k)=\sum_{k\geq1}\E(\phi(\xi,\eta_k)\vert H_k)\P(H_k),\text{ где гипотеза } H_k=(\eta=\eta_k) \]

            Найдем удобный вид для $\Lambda_M(\gamma,\theta)$. Имеем
            \begin{equation} \label{eq::lambda_transforming}
                \Lambda_M(\gamma,\theta)=\E(1-2I(y_1-\theta\leq0))=1-2\E I(\eps_1\leq\theta-a-z_1^\gamma\xi_1)=
                1-2\E G(\theta-a-z_1^\gamma\xi_1)
            \end{equation}
            так как $signx=1-2I(x<0),\ x\neq0$. Чтобы упростить \eqref{eq::lambda_transforming},
            введем две гипотезы $H_1=(z_1^\gamma=0)$ и $H_2=(z_2^\gamma=1)$.
            Тогда, используя задачу, получаем из \eqref{eq::lambda_transforming}:
            \[\Lambda_M(\gamma,\theta)=1-2(1-\gamma)G(\theta-a)-2\gamma\E G(\theta-a-\xi_1)\]
            Функция $\Lambda_M(\gamma,\theta)$ определена при всех $\gamma,\theta$.
            \item Функция $\Lambda_M(\gamma,\theta)$ в окрестности точки $(0,a)$ удовлетворяет
            всем предположениям теормы о неявной функции. А именно:
            \begin{enumerate}
                \item $\Lambda_M(0,a)=1-2G(0)=0$
                \item Существует и непрерывны по паре $(\gamma,\theta)$ функции
                $\frac{\partial\Lambda_M(\gamma,\theta)}{\partial\gamma}$ и $\frac{\partial\Lambda_M(\gamma,\theta)}{\partial\theta}$ 
                \item $$\frac{\partial\Lambda_M(\gamma,\theta)}{\partial\theta}=-2g(0)\neq0$$
            \end{enumerate}
            Значит, в окрестности точки $(0,a)$ определена функция $\theta_m(\gamma)=\theta_\gamma^m$ такая, что
            \[\Lambda_M(\gamma,\theta_\gamma^m)=0\]
            Кроме того, $\theta_0^m=a;\ \theta_\gamma^m\rightarrow\theta_0$ при $\gamma\rightarrow0$;
            Функция $\theta_0^m$ дифференцируема в точке $\gamma=0$, и
            \begin{equation} \label{eq::theta_func_derivative}
                \left.\frac{d\theta_\gamma^m}{d\gamma}\right\rvert_{\gamma=0}=-\left(\frac{\partial\Lambda_m(0,a)}{\partial\theta}\right)^{-1}\frac{\partial\Lambda_m(0,a)}{\partial\gamma}=\frac{1-2\E G(-\xi_1)}{2g(0)}
            \end{equation}
            \item Покажем, что
            \begin{equation} \label{eq::sample_median_p_conv}
                \widehat{m}_n^y\xrightarrow{\P}\theta_\gamma^m,\ n\rightarrow\infty
            \end{equation}
            Тогда из \eqref{eq::theta_func_derivative}-\eqref{eq::sample_median_p_conv}
            будет слеовать, что функционал влияния выборочной медианы равен
            \begin{equation}\label{eq::theta_influence_func}
                IF(\theta_\gamma^m,\mu_\xi)=\frac{1-2\E G(-\xi_1)}{2g(0)}
            \end{equation}
            Модуль числителя в \eqref{eq::theta_influence_func} не больше единицы, причем
            если $\theta_1$ неслучайно и $\theta_1\rightarrow\infty$,
            то числитель стремится к единице. Значит,
            \[GES(\theta_\gamma^m,M_\xi)=\sup_{\mu_\xi\in M_\xi}\lvert IF(\theta_\gamma^M,\mu_\xi)\rvert=\frac{1}{2g(0)}\]
            то есть мы докажем теорему.

            Докажем \eqref{eq::sample_median_p_conv}. Имеем при малых $\gamma,\ \xi,\ \theta$ ($\gamma$-фикс.) вблизи $a$:
            \[\frac{\partial\Lambda_M(\gamma,\theta)}{\partial\theta}=-2(1-\gamma)g(\theta-a)-2\gamma\E g(\theta-a-\xi_1)<0\]
            то есть \underline{$\Lambda_M(\gamma,\theta)$ убывает по $\theta$}.
            Значит, $\begin{cases}
                \Lambda_M(\gamma,\theta_\gamma^m-\Delta)>0\\
                \Lambda_M(\gamma,\theta_\gamma^m+\Delta)<0
            \end{cases}$
            % TODO
            \begin{figure}[h]
                \centering 
                \begin{gnuplot}[terminal=epslatex, scale=0.6]
                    set xlabel '$\theta$'
                    set border 0
                    set xrange [-1:4]
                    set yrange [-0.6:0.6]
                    set xzeroaxis ls -1
                    set yzeroaxis ls -1
                    unset ytics
                    unset xtics
                    
                    f(x) = (-0.4*x+1.6)**(3)-0.5
                    a = 1
                    b = 3
                    f_cropped(x) = (x >= a && x <= b) ? f(x) : 1/0

                    set arrow from a,0 to a,f(a) nohead dashtype 3
                    set arrow from b,0 to b,f(b) nohead dashtype 3

                    set label "$\\theta_\\gamma^m$" at 2.016,0 point pointtype 7 pointsize 1 offset -1,-1.3
                    set label "$\\theta_\\gamma^m-\\Delta$" at a,0 point pointtype 7 pointsize 1 offset 0,-1
                    set label "$\\theta_\\gamma^m+\\Delta$" at b,0 point pointtype 7 pointsize 1 offset 0,1
                    set label "0" at 0,0 offset -2,-1
                    plot f_cropped(x)  title '$\Lambda_M(\gamma,\theta_\gamma^m-\Delta)$'
                \end{gnuplot}
            \end{figure}
            Но 
            \begin{equation} \label{eq::sample_plaus_func_ineq}
                \begin{cases}
                    l_n(\theta_\gamma^m-\Delta)\xrightarrow{\P}\Lambda_M(\gamma,\theta_\gamma^m-\Delta)>0 \\
                    l_n(\theta_\gamma^m+\Delta)\xrightarrow{\P}\Lambda_M(\gamma,\theta_\gamma^m+\Delta)<0
                \end{cases}
            \end{equation}
            Функция $l_n(\theta)$ монотонно убывает (точнее, не возрастает) по $\theta$.
            В силу \eqref{eq::sample_plaus_func_ineq} с вероятностью сколь угодно близкой к
            единице при достаточно больших $n$ \underline{все} корни уравнения $l_n(\theta)=0$
            лежат в интервале $(\theta_\gamma^m-\Delta, \theta_\gamma^m+\Delta)$. А значит
            и выборочная медиана тоже! Поскольку $\Delta>0$ любое, то получаем
            \begin{equation*}
                \widehat{m}_n^y\xrightarrow{\P}\theta_\gamma^m,\ n\rightarrow\infty
            \end{equation*}
            что и доказывает теорему.
        \end{enumerate}
    \end{proof}
\end{example}

\subsubsection*{Как находить функционал влияния в общей ситуации?}
Пусть оценка $\widehat{\beta}_n$ ищется как корень уравнения:
\begin{equation}\label{eq::beta_estimate}
    l_n(\theta)\defeq \frac{1}{n}\sum_{t=1}^n\phi_t(J_n, \theta)=0
\end{equation}
Пусть будут выполнены следующие условия
\begin{enumerate}
    \item \[l_n(\theta)=\frac{1}{n}\sum_{t=1}^n\phi_t(J_n,\theta)\xrightarrow{\P}\Lambda(\gamma,\theta)\]
        при всех $\lvert\theta-\beta\rvert<\delta,\ 0\leq\gamma<\gamma_0$
    \item $\Lambda(0,\beta)=0$
    \item Пусть $\Lambda(\gamma,\theta)$ можно продолжить на отрезок малых $\gamma$ так,
    что при $\lvert\theta-\beta\rvert<\delta,\ \lvert\gamma\rvert<\gamma_0$ сущесвтуют
    и непрерывны по паре аргументов $(\gamma,\theta)$ частные производные
    $\frac{\partial\Lambda(\gamma,\theta)}{\partial\gamma}$, $\frac{\partial\Lambda(\gamma,\theta)}{\partial\theta}$.
    \item Пусть $\lambda(\beta)\defeq\frac{\partial\Lambda(0,\theta)}{\partial\theta}\neq0$
\end{enumerate}
\begin{theorem} \label{th::M_estimate_sample_median}
    Пусть выполнены условия (1)-(4), и функции $\phi_t(J_n,\theta)$ непрерывны по $\theta$.
    Тогда уравнение \eqref{eq::beta_estimate} с вероятностью, стремящейся к единице при $n\rightarrow\infty$,
    имеет при достаточно малых $\gamma>0$ решение $\widehat{\beta}_n$, что соответствующая
    оценка $\widetilde{\beta}_n\xrightarrow{\P}\theta_\gamma,\ \theta_0=0$, и существует
    функционал влияния:
    \[\boxed{IF(\theta_\gamma,\mu_\xi)=-\frac{1}{\lambda(\beta)}\frac{\partial}{\partial\gamma}\Lambda(0,\beta)}\]
\end{theorem}
\begin{example}[$M$-оценка медианы]
    Пусть 
    \[\begin{cases}
        u_t=a+\eps_t \\ y_t=u_t+z_t^\gamma\xi_t
    \end{cases} \{\eps_t\}\text{-н.о.р.},\ \eps_1\sim g(x)=G'(x),\ g(x)=g(-x)\]
     Тогда $a$ - медиана функции распределения случайной величины $u_1$.

     Будем искать оценку $a$, обозначим ее как $\widehat{a}_n$, как корень уравнения
     \begin{equation}\label{eq::a_estimate}
         \sum_{t=1}^n\psi(y_t-\theta)=0
     \end{equation}
     Тогда оценка называется \defin{$M$-оценкой}. В частности, при $\psi(x)=x,\ \widehat{a}_n=\overline{y}$;
     при $\psi(x)=sgn(x),\ \widehat{a}_n=\widehat{m}_n^y$.

     Пусть выполняются условия: \begin{enumerate}
         \item $\psi(x)$ - нечетная строго возрастающая фунцкия
         \[\lim_{x\rightarrow+\infty}\psi(x)=c_1>0,\ \lim_{x\rightarrow-\infty}\psi(x)=c_2<\infty\]
         \item Существует непрерывная и ограниченая $\psi'(x)$, $\E\psi'(\eps_1)\neq0$
     \end{enumerate}
     Тогда уравнение \eqref{eq::a_estimate} всегда имеет и притом \underline{единственное} решение.
     Условия (1)-(2) выполнены, например, для $\psi(x)=arctan(x)$.

     Найдем функционал влияния и чувствительность $M$-оценки, используя теорему \ref{th::M_estimate_sample_median}.
     Проверим ее условия:
     \begin{enumerate}
         \item 
         \[\frac{1}{n}\sum_{t=1}^n\psi(y_t-\theta)\xrightarrow{\P}\E\psi(y_1-\theta)\defeq\Lambda(\gamma,\theta),\ \forall\theta,\ \forall0\leq\gamma\leq1\]
         Введем гипотезы $H_1=(z_1^\gamma=0),\ H_2=(z_2^\gamma=0)$. Тогда 
         \[\Lambda(\gamma,\theta)=\sum_{i=1}^2\E(\psi(\underbrace{\eps_1+a+z_1^\gamma\xi_1}_{y_1}-\theta)\vert H_i)\P(H_i)
         =(1-\gamma)\E\psi(\eps_1+a-\theta)+\gamma\E\psi(\eps_1+\xi_1+a-\theta)\]
         \item $\Lambda(0,a)=\E\psi(\eps_1)=0$, так как $\psi(x)$ - нечетная, а $g(x)$-четная.
         \item Функция $\Lambda(\gamma,\theta)$ определена при всех $\gamma$ и $\theta$.
         Частные производные $\frac{\partial\Lambda(\gamma,\theta)}{\partial\gamma}$, $\frac{\partial\Lambda(\gamma,\theta)}{\partial\theta}$
         существуют при условиях (1)-(2) и непрерывны по паре $(\gamma,\theta)$. В частности,
         \[\frac{\partial\Lambda(0,a)}{\partial\gamma}=-\E\psi(\eps_1)+\E\psi(\eps_1+\xi_1)=\E\psi(\eps_1+\xi_1)\]
         \item $\frac{\partial\Lambda(0,a)}{\partial\theta}=-\E\psi'(\eps_1)\neq0$
     \end{enumerate}
     В силу теоремы \ref{th::M_estimate_sample_median} 
    \[\widehat{a}_n\xrightarrow{\P}\theta_\gamma,\ \theta_0=a \]
    \[IF(\theta_\gamma,\mu_\xi)=\frac{\E\psi(\eps_1+\xi_1)}{\E\psi'(\eps_1)} \]
    \[GES(\theta_\gamma, M_\xi)\leq\frac{\max\{|c_1|,|c_2|\}}{\E\psi'(\eps_1)}<\infty,\text{ $M_\xi$-класс всех вер. распределений}\]
\end{example}

\end{document}